{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an example for the polygon dataset made by [labelme](https://github.com/wkentaro/labelme).\n",
    "\n",
    "You don't need to make background polygon. If there are no polygon in the image, labelme don't export json file, but it's ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to this package's src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3plus_srcdir=\"./src\"\n",
    "sys.path.append(deeplabv3plus_srcdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, write about gpu setting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(visible_device_list=\"1\", allow_growth=True)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = gpu_options)\n",
    "tf.compat.v1.enable_eager_execution(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block is for debugging.\n",
    "#import importlib\n",
    "#importlib.reload(sys.modules['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import from .src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import deeplab_v3plus_transfer_os16\n",
    "#from image_utils import make_x_from_image_paths,make_y_from_image_paths,convert_y_to_image_array\n",
    "from data_gen import DataGenerator\n",
    "from metrics import IoU\n",
    "from label import Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned model and loss curve are exported to out_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"../crop\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make train_x_paths and train_y_paths as list.\n",
    "\n",
    "The order of train_x_paths and train_y_paths mast be correspond.\n",
    "\n",
    "If there are no polygon in the image, labelme dosen't export any json file. Therefore, append to train_y_paths \"None\" instead of path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "traindata_dir = '../train_data'\n",
    "#testdata_dir = '../kaigan_block/dataset/x_test2'\n",
    "\n",
    "train_x_paths = glob.glob(os.path.join(traindata_dir,'*.jpg'))\n",
    "train_x_paths.sort()\n",
    "image_names = [os.path.basename(train_x_paths[i]).split('.')[0] for i in range(len(train_x_paths))]\n",
    "train_y_paths=[]\n",
    "for i, image_name in enumerate(image_names):\n",
    "    p = os.path.join(traindata_dir, image_name+'.json')\n",
    "    if os.path.exists(p):\n",
    "        train_y_paths.append(p)\n",
    "    else:\n",
    "        train_y_paths.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set label_file_path, and image_size. Every image is resize to image_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_path = os.path.join(traindata_dir, 'label_list.csv')\n",
    "label = Label(label_file_path)\n",
    "image_size = (512,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set batch_size and n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=6\n",
    "n_epochs=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose model from [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "If you want to do transfer learning, preprocess must be correspond to the encoder.\n",
    "\n",
    "layer_name_to_decorder means the layer name which correspond to \"Low-Level Features\" arrow in Fig.2 of the [paper](https://arxiv.org/pdf/1802.02611.pdf).\n",
    "\n",
    "encoder_end_layer_name means the layer name input to ASPP block.\n",
    "\n",
    "deeplab_v3plus_transfer_os16 makes model.\n",
    "- If you don't want to freeze encoder, write freeze_encoder=False in the function.\n",
    "- Default activation function of the last layer is softmax. If you want to use sigmoid, write output_activation='sigmoid' in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.applications.Xception(input_shape=(512,512,3), weights=\"imagenet\", include_top=False)\n",
    "preprocess = keras.applications.xception.preprocess_input\n",
    "layer_name_to_decoder = \"block3_sepconv2_bn\"\n",
    "encoder_end_layer_name = \"block13_sepconv2_bn\"\n",
    "model = deeplab_v3plus_transfer_os16(label.n_labels, encoder, layer_name_to_decoder, encoder_end_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data generator like this. \n",
    "\n",
    "You can make about validation data in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(train_x_paths,\n",
    "                               train_y_paths,\n",
    "                               image_size,\n",
    "                               label,\n",
    "                               batch_size,\n",
    "                               preprocess,\n",
    "                               augmentation=True,\n",
    "                               shuffle=True,\n",
    "                               resize_or_crop=\"crop\",\n",
    "                               data_type=\"polygon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model. You can change it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.categorical_crossentropy\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=loss_function, metrics=[IoU])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = os.path.join(out_dir,'{epoch:06d}.h5')\n",
    "filepath = os.path.join(out_dir,'best_model.h5')\n",
    "cp_cb = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                        #monitor='val_IoU', \n",
    "                                        monitor='IoU', \n",
    "                                        verbose=1, \n",
    "                                        save_best_only=True, \n",
    "                                        save_weights_only=False, \n",
    "                                        mode='max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_data_gen,\n",
    "                           epochs=n_epochs,\n",
    "                           steps_per_epoch=len(train_data_gen),\n",
    "                           #validation_data=valid_data_gen,\n",
    "                           #validation_steps=len(valid_data_gen),\n",
    "                           #shuffle = False,\n",
    "                           workers=8,\n",
    "                           use_multiprocessing=True,\n",
    "                           callbacks=[cp_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(hist.history[\"loss\"], label=\"loss\")\n",
    "#plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(hist.history[\"IoU\"], label=\"IoU\")\n",
    "#plt.plot(hist.history[\"val_IoU\"], label=\"val_IoU\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(out_dir,'losscurve.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save last epoch model, loss, and metrics,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(out_dir,'final_epoch.h5'))\n",
    "for key in sorted(hist.history.keys()):\n",
    "    np.savetxt(os.path.join(out_dir,key+'.txt'),np.array(hist.history[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
