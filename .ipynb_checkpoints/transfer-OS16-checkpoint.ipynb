{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3plus_dir=\"./src\"\n",
    "sys.path.append(deeplabv3plus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(visible_device_list=\"1\", allow_growth=True)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = gpu_options)\n",
    "tf.compat.v1.enable_eager_execution(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import deeplab_v3plus_transfer_os16\n",
    "#from image_utils import make_x_from_image_paths,make_y_from_image_paths,convert_y_to_image_array\n",
    "from data_gen import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from './src/model.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sys.modules['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"xception_transfer_os16\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 21\n",
    "image_size = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_img_dir = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/SegmentationClass\"\n",
    "img_dir = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/JPEGImages\"\n",
    "train_set_path = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt\"\n",
    "valid_set_path = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt\"\n",
    "\n",
    "with open(train_set_path) as f:\n",
    "    train_img_names = f.read().split(\"\\n\")[:-1]\n",
    "with open(valid_set_path) as f:\n",
    "    valid_img_names = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "img_paths = [os.path.join(img_dir,train_img_names[i]) + \".jpg\" for i in range(len(train_img_names))]\n",
    "seg_img_paths = [os.path.join(seg_img_dir,train_img_names[i]) + \".png\" for i in range(len(train_img_names))]\n",
    "\n",
    "valid_x_paths = [os.path.join(img_dir,valid_img_names[i]) + \".jpg\" for i in range(len(valid_img_names))]\n",
    "valid_y_paths = [os.path.join(seg_img_dir,valid_img_names[i]) + \".png\" for i in range(len(valid_img_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "n_epochs=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(n_categories, image_size, batch_size, img_paths    , seg_img_paths, augmentation=True )\n",
    "valid_data_gen = DataGenerator(n_categories, image_size, batch_size, valid_x_paths, valid_y_paths, augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v1.keras.layers' has no attribute 'Upsampling2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1dc65456e87b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayer_name_to_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"block4_sepconv1_act\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder_end_layer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"block13_sepconv2_bn\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeplab_v3plus_transfer_os16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name_to_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_end_layer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/stg318/User/taniguchi-j/DeepLab_v3plus/src/model.py\u001b[0m in \u001b[0;36mdeeplab_v3plus_transfer_os16\u001b[0;34m(n_categories, encoder, layer_name_to_decoder, encoder_end_layer_name, freeze_encoder)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m#decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mx_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dec1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mx_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsampling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dec_upsample1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mx_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mASPP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dec_concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mx_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSepConv_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dec1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppc/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.layers' has no attribute 'Upsampling2D'"
     ]
    }
   ],
   "source": [
    "encoder = keras.applications.Xception(input_shape=(512,512,3), weights=\"imagenet\", include_top=False)\n",
    "layer_name_to_decoder = \"block4_sepconv1_act\"\n",
    "encoder_end_layer_name = \"block13_sepconv2_bn\"\n",
    "model = deeplab_v3plus_transfer_os16(n_categories, encoder, layer_name_to_decoder, encoder_end_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xception\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 255, 255, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 255, 255, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 255, 255, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 253, 253, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 253, 253, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 253, 253, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 253, 253, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 253, 253, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 253, 253, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 253, 253, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 253, 253, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 127, 127, 128 8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 127, 127, 128 0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 127, 127, 128 512         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 127, 127, 128 0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 127, 127, 128 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 127, 127, 256 33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 127, 127, 256 1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 127, 127, 256 0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 127, 127, 256 67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 127, 127, 256 1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 256)  32768       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 64, 64, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 64, 64, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 64, 64, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 64, 64, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 64, 64, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 64, 64, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 64, 64, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 728)  186368      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 32, 32, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 728)  2912        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 32, 32, 728)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 32, 32, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 32, 32, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 32, 32, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 32, 32, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 32, 32, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 32, 32, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 32, 32, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 32, 32, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 32, 32, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 32, 32, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 32, 32, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 32, 32, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 32, 32, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 32, 32, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 32, 32, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 32, 32, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 32, 32, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 32, 32, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 32, 32, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 32, 32, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 32, 32, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 32, 32, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 32, 32, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 32, 32, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 32, 32, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 32, 32, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 32, 32, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 32, 32, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 32, 32, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 32, 32, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 32, 32, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 32, 32, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 32, 32, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 32, 32, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 32, 32, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 32, 32, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 32, 32, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 32, 32, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 32, 32, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 32, 32, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 32, 32, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 32, 32, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 32, 32, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 32, 32, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 32, 32, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 32, 32, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 32, 32, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 32, 32, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 32, 32, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 32, 32, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 32, 32, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 32, 32, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 32, 32, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 32, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 32, 32, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 32, 32, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 32, 32, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 32, 32, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 32, 32, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 32, 32, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 32, 32, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 32, 32, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 32, 32, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 32, 32, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 32, 32, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 32, 32, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 32, 32, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 32, 32, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 32, 32, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 32, 32, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 32, 32, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 32, 32, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 32, 32, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 32, 32, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 32, 32, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 32, 32, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 32, 32, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 32, 32, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 1024) 745472      add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 16, 16, 1024) 0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 1024) 4096        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 1024) 0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 16, 16, 1536) 1582080     add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 16, 16, 1536) 6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 16, 16, 1536) 0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 16, 16, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 16, 16, 2048) 8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 16, 16, 2048) 0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.categorical_crossentropy\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=loss_function, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(out_dir,'{epoch:06d}.h5')\n",
    "cp_cb = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                        monitor='val_loss', \n",
    "                                        verbose=0, \n",
    "                                        save_best_only=True, \n",
    "                                        save_weights_only=False, \n",
    "                                        mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1127 16:51:12.641300 140735963811024 deprecation.py:323] From /home/taniguchi-j/anaconda3/envs/ppc/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/92 [============================>.] - ETA: 1s - loss: 2.5627 - acc: 0.4969Epoch 1/300\n",
      "92/92 [==============================] - 178s 2s/step - loss: 2.5580 - acc: 0.4997 - val_loss: 2.6768 - val_acc: 0.7455\n",
      "Epoch 2/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 2.1443 - acc: 0.6818Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 2.3197 - acc: 0.7507Epoch 1/300\n",
      "92/92 [==============================] - 193s 2s/step - loss: 2.1407 - acc: 0.6830 - val_loss: 2.3218 - val_acc: 0.7497\n",
      "Epoch 3/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.9598 - acc: 0.7176Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 1.9973 - acc: 0.7875Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 1.9593 - acc: 0.7174 - val_loss: 1.9996 - val_acc: 0.7867\n",
      "Epoch 4/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.8007 - acc: 0.7418Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 1.8020 - acc: 0.7414 - val_loss: 1.6681 - val_acc: 0.8168\n",
      "Epoch 5/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.6664 - acc: 0.7604Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 1.4298 - acc: 0.8413\n",
      "92/92 [==============================] - 204s 2s/step - loss: 1.6649 - acc: 0.7608 - val_loss: 1.4313 - val_acc: 0.8407\n",
      "Epoch 6/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.5418 - acc: 0.7743Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 1.3864 - acc: 0.8315Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 1.5415 - acc: 0.7746 - val_loss: 1.3877 - val_acc: 0.8310\n",
      "Epoch 7/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.4183 - acc: 0.7950Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 1.2783 - acc: 0.8282Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 1.4191 - acc: 0.7950 - val_loss: 1.2796 - val_acc: 0.8277\n",
      "Epoch 8/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.3130 - acc: 0.8041Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 1.3132 - acc: 0.8042 - val_loss: 1.0760 - val_acc: 0.8425\n",
      "Epoch 9/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.2283 - acc: 0.8124Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 1.2280 - acc: 0.8127 - val_loss: 1.2089 - val_acc: 0.8084\n",
      "Epoch 10/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.1546 - acc: 0.8206Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 1.1537 - acc: 0.8209 - val_loss: 1.1759 - val_acc: 0.8021\n",
      "Epoch 11/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.0797 - acc: 0.8269Epoch 1/300\n",
      "92/92 [==============================] - 189s 2s/step - loss: 1.0783 - acc: 0.8267 - val_loss: 0.9581 - val_acc: 0.8448\n",
      "Epoch 12/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.0230 - acc: 0.8308Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 1.0218 - acc: 0.8312 - val_loss: 0.8840 - val_acc: 0.8418\n",
      "Epoch 13/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.9501 - acc: 0.8403Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.8767 - acc: 0.8478Epoch 1/300\n",
      "92/92 [==============================] - 201s 2s/step - loss: 0.9502 - acc: 0.8400 - val_loss: 0.8790 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.8994 - acc: 0.8433Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.8878 - acc: 0.8420\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.8988 - acc: 0.8436 - val_loss: 0.8929 - val_acc: 0.8414\n",
      "Epoch 15/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.8580 - acc: 0.8454Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.9453 - acc: 0.8284Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.8578 - acc: 0.8454 - val_loss: 0.9473 - val_acc: 0.8280\n",
      "Epoch 16/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.8118 - acc: 0.8521Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.8127 - acc: 0.8517 - val_loss: 0.8657 - val_acc: 0.8372\n",
      "Epoch 17/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.7727 - acc: 0.8547Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.7729 - acc: 0.8544 - val_loss: 0.8283 - val_acc: 0.8425\n",
      "Epoch 18/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.7362 - acc: 0.8573Epoch 1/300\n",
      "92/92 [==============================] - 227s 2s/step - loss: 0.7366 - acc: 0.8570 - val_loss: 0.7186 - val_acc: 0.8579\n",
      "Epoch 19/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.7040 - acc: 0.8616Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6456 - acc: 0.8625\n",
      "92/92 [==============================] - 160s 2s/step - loss: 0.7035 - acc: 0.8616 - val_loss: 0.6475 - val_acc: 0.8622\n",
      "Epoch 20/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6772 - acc: 0.8637Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6391 - acc: 0.8603Epoch 1/300\n",
      "92/92 [==============================] - 184s 2s/step - loss: 0.6783 - acc: 0.8631 - val_loss: 0.6419 - val_acc: 0.8598\n",
      "Epoch 21/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6494 - acc: 0.8643Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.6491 - acc: 0.8642 - val_loss: 0.6338 - val_acc: 0.8655\n",
      "Epoch 22/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6218 - acc: 0.8685Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6565 - acc: 0.8553Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.6222 - acc: 0.8679 - val_loss: 0.6600 - val_acc: 0.8547\n",
      "Epoch 23/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6011 - acc: 0.8706Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6083 - acc: 0.8611Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.6005 - acc: 0.8706 - val_loss: 0.6096 - val_acc: 0.8608\n",
      "Epoch 24/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5806 - acc: 0.8709Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6226 - acc: 0.8539Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.5795 - acc: 0.8713 - val_loss: 0.6257 - val_acc: 0.8534\n",
      "Epoch 25/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5651 - acc: 0.8720Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.5643 - acc: 0.8721 - val_loss: 0.6339 - val_acc: 0.8559\n",
      "Epoch 26/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5456 - acc: 0.8748Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.5442 - acc: 0.8750 - val_loss: 0.5576 - val_acc: 0.8669\n",
      "Epoch 27/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5256 - acc: 0.8766Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6057 - acc: 0.8585Epoch 1/300\n",
      "92/92 [==============================] - 161s 2s/step - loss: 0.5265 - acc: 0.8764 - val_loss: 0.6094 - val_acc: 0.8580\n",
      "Epoch 28/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5048 - acc: 0.8800Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5587 - acc: 0.8665Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.5043 - acc: 0.8802 - val_loss: 0.5625 - val_acc: 0.8659\n",
      "Epoch 29/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4922 - acc: 0.8824Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4926 - acc: 0.8713Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.4917 - acc: 0.8826 - val_loss: 0.4951 - val_acc: 0.8709\n",
      "Epoch 30/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4875 - acc: 0.8794Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5270 - acc: 0.8665Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.4878 - acc: 0.8793 - val_loss: 0.5296 - val_acc: 0.8661\n",
      "Epoch 31/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4775 - acc: 0.8805Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5256 - acc: 0.8619Epoch 1/300\n",
      "92/92 [==============================] - 158s 2s/step - loss: 0.4765 - acc: 0.8807 - val_loss: 0.5283 - val_acc: 0.8614\n",
      "Epoch 32/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4558 - acc: 0.8856Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.4562 - acc: 0.8855 - val_loss: 0.5120 - val_acc: 0.8708\n",
      "Epoch 33/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4490 - acc: 0.8844Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4802 - acc: 0.8722Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.4494 - acc: 0.8841 - val_loss: 0.4821 - val_acc: 0.8718\n",
      "Epoch 34/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4374 - acc: 0.8863Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5447 - acc: 0.8654\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.4383 - acc: 0.8859 - val_loss: 0.5476 - val_acc: 0.8650\n",
      "Epoch 35/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4331 - acc: 0.8859Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5235 - acc: 0.8619Epoch 1/300\n",
      "92/92 [==============================] - 229s 2s/step - loss: 0.4320 - acc: 0.8861 - val_loss: 0.5265 - val_acc: 0.8614\n",
      "Epoch 36/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4228 - acc: 0.8871Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.4237 - acc: 0.8866 - val_loss: 0.4660 - val_acc: 0.8708\n",
      "Epoch 37/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4152 - acc: 0.8885Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4741 - acc: 0.8715\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.4169 - acc: 0.8878 - val_loss: 0.4777 - val_acc: 0.8710\n",
      "Epoch 38/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3918 - acc: 0.8939Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.3917 - acc: 0.8939 - val_loss: 0.4914 - val_acc: 0.8682\n",
      "Epoch 39/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3975 - acc: 0.8908Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4901 - acc: 0.8698Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.3971 - acc: 0.8909 - val_loss: 0.4936 - val_acc: 0.8693\n",
      "Epoch 40/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3833 - acc: 0.8943Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4698 - acc: 0.8672Epoch 1/300\n",
      "92/92 [==============================] - 160s 2s/step - loss: 0.3831 - acc: 0.8944 - val_loss: 0.4736 - val_acc: 0.8667\n",
      "Epoch 41/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3757 - acc: 0.8954Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4734 - acc: 0.8658Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.3751 - acc: 0.8956 - val_loss: 0.4774 - val_acc: 0.8653\n",
      "Epoch 42/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3746 - acc: 0.8954Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4728 - acc: 0.8699Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.3739 - acc: 0.8956 - val_loss: 0.4774 - val_acc: 0.8695\n",
      "Epoch 43/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3598 - acc: 0.8979Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4626 - acc: 0.8682Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.3599 - acc: 0.8978 - val_loss: 0.4654 - val_acc: 0.8677\n",
      "Epoch 44/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3526 - acc: 0.8993Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4603 - acc: 0.8711Epoch 1/300\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.3528 - acc: 0.8991 - val_loss: 0.4642 - val_acc: 0.8705\n",
      "Epoch 45/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3646 - acc: 0.8933Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4461 - acc: 0.8724Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.3647 - acc: 0.8934 - val_loss: 0.4504 - val_acc: 0.8719\n",
      "Epoch 46/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3553 - acc: 0.8963Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4291 - acc: 0.8708Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.3547 - acc: 0.8966 - val_loss: 0.4318 - val_acc: 0.8704\n",
      "Epoch 47/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3441 - acc: 0.8994Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4352 - acc: 0.8759Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.3473 - acc: 0.8983 - val_loss: 0.4402 - val_acc: 0.8754\n",
      "Epoch 48/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3373 - acc: 0.8997Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4781 - acc: 0.8713Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.3369 - acc: 0.8999 - val_loss: 0.4808 - val_acc: 0.8709\n",
      "Epoch 49/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3346 - acc: 0.9001Epoch 1/300\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.3343 - acc: 0.9002 - val_loss: 0.4303 - val_acc: 0.8743\n",
      "Epoch 50/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3388 - acc: 0.8986Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.3379 - acc: 0.8989 - val_loss: 0.4246 - val_acc: 0.8734\n",
      "Epoch 51/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3312 - acc: 0.8995Epoch 1/300\n",
      "92/92 [==============================] - 204s 2s/step - loss: 0.3311 - acc: 0.8995 - val_loss: 0.4170 - val_acc: 0.8762\n",
      "Epoch 52/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3191 - acc: 0.9030Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.3194 - acc: 0.9030 - val_loss: 0.4334 - val_acc: 0.8746\n",
      "Epoch 53/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3165 - acc: 0.9038Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4358 - acc: 0.8715Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.3166 - acc: 0.9037 - val_loss: 0.4389 - val_acc: 0.8709\n",
      "Epoch 54/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3131 - acc: 0.9038Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3930 - acc: 0.8799Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.3135 - acc: 0.9037 - val_loss: 0.3955 - val_acc: 0.8794\n",
      "Epoch 55/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3079 - acc: 0.9050Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4357 - acc: 0.8698Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.3072 - acc: 0.9052 - val_loss: 0.4397 - val_acc: 0.8693\n",
      "Epoch 56/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3067 - acc: 0.9048Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.3065 - acc: 0.9048 - val_loss: 0.4126 - val_acc: 0.8761\n",
      "Epoch 57/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3004 - acc: 0.9059Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.3004 - acc: 0.9059 - val_loss: 0.4197 - val_acc: 0.8750\n",
      "Epoch 58/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3005 - acc: 0.9051Epoch 1/300\n",
      "92/92 [==============================] - 225s 2s/step - loss: 0.2995 - acc: 0.9055 - val_loss: 0.4670 - val_acc: 0.8667\n",
      "Epoch 59/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2969 - acc: 0.9060Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4109 - acc: 0.8744Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.2975 - acc: 0.9057 - val_loss: 0.4129 - val_acc: 0.8740\n",
      "Epoch 60/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2959 - acc: 0.9064Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4215 - acc: 0.8730Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.2958 - acc: 0.9064 - val_loss: 0.4243 - val_acc: 0.8725\n",
      "Epoch 61/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2959 - acc: 0.9068Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4140 - acc: 0.8755Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.2954 - acc: 0.9069 - val_loss: 0.4178 - val_acc: 0.8749\n",
      "Epoch 62/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2923 - acc: 0.9070Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4206 - acc: 0.8725Epoch 1/300\n",
      "92/92 [==============================] - 197s 2s/step - loss: 0.2927 - acc: 0.9069 - val_loss: 0.4236 - val_acc: 0.8720\n",
      "Epoch 63/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2852 - acc: 0.9083Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.2858 - acc: 0.9081 - val_loss: 0.4220 - val_acc: 0.8713\n",
      "Epoch 64/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2825 - acc: 0.9096Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4000 - acc: 0.8806\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2821 - acc: 0.9099 - val_loss: 0.4051 - val_acc: 0.8800\n",
      "Epoch 65/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2861 - acc: 0.9072Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.2861 - acc: 0.9072 - val_loss: 0.4091 - val_acc: 0.8737\n",
      "Epoch 66/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2856 - acc: 0.9079Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.2858 - acc: 0.9077 - val_loss: 0.3982 - val_acc: 0.8792\n",
      "Epoch 67/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2767 - acc: 0.9100Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4025 - acc: 0.8771Epoch 1/300\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.2764 - acc: 0.9101 - val_loss: 0.4051 - val_acc: 0.8767\n",
      "Epoch 68/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2724 - acc: 0.9107Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.2720 - acc: 0.9109 - val_loss: 0.4169 - val_acc: 0.8754\n",
      "Epoch 69/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2749 - acc: 0.9107Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4122 - acc: 0.8720Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.2751 - acc: 0.9106 - val_loss: 0.4149 - val_acc: 0.8715\n",
      "Epoch 70/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2693 - acc: 0.9115Epoch 1/300\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.2710 - acc: 0.9111 - val_loss: 0.4250 - val_acc: 0.8732\n",
      "Epoch 71/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2747 - acc: 0.9081Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.2740 - acc: 0.9083 - val_loss: 0.4126 - val_acc: 0.8727\n",
      "Epoch 72/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2717 - acc: 0.9104Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3935 - acc: 0.8754Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.2710 - acc: 0.9106 - val_loss: 0.3968 - val_acc: 0.8749\n",
      "Epoch 73/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2689 - acc: 0.9110Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3861 - acc: 0.8776Epoch 1/300\n",
      "92/92 [==============================] - 161s 2s/step - loss: 0.2688 - acc: 0.9110 - val_loss: 0.3899 - val_acc: 0.8770\n",
      "Epoch 74/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2613 - acc: 0.9128Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 0.2609 - acc: 0.9130 - val_loss: 0.3964 - val_acc: 0.8770\n",
      "Epoch 75/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2600 - acc: 0.9130Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4088 - acc: 0.8724Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2597 - acc: 0.9132 - val_loss: 0.4121 - val_acc: 0.8719\n",
      "Epoch 76/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2631 - acc: 0.9110Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4227 - acc: 0.8670Epoch 1/300\n",
      "92/92 [==============================] - 194s 2s/step - loss: 0.2625 - acc: 0.9113 - val_loss: 0.4257 - val_acc: 0.8664\n",
      "Epoch 77/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2580 - acc: 0.9137Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3958 - acc: 0.8759Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2574 - acc: 0.9140 - val_loss: 0.4001 - val_acc: 0.8752\n",
      "Epoch 78/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2690 - acc: 0.9105Epoch 1/300\n",
      "92/92 [==============================] - 182s 2s/step - loss: 0.2683 - acc: 0.9108 - val_loss: 0.4186 - val_acc: 0.8723\n",
      "Epoch 79/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2598 - acc: 0.9129Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2597 - acc: 0.9128 - val_loss: 0.3983 - val_acc: 0.8780\n",
      "Epoch 80/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2599 - acc: 0.9119Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3953 - acc: 0.8753Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.2592 - acc: 0.9122 - val_loss: 0.3992 - val_acc: 0.8748\n",
      "Epoch 81/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2443 - acc: 0.9171Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3881 - acc: 0.8802Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.2447 - acc: 0.9169 - val_loss: 0.3923 - val_acc: 0.8796\n",
      "Epoch 82/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2515 - acc: 0.9147Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3865 - acc: 0.8804Epoch 1/300\n",
      "92/92 [==============================] - 193s 2s/step - loss: 0.2509 - acc: 0.9149 - val_loss: 0.3903 - val_acc: 0.8797\n",
      "Epoch 83/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2542 - acc: 0.9136Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4055 - acc: 0.8766\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.2532 - acc: 0.9139 - val_loss: 0.4100 - val_acc: 0.8760\n",
      "Epoch 84/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2516 - acc: 0.9147Epoch 1/300\n",
      "92/92 [==============================] - 194s 2s/step - loss: 0.2510 - acc: 0.9148 - val_loss: 0.3981 - val_acc: 0.8752\n",
      "Epoch 85/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2393 - acc: 0.9184Epoch 1/300\n",
      "92/92 [==============================] - 160s 2s/step - loss: 0.2391 - acc: 0.9184 - val_loss: 0.3987 - val_acc: 0.8765\n",
      "Epoch 86/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2431 - acc: 0.9168Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3937 - acc: 0.8796Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2443 - acc: 0.9164 - val_loss: 0.3978 - val_acc: 0.8789\n",
      "Epoch 87/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2364 - acc: 0.9182Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3939 - acc: 0.8768Epoch 1/300\n",
      "92/92 [==============================] - 159s 2s/step - loss: 0.2359 - acc: 0.9184 - val_loss: 0.3980 - val_acc: 0.8761\n",
      "Epoch 88/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2465 - acc: 0.9156Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3832 - acc: 0.8798Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2464 - acc: 0.9157 - val_loss: 0.3865 - val_acc: 0.8792\n",
      "Epoch 89/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2437 - acc: 0.9159Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4142 - acc: 0.8697Epoch 1/300\n",
      "92/92 [==============================] - 198s 2s/step - loss: 0.2434 - acc: 0.9161 - val_loss: 0.4180 - val_acc: 0.8691\n",
      "Epoch 90/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2353 - acc: 0.9186Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.2366 - acc: 0.9181 - val_loss: 0.4100 - val_acc: 0.8749\n",
      "Epoch 91/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2452 - acc: 0.9153Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4116 - acc: 0.8705Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.2444 - acc: 0.9156 - val_loss: 0.4151 - val_acc: 0.8700\n",
      "Epoch 92/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2422 - acc: 0.9158Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4049 - acc: 0.8762Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.2423 - acc: 0.9158 - val_loss: 0.4090 - val_acc: 0.8756\n",
      "Epoch 93/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2344 - acc: 0.9188Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.2350 - acc: 0.9186 - val_loss: 0.4046 - val_acc: 0.8759\n",
      "Epoch 94/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2394 - acc: 0.9179Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4119 - acc: 0.8759Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2397 - acc: 0.9178 - val_loss: 0.4159 - val_acc: 0.8752\n",
      "Epoch 95/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2322 - acc: 0.9196Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3834 - acc: 0.8809Epoch 1/300\n",
      "92/92 [==============================] - 160s 2s/step - loss: 0.2330 - acc: 0.9194 - val_loss: 0.3871 - val_acc: 0.8803\n",
      "Epoch 96/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2315 - acc: 0.9190Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4036 - acc: 0.8748Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.2316 - acc: 0.9191 - val_loss: 0.4063 - val_acc: 0.8742\n",
      "Epoch 97/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2233 - acc: 0.9220Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3968 - acc: 0.8782Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 0.2231 - acc: 0.9220 - val_loss: 0.3999 - val_acc: 0.8777\n",
      "Epoch 98/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2296 - acc: 0.9198Epoch 1/300\n",
      "92/92 [==============================] - 198s 2s/step - loss: 0.2294 - acc: 0.9198 - val_loss: 0.3920 - val_acc: 0.8782\n",
      "Epoch 99/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2210 - acc: 0.9234Epoch 1/300\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.2212 - acc: 0.9234 - val_loss: 0.4068 - val_acc: 0.8774\n",
      "Epoch 100/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2309 - acc: 0.9193Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.2312 - acc: 0.9191 - val_loss: 0.4050 - val_acc: 0.8792\n",
      "Epoch 101/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2278 - acc: 0.9211Epoch 1/300\n",
      "92/92 [==============================] - 194s 2s/step - loss: 0.2282 - acc: 0.9208 - val_loss: 0.4098 - val_acc: 0.8765\n",
      "Epoch 102/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2263 - acc: 0.9213Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4081 - acc: 0.8758Epoch 1/300\n",
      "92/92 [==============================] - 206s 2s/step - loss: 0.2261 - acc: 0.9214 - val_loss: 0.4131 - val_acc: 0.8751\n",
      "Epoch 103/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2192 - acc: 0.9227Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4051 - acc: 0.8771Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.2193 - acc: 0.9227 - val_loss: 0.4102 - val_acc: 0.8764\n",
      "Epoch 104/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2254 - acc: 0.9212Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3864 - acc: 0.8809Epoch 1/300\n",
      "92/92 [==============================] - 200s 2s/step - loss: 0.2250 - acc: 0.9213 - val_loss: 0.3907 - val_acc: 0.8803\n",
      "Epoch 105/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2233 - acc: 0.9213Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4024 - acc: 0.8774Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.2243 - acc: 0.9209 - val_loss: 0.4062 - val_acc: 0.8768\n",
      "Epoch 106/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2274 - acc: 0.9211Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.2275 - acc: 0.9210 - val_loss: 0.4305 - val_acc: 0.8715\n",
      "Epoch 107/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2153 - acc: 0.9238Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3963 - acc: 0.8776\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.2151 - acc: 0.9239 - val_loss: 0.4003 - val_acc: 0.8770\n",
      "Epoch 108/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2203 - acc: 0.9222Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.2208 - acc: 0.9221 - val_loss: 0.3997 - val_acc: 0.8778\n",
      "Epoch 109/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2171 - acc: 0.9236Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.2175 - acc: 0.9235 - val_loss: 0.4069 - val_acc: 0.8752\n",
      "Epoch 110/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2278 - acc: 0.9198Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3995 - acc: 0.8783\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.2276 - acc: 0.9198 - val_loss: 0.4049 - val_acc: 0.8777\n",
      "Epoch 111/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2205 - acc: 0.9224Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3799 - acc: 0.8823Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.2202 - acc: 0.9225 - val_loss: 0.3848 - val_acc: 0.8818\n",
      "Epoch 112/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2145 - acc: 0.9242Epoch 1/300\n",
      "92/92 [==============================] - 193s 2s/step - loss: 0.2143 - acc: 0.9242 - val_loss: 0.3810 - val_acc: 0.8838\n",
      "Epoch 113/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2186 - acc: 0.9227Epoch 1/300\n",
      "92/92 [==============================] - 160s 2s/step - loss: 0.2182 - acc: 0.9229 - val_loss: 0.3827 - val_acc: 0.8810\n",
      "Epoch 114/300\n",
      "91/92 [============================>.] - ETA: 2s - loss: 0.2095 - acc: 0.9259Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3838 - acc: 0.8780Epoch 1/300\n",
      "92/92 [==============================] - 281s 3s/step - loss: 0.2100 - acc: 0.9258 - val_loss: 0.3874 - val_acc: 0.8774\n",
      "Epoch 115/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2132 - acc: 0.9238Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3908 - acc: 0.8829Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.2124 - acc: 0.9241 - val_loss: 0.3960 - val_acc: 0.8823\n",
      "Epoch 116/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2225 - acc: 0.9213Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3833 - acc: 0.8824Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.2236 - acc: 0.9210 - val_loss: 0.3868 - val_acc: 0.8819\n",
      "Epoch 117/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2097 - acc: 0.9251Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.2096 - acc: 0.9251 - val_loss: 0.3877 - val_acc: 0.8815\n",
      "Epoch 118/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2113 - acc: 0.9252Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2122 - acc: 0.9248 - val_loss: 0.4061 - val_acc: 0.8791\n",
      "Epoch 119/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2110 - acc: 0.9247Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.2113 - acc: 0.9246 - val_loss: 0.3932 - val_acc: 0.8801\n",
      "Epoch 120/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2137 - acc: 0.9249Epoch 1/300\n",
      "92/92 [==============================] - 161s 2s/step - loss: 0.2132 - acc: 0.9251 - val_loss: 0.4123 - val_acc: 0.8758\n",
      "Epoch 121/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2167 - acc: 0.9234Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3963 - acc: 0.8798Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.2159 - acc: 0.9237 - val_loss: 0.4012 - val_acc: 0.8793\n",
      "Epoch 122/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2060 - acc: 0.9260Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3898 - acc: 0.8796Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.2056 - acc: 0.9261 - val_loss: 0.3921 - val_acc: 0.8791\n",
      "Epoch 123/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2133 - acc: 0.9248Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3847 - acc: 0.8825Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.2140 - acc: 0.9246 - val_loss: 0.3888 - val_acc: 0.8820\n",
      "Epoch 124/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2186 - acc: 0.9213Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3836 - acc: 0.8796Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.2181 - acc: 0.9215 - val_loss: 0.3852 - val_acc: 0.8792\n",
      "Epoch 125/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2054 - acc: 0.9266Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3957 - acc: 0.8797Epoch 1/300\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.2061 - acc: 0.9263 - val_loss: 0.3991 - val_acc: 0.8791\n",
      "Epoch 126/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2079 - acc: 0.9259Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4076 - acc: 0.8769Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.2073 - acc: 0.9261 - val_loss: 0.4116 - val_acc: 0.8764\n",
      "Epoch 127/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2034 - acc: 0.9273Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4141 - acc: 0.8774Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 0.2031 - acc: 0.9274 - val_loss: 0.4174 - val_acc: 0.8769\n",
      "Epoch 128/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2130 - acc: 0.9243Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4028 - acc: 0.8749Epoch 1/300\n",
      "92/92 [==============================] - 228s 2s/step - loss: 0.2123 - acc: 0.9246 - val_loss: 0.4072 - val_acc: 0.8743\n",
      "Epoch 129/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2044 - acc: 0.9265Epoch 1/300\n",
      "92/92 [==============================] - 159s 2s/step - loss: 0.2041 - acc: 0.9267 - val_loss: 0.4047 - val_acc: 0.8775\n",
      "Epoch 130/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1962 - acc: 0.9297Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3887 - acc: 0.8819\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.1957 - acc: 0.9299 - val_loss: 0.3913 - val_acc: 0.8814\n",
      "Epoch 131/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1988 - acc: 0.9295Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1989 - acc: 0.9295 - val_loss: 0.4066 - val_acc: 0.8805\n",
      "Epoch 132/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1992 - acc: 0.9288Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3872 - acc: 0.8794Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1991 - acc: 0.9289 - val_loss: 0.3907 - val_acc: 0.8789\n",
      "Epoch 133/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1953 - acc: 0.9302Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4079 - acc: 0.8771\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.1953 - acc: 0.9301 - val_loss: 0.4113 - val_acc: 0.8765\n",
      "Epoch 134/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1964 - acc: 0.9292Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3823 - acc: 0.8807Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1959 - acc: 0.9294 - val_loss: 0.3861 - val_acc: 0.8802\n",
      "Epoch 135/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1940 - acc: 0.9299Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3948 - acc: 0.8790Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 0.1943 - acc: 0.9298 - val_loss: 0.3985 - val_acc: 0.8784\n",
      "Epoch 136/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1961 - acc: 0.9295Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3926 - acc: 0.8820Epoch 1/300\n",
      "92/92 [==============================] - 185s 2s/step - loss: 0.1972 - acc: 0.9292 - val_loss: 0.3961 - val_acc: 0.8814\n",
      "Epoch 137/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2038 - acc: 0.9267Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3907 - acc: 0.8809Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.2032 - acc: 0.9270 - val_loss: 0.3939 - val_acc: 0.8804\n",
      "Epoch 138/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2070 - acc: 0.9258Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3907 - acc: 0.8802Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 0.2069 - acc: 0.9259 - val_loss: 0.3931 - val_acc: 0.8798\n",
      "Epoch 139/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1974 - acc: 0.9290Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3908 - acc: 0.8813Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.1974 - acc: 0.9291 - val_loss: 0.3941 - val_acc: 0.8807\n",
      "Epoch 140/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1978 - acc: 0.9291Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1983 - acc: 0.9291 - val_loss: 0.3858 - val_acc: 0.8806\n",
      "Epoch 141/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.2041 - acc: 0.9267Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3850 - acc: 0.8810Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.2043 - acc: 0.9266 - val_loss: 0.3875 - val_acc: 0.8806\n",
      "Epoch 142/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1939 - acc: 0.9307Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4086 - acc: 0.8772Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1936 - acc: 0.9308 - val_loss: 0.4124 - val_acc: 0.8767\n",
      "Epoch 143/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1965 - acc: 0.9293Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1958 - acc: 0.9296 - val_loss: 0.4279 - val_acc: 0.8692\n",
      "Epoch 144/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1997 - acc: 0.9279Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3954 - acc: 0.8801Epoch 1/300\n",
      "92/92 [==============================] - 177s 2s/step - loss: 0.1998 - acc: 0.9278 - val_loss: 0.3994 - val_acc: 0.8796\n",
      "Epoch 145/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1926 - acc: 0.9310Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3909 - acc: 0.8788Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 0.1923 - acc: 0.9311 - val_loss: 0.3947 - val_acc: 0.8783\n",
      "Epoch 146/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1949 - acc: 0.9288Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1948 - acc: 0.9288 - val_loss: 0.3938 - val_acc: 0.8808\n",
      "Epoch 147/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1925 - acc: 0.9303Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 2s - loss: 0.3997 - acc: 0.8803Epoch 1/300\n",
      "92/92 [==============================] - 229s 2s/step - loss: 0.1929 - acc: 0.9302 - val_loss: 0.4032 - val_acc: 0.8798\n",
      "Epoch 148/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1932 - acc: 0.9296Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4161 - acc: 0.8773Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1931 - acc: 0.9296 - val_loss: 0.4199 - val_acc: 0.8767\n",
      "Epoch 149/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1894 - acc: 0.9309Epoch 1/300\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.1894 - acc: 0.9309 - val_loss: 0.3897 - val_acc: 0.8801\n",
      "Epoch 150/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1856 - acc: 0.9330Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4032 - acc: 0.8795Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 0.1857 - acc: 0.9329 - val_loss: 0.4075 - val_acc: 0.8789\n",
      "Epoch 151/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1878 - acc: 0.9314Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3959 - acc: 0.8816\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1874 - acc: 0.9316 - val_loss: 0.4009 - val_acc: 0.8810\n",
      "Epoch 152/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1832 - acc: 0.9334Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4210 - acc: 0.8727Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1829 - acc: 0.9335 - val_loss: 0.4264 - val_acc: 0.8720\n",
      "Epoch 153/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1818 - acc: 0.9336Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1818 - acc: 0.9336 - val_loss: 0.3968 - val_acc: 0.8802\n",
      "Epoch 154/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1878 - acc: 0.9311Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1879 - acc: 0.9310 - val_loss: 0.4013 - val_acc: 0.8779\n",
      "Epoch 155/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1950 - acc: 0.9303Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1940 - acc: 0.9307 - val_loss: 0.4111 - val_acc: 0.8781\n",
      "Epoch 156/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1899 - acc: 0.9314Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4022 - acc: 0.8807Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.1894 - acc: 0.9317 - val_loss: 0.4070 - val_acc: 0.8800\n",
      "Epoch 157/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1980 - acc: 0.9285Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1977 - acc: 0.9286 - val_loss: 0.4089 - val_acc: 0.8757\n",
      "Epoch 158/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1920 - acc: 0.9306Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4095 - acc: 0.8747Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.1918 - acc: 0.9306 - val_loss: 0.4133 - val_acc: 0.8742\n",
      "Epoch 159/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1884 - acc: 0.9310Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.1882 - acc: 0.9311 - val_loss: 0.4202 - val_acc: 0.8779\n",
      "Epoch 160/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1919 - acc: 0.9304Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 0.1919 - acc: 0.9304 - val_loss: 0.4178 - val_acc: 0.8785\n",
      "Epoch 161/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1904 - acc: 0.9301Epoch 1/300\n",
      "92/92 [==============================] - 177s 2s/step - loss: 0.1898 - acc: 0.9304 - val_loss: 0.4018 - val_acc: 0.8798\n",
      "Epoch 162/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1883 - acc: 0.9319Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4244 - acc: 0.8723Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.1887 - acc: 0.9317 - val_loss: 0.4275 - val_acc: 0.8717\n",
      "Epoch 163/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1831 - acc: 0.9333Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4262 - acc: 0.8747Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1829 - acc: 0.9334 - val_loss: 0.4299 - val_acc: 0.8742\n",
      "Epoch 164/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1823 - acc: 0.9332Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 2s - loss: 0.4077 - acc: 0.8766Epoch 1/300\n",
      "92/92 [==============================] - 260s 3s/step - loss: 0.1828 - acc: 0.9331 - val_loss: 0.4119 - val_acc: 0.8761\n",
      "Epoch 165/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1786 - acc: 0.9352Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1785 - acc: 0.9352 - val_loss: 0.4026 - val_acc: 0.8811\n",
      "Epoch 166/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1872 - acc: 0.9313Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4046 - acc: 0.8809Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1872 - acc: 0.9314 - val_loss: 0.4091 - val_acc: 0.8803\n",
      "Epoch 167/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1851 - acc: 0.9324Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.1849 - acc: 0.9325 - val_loss: 0.4105 - val_acc: 0.8792\n",
      "Epoch 168/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1858 - acc: 0.9329Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4097 - acc: 0.8753Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.1853 - acc: 0.9331 - val_loss: 0.4129 - val_acc: 0.8749\n",
      "Epoch 169/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1832 - acc: 0.9338Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 2s - loss: 0.3889 - acc: 0.8826Epoch 1/300\n",
      "92/92 [==============================] - 257s 3s/step - loss: 0.1831 - acc: 0.9339 - val_loss: 0.3944 - val_acc: 0.8820\n",
      "Epoch 170/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1837 - acc: 0.9330Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1837 - acc: 0.9330 - val_loss: 0.4053 - val_acc: 0.8802\n",
      "Epoch 171/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1932 - acc: 0.9306Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1925 - acc: 0.9309 - val_loss: 0.3918 - val_acc: 0.8789\n",
      "Epoch 172/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1853 - acc: 0.9321Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3912 - acc: 0.8809Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1849 - acc: 0.9322 - val_loss: 0.3934 - val_acc: 0.8804\n",
      "Epoch 173/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1868 - acc: 0.9322Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4069 - acc: 0.8768Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1881 - acc: 0.9319 - val_loss: 0.4104 - val_acc: 0.8763\n",
      "Epoch 174/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1898 - acc: 0.9312Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4012 - acc: 0.8796Epoch 1/300\n",
      "92/92 [==============================] - 197s 2s/step - loss: 0.1894 - acc: 0.9314 - val_loss: 0.4048 - val_acc: 0.8791\n",
      "Epoch 175/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1821 - acc: 0.9338Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4316 - acc: 0.8710Epoch 1/300\n",
      "92/92 [==============================] - 160s 2s/step - loss: 0.1829 - acc: 0.9334 - val_loss: 0.4346 - val_acc: 0.8706\n",
      "Epoch 176/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1859 - acc: 0.9319Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4053 - acc: 0.8757Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.1858 - acc: 0.9320 - val_loss: 0.4079 - val_acc: 0.8753\n",
      "Epoch 177/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1833 - acc: 0.9327Epoch 1/300\n",
      "92/92 [==============================] - 229s 2s/step - loss: 0.1833 - acc: 0.9328 - val_loss: 0.4028 - val_acc: 0.8805\n",
      "Epoch 178/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1796 - acc: 0.9343Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 0.1791 - acc: 0.9345 - val_loss: 0.4225 - val_acc: 0.8758\n",
      "Epoch 179/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1739 - acc: 0.9363Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1740 - acc: 0.9362 - val_loss: 0.3872 - val_acc: 0.8829\n",
      "Epoch 180/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1829 - acc: 0.9333Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4077 - acc: 0.8790Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1824 - acc: 0.9335 - val_loss: 0.4124 - val_acc: 0.8785\n",
      "Epoch 181/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1814 - acc: 0.9339Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4140 - acc: 0.8798Epoch 1/300\n",
      "92/92 [==============================] - 161s 2s/step - loss: 0.1809 - acc: 0.9341 - val_loss: 0.4184 - val_acc: 0.8793\n",
      "Epoch 182/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1858 - acc: 0.9325Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4191 - acc: 0.8759Epoch 1/300\n",
      "92/92 [==============================] - 185s 2s/step - loss: 0.1859 - acc: 0.9326 - val_loss: 0.4249 - val_acc: 0.8753\n",
      "Epoch 183/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1749 - acc: 0.9362Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4025 - acc: 0.8806Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.1746 - acc: 0.9364 - val_loss: 0.4064 - val_acc: 0.8801\n",
      "Epoch 184/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1781 - acc: 0.9348Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.1780 - acc: 0.9348 - val_loss: 0.4082 - val_acc: 0.8810\n",
      "Epoch 185/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1801 - acc: 0.9345Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4050 - acc: 0.8802Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1801 - acc: 0.9345 - val_loss: 0.4095 - val_acc: 0.8797\n",
      "Epoch 186/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1756 - acc: 0.9359Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.1752 - acc: 0.9360 - val_loss: 0.4509 - val_acc: 0.8700\n",
      "Epoch 187/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1812 - acc: 0.9337Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1811 - acc: 0.9338 - val_loss: 0.4122 - val_acc: 0.8825\n",
      "Epoch 188/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1810 - acc: 0.9339Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3988 - acc: 0.8798\n",
      "92/92 [==============================] - 182s 2s/step - loss: 0.1811 - acc: 0.9338 - val_loss: 0.4038 - val_acc: 0.8792\n",
      "Epoch 189/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1746 - acc: 0.9357Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4076 - acc: 0.8773Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1753 - acc: 0.9355 - val_loss: 0.4116 - val_acc: 0.8768\n",
      "Epoch 190/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1742 - acc: 0.9356Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4025 - acc: 0.8804Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1746 - acc: 0.9355 - val_loss: 0.4064 - val_acc: 0.8800\n",
      "Epoch 191/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1764 - acc: 0.9349Epoch 1/300\n",
      "92/92 [==============================] - 182s 2s/step - loss: 0.1760 - acc: 0.9350 - val_loss: 0.4274 - val_acc: 0.8744\n",
      "Epoch 192/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1761 - acc: 0.9357Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1764 - acc: 0.9355 - val_loss: 0.3987 - val_acc: 0.8799\n",
      "Epoch 193/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1747 - acc: 0.9356Epoch 1/300\n",
      "92/92 [==============================] - 182s 2s/step - loss: 0.1744 - acc: 0.9357 - val_loss: 0.4272 - val_acc: 0.8757\n",
      "Epoch 194/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1772 - acc: 0.9349Epoch 1/300\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.1779 - acc: 0.9347 - val_loss: 0.4115 - val_acc: 0.8767\n",
      "Epoch 195/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1739 - acc: 0.9367Epoch 1/300\n",
      "92/92 [==============================] - 185s 2s/step - loss: 0.1741 - acc: 0.9367 - val_loss: 0.4041 - val_acc: 0.8829\n",
      "Epoch 196/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1834 - acc: 0.9333Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4129 - acc: 0.8806\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.1832 - acc: 0.9334 - val_loss: 0.4188 - val_acc: 0.8800\n",
      "Epoch 197/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1775 - acc: 0.9353Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4007 - acc: 0.8797Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1769 - acc: 0.9355 - val_loss: 0.4064 - val_acc: 0.8791\n",
      "Epoch 198/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1767 - acc: 0.9354Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4182 - acc: 0.8787Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1765 - acc: 0.9354 - val_loss: 0.4231 - val_acc: 0.8783\n",
      "Epoch 199/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1734 - acc: 0.9367Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.1733 - acc: 0.9367 - val_loss: 0.4538 - val_acc: 0.8691\n",
      "Epoch 200/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1711 - acc: 0.9373Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3930 - acc: 0.8846Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1713 - acc: 0.9372 - val_loss: 0.3985 - val_acc: 0.8841\n",
      "Epoch 201/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1727 - acc: 0.9359Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3957 - acc: 0.8836Epoch 1/300\n",
      "92/92 [==============================] - 199s 2s/step - loss: 0.1728 - acc: 0.9359 - val_loss: 0.4014 - val_acc: 0.8830\n",
      "Epoch 202/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1645 - acc: 0.9390Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3977 - acc: 0.8819Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1647 - acc: 0.9390 - val_loss: 0.4018 - val_acc: 0.8814\n",
      "Epoch 203/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1693 - acc: 0.9380Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4168 - acc: 0.8772Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1693 - acc: 0.9380 - val_loss: 0.4207 - val_acc: 0.8767\n",
      "Epoch 204/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1688 - acc: 0.9380Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4622 - acc: 0.8676Epoch 1/300\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.1691 - acc: 0.9379 - val_loss: 0.4663 - val_acc: 0.8671\n",
      "Epoch 205/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1746 - acc: 0.9358Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1750 - acc: 0.9356 - val_loss: 0.4101 - val_acc: 0.8798\n",
      "Epoch 206/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1711 - acc: 0.9363Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4046 - acc: 0.8822Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1708 - acc: 0.9364 - val_loss: 0.4093 - val_acc: 0.8816\n",
      "Epoch 207/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1719 - acc: 0.9366Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4118 - acc: 0.8781Epoch 1/300\n",
      "92/92 [==============================] - 197s 2s/step - loss: 0.1712 - acc: 0.9369 - val_loss: 0.4153 - val_acc: 0.8776\n",
      "Epoch 208/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1691 - acc: 0.9374Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4103 - acc: 0.8807\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1694 - acc: 0.9373 - val_loss: 0.4139 - val_acc: 0.8802\n",
      "Epoch 209/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1728 - acc: 0.9362Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4100 - acc: 0.8787Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1728 - acc: 0.9362 - val_loss: 0.4136 - val_acc: 0.8782\n",
      "Epoch 210/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1685 - acc: 0.9380Epoch 1/300\n",
      "92/92 [==============================] - 198s 2s/step - loss: 0.1679 - acc: 0.9382 - val_loss: 0.4109 - val_acc: 0.8786\n",
      "Epoch 211/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1616 - acc: 0.9399Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4097 - acc: 0.8810\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.1615 - acc: 0.9400 - val_loss: 0.4139 - val_acc: 0.8805\n",
      "Epoch 212/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1654 - acc: 0.9386Epoch 1/300\n",
      "92/92 [==============================] - 237s 3s/step - loss: 0.1651 - acc: 0.9387 - val_loss: 0.4144 - val_acc: 0.8774\n",
      "Epoch 213/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1625 - acc: 0.9398Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4138 - acc: 0.8785\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1627 - acc: 0.9397 - val_loss: 0.4191 - val_acc: 0.8780\n",
      "Epoch 214/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1700 - acc: 0.9370Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4170 - acc: 0.8789Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1696 - acc: 0.9372 - val_loss: 0.4209 - val_acc: 0.8783\n",
      "Epoch 215/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1661 - acc: 0.9385Epoch 1/300\n",
      "92/92 [==============================] - 182s 2s/step - loss: 0.1669 - acc: 0.9382 - val_loss: 0.4192 - val_acc: 0.8787\n",
      "Epoch 216/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1656 - acc: 0.9387Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4049 - acc: 0.8813Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.1661 - acc: 0.9386 - val_loss: 0.4100 - val_acc: 0.8808\n",
      "Epoch 217/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1697 - acc: 0.9368Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1692 - acc: 0.9370 - val_loss: 0.4415 - val_acc: 0.8750\n",
      "Epoch 218/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1714 - acc: 0.9371Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4122 - acc: 0.8779Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.1709 - acc: 0.9373 - val_loss: 0.4145 - val_acc: 0.8774\n",
      "Epoch 219/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1685 - acc: 0.9372Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1682 - acc: 0.9373 - val_loss: 0.4257 - val_acc: 0.8748\n",
      "Epoch 220/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1670 - acc: 0.9390Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4262 - acc: 0.8773Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.1665 - acc: 0.9391 - val_loss: 0.4288 - val_acc: 0.8768\n",
      "Epoch 221/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1696 - acc: 0.9375Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4408 - acc: 0.8739Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1696 - acc: 0.9374 - val_loss: 0.4424 - val_acc: 0.8735\n",
      "Epoch 222/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1619 - acc: 0.9403Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4195 - acc: 0.8814Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1613 - acc: 0.9405 - val_loss: 0.4219 - val_acc: 0.8810\n",
      "Epoch 223/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1626 - acc: 0.9396Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.1627 - acc: 0.9395 - val_loss: 0.4140 - val_acc: 0.8829\n",
      "Epoch 224/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1721 - acc: 0.9364Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1718 - acc: 0.9365 - val_loss: 0.4233 - val_acc: 0.8762\n",
      "Epoch 225/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1694 - acc: 0.9377Epoch 1/300\n",
      "92/92 [==============================] - 207s 2s/step - loss: 0.1690 - acc: 0.9378 - val_loss: 0.4116 - val_acc: 0.8802\n",
      "Epoch 226/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1665 - acc: 0.9384Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4107 - acc: 0.8793Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 0.1666 - acc: 0.9384 - val_loss: 0.4143 - val_acc: 0.8788\n",
      "Epoch 227/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1662 - acc: 0.9383Epoch 1/300\n",
      "92/92 [==============================] - 177s 2s/step - loss: 0.1670 - acc: 0.9380 - val_loss: 0.4273 - val_acc: 0.8770\n",
      "Epoch 228/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1639 - acc: 0.9394Epoch 1/300\n",
      "92/92 [==============================] - 223s 2s/step - loss: 0.1632 - acc: 0.9396 - val_loss: 0.4189 - val_acc: 0.8796\n",
      "Epoch 229/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1661 - acc: 0.9388Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1666 - acc: 0.9387 - val_loss: 0.4489 - val_acc: 0.8722\n",
      "Epoch 230/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1668 - acc: 0.9385Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.1664 - acc: 0.9386 - val_loss: 0.4059 - val_acc: 0.8816\n",
      "Epoch 231/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1638 - acc: 0.9392Epoch 1/300\n",
      "92/92 [==============================] - 196s 2s/step - loss: 0.1631 - acc: 0.9395 - val_loss: 0.4213 - val_acc: 0.8791\n",
      "Epoch 232/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1646 - acc: 0.9390Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4086 - acc: 0.8836Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1645 - acc: 0.9390 - val_loss: 0.4139 - val_acc: 0.8830\n",
      "Epoch 233/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1723 - acc: 0.9365Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4172 - acc: 0.8786\n",
      "92/92 [==============================] - 162s 2s/step - loss: 0.1721 - acc: 0.9366 - val_loss: 0.4224 - val_acc: 0.8781\n",
      "Epoch 234/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1638 - acc: 0.9393Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.1640 - acc: 0.9392 - val_loss: 0.4275 - val_acc: 0.8767\n",
      "Epoch 235/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1622 - acc: 0.9396Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4068 - acc: 0.8825Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1619 - acc: 0.9397 - val_loss: 0.4104 - val_acc: 0.8819\n",
      "Epoch 236/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1572 - acc: 0.9415Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4146 - acc: 0.8815Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1578 - acc: 0.9412 - val_loss: 0.4199 - val_acc: 0.8809\n",
      "Epoch 237/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1592 - acc: 0.9409Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4016 - acc: 0.8824\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.1592 - acc: 0.9409 - val_loss: 0.4061 - val_acc: 0.8818\n",
      "Epoch 238/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1696 - acc: 0.9376Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.1698 - acc: 0.9375 - val_loss: 0.4223 - val_acc: 0.8760\n",
      "Epoch 239/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1607 - acc: 0.9401Epoch 1/300\n",
      "92/92 [==============================] - 219s 2s/step - loss: 0.1602 - acc: 0.9402 - val_loss: 0.4303 - val_acc: 0.8798\n",
      "Epoch 240/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1669 - acc: 0.9384Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.1665 - acc: 0.9386 - val_loss: 0.4026 - val_acc: 0.8823\n",
      "Epoch 241/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1660 - acc: 0.9391Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4140 - acc: 0.8818Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1660 - acc: 0.9391 - val_loss: 0.4179 - val_acc: 0.8813\n",
      "Epoch 242/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1578 - acc: 0.9414Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.1582 - acc: 0.9413 - val_loss: 0.4326 - val_acc: 0.8778\n",
      "Epoch 243/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1593 - acc: 0.9410Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1586 - acc: 0.9412 - val_loss: 0.4068 - val_acc: 0.8829\n",
      "Epoch 244/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1581 - acc: 0.9414Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4100 - acc: 0.8811Epoch 1/300\n",
      "92/92 [==============================] - 161s 2s/step - loss: 0.1586 - acc: 0.9412 - val_loss: 0.4142 - val_acc: 0.8806\n",
      "Epoch 245/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1590 - acc: 0.9407Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.1595 - acc: 0.9405 - val_loss: 0.4318 - val_acc: 0.8765\n",
      "Epoch 246/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1647 - acc: 0.9388Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4053 - acc: 0.8832Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1648 - acc: 0.9388 - val_loss: 0.4096 - val_acc: 0.8826\n",
      "Epoch 247/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1648 - acc: 0.9391Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4312 - acc: 0.8784Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1651 - acc: 0.9390 - val_loss: 0.4356 - val_acc: 0.8779\n",
      "Epoch 248/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1618 - acc: 0.9405Epoch 1/300\n",
      "92/92 [==============================] - 196s 2s/step - loss: 0.1616 - acc: 0.9406 - val_loss: 0.4419 - val_acc: 0.8758\n",
      "Epoch 249/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1646 - acc: 0.9392Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4186 - acc: 0.8826Epoch 1/300\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.1659 - acc: 0.9389 - val_loss: 0.4237 - val_acc: 0.8821\n",
      "Epoch 250/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1630 - acc: 0.9397Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4205 - acc: 0.8787\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.1631 - acc: 0.9397 - val_loss: 0.4257 - val_acc: 0.8781\n",
      "Epoch 251/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1629 - acc: 0.9396Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4250 - acc: 0.8800Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1626 - acc: 0.9397 - val_loss: 0.4308 - val_acc: 0.8794\n",
      "Epoch 252/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1660 - acc: 0.9384Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4548 - acc: 0.8723Epoch 1/300\n",
      "92/92 [==============================] - 187s 2s/step - loss: 0.1663 - acc: 0.9383 - val_loss: 0.4591 - val_acc: 0.8718\n",
      "Epoch 253/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1662 - acc: 0.9386Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.1667 - acc: 0.9385 - val_loss: 0.4273 - val_acc: 0.8786\n",
      "Epoch 254/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1643 - acc: 0.9393Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4217 - acc: 0.8782Epoch 1/300\n",
      "92/92 [==============================] - 163s 2s/step - loss: 0.1637 - acc: 0.9395 - val_loss: 0.4264 - val_acc: 0.8777\n",
      "Epoch 255/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1578 - acc: 0.9414Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4104 - acc: 0.8813Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1578 - acc: 0.9414 - val_loss: 0.4147 - val_acc: 0.8808\n",
      "Epoch 256/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1573 - acc: 0.9414Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3950 - acc: 0.8807\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1569 - acc: 0.9416 - val_loss: 0.3987 - val_acc: 0.8802\n",
      "Epoch 257/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1518 - acc: 0.9438Epoch 1/300\n",
      "92/92 [==============================] - 197s 2s/step - loss: 0.1518 - acc: 0.9438 - val_loss: 0.4105 - val_acc: 0.8821\n",
      "Epoch 258/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1570 - acc: 0.9417Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4265 - acc: 0.8787Epoch 1/300\n",
      "92/92 [==============================] - 228s 2s/step - loss: 0.1570 - acc: 0.9416 - val_loss: 0.4327 - val_acc: 0.8781\n",
      "Epoch 259/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1562 - acc: 0.9417Epoch 1/300\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1559 - acc: 0.9418 - val_loss: 0.4150 - val_acc: 0.8813\n",
      "Epoch 260/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1548 - acc: 0.9423Epoch 1/300\n",
      "92/92 [==============================] - 189s 2s/step - loss: 0.1545 - acc: 0.9424 - val_loss: 0.4226 - val_acc: 0.8800\n",
      "Epoch 261/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1665 - acc: 0.9385Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.1664 - acc: 0.9385 - val_loss: 0.4250 - val_acc: 0.8826\n",
      "Epoch 262/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1558 - acc: 0.9423Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4298 - acc: 0.8815Epoch 1/300\n",
      "92/92 [==============================] - 193s 2s/step - loss: 0.1551 - acc: 0.9426 - val_loss: 0.4357 - val_acc: 0.8809\n",
      "Epoch 263/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1587 - acc: 0.9409Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4212 - acc: 0.8780Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1588 - acc: 0.9408 - val_loss: 0.4262 - val_acc: 0.8774\n",
      "Epoch 264/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1619 - acc: 0.9397Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4268 - acc: 0.8766Epoch 1/300\n",
      "92/92 [==============================] - 237s 3s/step - loss: 0.1618 - acc: 0.9398 - val_loss: 0.4296 - val_acc: 0.8762\n",
      "Epoch 265/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1539 - acc: 0.9426Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1540 - acc: 0.9425 - val_loss: 0.4258 - val_acc: 0.8785\n",
      "Epoch 266/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1559 - acc: 0.9419Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4019 - acc: 0.8852Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.1557 - acc: 0.9419 - val_loss: 0.4060 - val_acc: 0.8847\n",
      "Epoch 267/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1499 - acc: 0.9443Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4206 - acc: 0.8799\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.1500 - acc: 0.9443 - val_loss: 0.4250 - val_acc: 0.8794\n",
      "Epoch 268/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1590 - acc: 0.9403Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4210 - acc: 0.8780Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1591 - acc: 0.9402 - val_loss: 0.4256 - val_acc: 0.8775\n",
      "Epoch 269/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1526 - acc: 0.9431Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4557 - acc: 0.8710Epoch 1/300\n",
      "92/92 [==============================] - 190s 2s/step - loss: 0.1525 - acc: 0.9432 - val_loss: 0.4614 - val_acc: 0.8705\n",
      "Epoch 270/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1566 - acc: 0.9417Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4337 - acc: 0.8784Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1574 - acc: 0.9414 - val_loss: 0.4389 - val_acc: 0.8779\n",
      "Epoch 271/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1483 - acc: 0.9443Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4180 - acc: 0.8806Epoch 1/300\n",
      "92/92 [==============================] - 204s 2s/step - loss: 0.1480 - acc: 0.9444 - val_loss: 0.4205 - val_acc: 0.8801\n",
      "Epoch 272/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1590 - acc: 0.9411Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4257 - acc: 0.8808Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.1587 - acc: 0.9412 - val_loss: 0.4304 - val_acc: 0.8802\n",
      "Epoch 273/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1526 - acc: 0.9431Epoch 1/300\n",
      "92/92 [==============================] - 165s 2s/step - loss: 0.1528 - acc: 0.9430 - val_loss: 0.4249 - val_acc: 0.8803\n",
      "Epoch 274/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1537 - acc: 0.9431Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4074 - acc: 0.8812\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1541 - acc: 0.9429 - val_loss: 0.4107 - val_acc: 0.8808\n",
      "Epoch 275/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1511 - acc: 0.9434Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4147 - acc: 0.8783Epoch 1/300\n",
      "92/92 [==============================] - 203s 2s/step - loss: 0.1508 - acc: 0.9435 - val_loss: 0.4185 - val_acc: 0.8777\n",
      "Epoch 276/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1489 - acc: 0.9445Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4245 - acc: 0.8787Epoch 1/300\n",
      "92/92 [==============================] - 172s 2s/step - loss: 0.1491 - acc: 0.9444 - val_loss: 0.4285 - val_acc: 0.8782\n",
      "Epoch 277/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1572 - acc: 0.9416Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4058 - acc: 0.8809\n",
      "92/92 [==============================] - 193s 2s/step - loss: 0.1581 - acc: 0.9413 - val_loss: 0.4088 - val_acc: 0.8803\n",
      "Epoch 278/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1600 - acc: 0.9408Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.1604 - acc: 0.9406 - val_loss: 0.4218 - val_acc: 0.8762\n",
      "Epoch 279/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1591 - acc: 0.9407Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 0.1593 - acc: 0.9406 - val_loss: 0.4356 - val_acc: 0.8778\n",
      "Epoch 280/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1576 - acc: 0.9411Epoch 1/300\n",
      "92/92 [==============================] - 159s 2s/step - loss: 0.1571 - acc: 0.9413 - val_loss: 0.4332 - val_acc: 0.8749\n",
      "Epoch 281/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1568 - acc: 0.9419Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4282 - acc: 0.8790Epoch 1/300\n",
      "92/92 [==============================] - 182s 2s/step - loss: 0.1566 - acc: 0.9419 - val_loss: 0.4322 - val_acc: 0.8785\n",
      "Epoch 282/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1512 - acc: 0.9433Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4672 - acc: 0.8718Epoch 1/300\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.1508 - acc: 0.9435 - val_loss: 0.4699 - val_acc: 0.8713\n",
      "Epoch 283/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1518 - acc: 0.9433Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4356 - acc: 0.8785Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 0.1519 - acc: 0.9433 - val_loss: 0.4390 - val_acc: 0.8781\n",
      "Epoch 284/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1535 - acc: 0.9425Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4242 - acc: 0.8769Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1537 - acc: 0.9424 - val_loss: 0.4280 - val_acc: 0.8764\n",
      "Epoch 285/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1523 - acc: 0.9429Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4195 - acc: 0.8814Epoch 1/300\n",
      "92/92 [==============================] - 235s 3s/step - loss: 0.1524 - acc: 0.9428 - val_loss: 0.4224 - val_acc: 0.8809\n",
      "Epoch 286/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1494 - acc: 0.9443Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4425 - acc: 0.8764Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.1494 - acc: 0.9443 - val_loss: 0.4455 - val_acc: 0.8759\n",
      "Epoch 287/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1530 - acc: 0.9431Epoch 1/300\n",
      "92/92 [==============================] - 166s 2s/step - loss: 0.1528 - acc: 0.9431 - val_loss: 0.4141 - val_acc: 0.8796\n",
      "Epoch 288/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1497 - acc: 0.9441Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4627 - acc: 0.8719Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.1500 - acc: 0.9440 - val_loss: 0.4658 - val_acc: 0.8714\n",
      "Epoch 289/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1556 - acc: 0.9419Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4472 - acc: 0.8726Epoch 1/300\n",
      "92/92 [==============================] - 195s 2s/step - loss: 0.1565 - acc: 0.9414 - val_loss: 0.4501 - val_acc: 0.8721\n",
      "Epoch 290/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1501 - acc: 0.9441Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1500 - acc: 0.9441 - val_loss: 0.4291 - val_acc: 0.8792\n",
      "Epoch 291/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1515 - acc: 0.9437Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4375 - acc: 0.8790Epoch 1/300\n",
      "92/92 [==============================] - 161s 2s/step - loss: 0.1510 - acc: 0.9439 - val_loss: 0.4410 - val_acc: 0.8785\n",
      "Epoch 292/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1566 - acc: 0.9417Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4340 - acc: 0.8760Epoch 1/300\n",
      "92/92 [==============================] - 169s 2s/step - loss: 0.1565 - acc: 0.9418 - val_loss: 0.4372 - val_acc: 0.8755\n",
      "Epoch 293/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1603 - acc: 0.9407Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4118 - acc: 0.8829Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1601 - acc: 0.9409 - val_loss: 0.4160 - val_acc: 0.8823\n",
      "Epoch 294/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1548 - acc: 0.9426Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4292 - acc: 0.8795Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.1551 - acc: 0.9425 - val_loss: 0.4325 - val_acc: 0.8789\n",
      "Epoch 295/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1538 - acc: 0.9426Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4398 - acc: 0.8757Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.1544 - acc: 0.9424 - val_loss: 0.4427 - val_acc: 0.8752\n",
      "Epoch 296/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1546 - acc: 0.9421Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4271 - acc: 0.8811Epoch 1/300\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.1543 - acc: 0.9422 - val_loss: 0.4324 - val_acc: 0.8806\n",
      "Epoch 297/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1533 - acc: 0.9429Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4301 - acc: 0.8786Epoch 1/300\n",
      "92/92 [==============================] - 167s 2s/step - loss: 0.1530 - acc: 0.9430 - val_loss: 0.4355 - val_acc: 0.8782\n",
      "Epoch 298/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1508 - acc: 0.9443Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4453 - acc: 0.8759Epoch 1/300\n",
      "92/92 [==============================] - 164s 2s/step - loss: 0.1505 - acc: 0.9445 - val_loss: 0.4498 - val_acc: 0.8754\n",
      "Epoch 299/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1535 - acc: 0.9426Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4454 - acc: 0.8757Epoch 1/300\n",
      "92/92 [==============================] - 177s 2s/step - loss: 0.1532 - acc: 0.9427 - val_loss: 0.4484 - val_acc: 0.8752\n",
      "Epoch 300/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.1528 - acc: 0.9428Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 0.1530 - acc: 0.9428 - val_loss: 0.4088 - val_acc: 0.8811\n"
     ]
    }
   ],
   "source": [
    "#hist = model.fit_generator(data_gen, validation_data=(valid_x, valid_y), epochs=par.n_epochs, steps_per_epoch=par.n_batch, callbacks=[cp_cb])\n",
    "hist = model.fit_generator(train_data_gen,\n",
    "                           epochs=n_epochs,\n",
    "                           steps_per_epoch=len(train_data_gen),\n",
    "                           validation_data=valid_data_gen,\n",
    "                           validation_steps=len(valid_data_gen),\n",
    "                           shuffle = True,\n",
    "                           workers=8,\n",
    "                           use_multiprocessing=True,\n",
    "                           callbacks=[cp_cb])\n",
    "#hist = model.fit_generator(data_gen, epochs=par.n_epochs, steps_per_epoch=par.n_batch, workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGAAAAI/CAYAAAA4Bq+xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3RU1drH8e+ZkkZCek8gEErooTfpCIggiAUUUeyCDb36ehXrVe69NlQsIBcVERFQRBAQBek9hBZqIAmQQnrvycx5/9gpBBKKEgLh+azFSmbmnDP7lITs3zx7H03XdYQQQgghhBBCCCFE7THUdQOEEEIIIYQQQggh6jsJYIQQQgghhBBCCCFqmQQwQgghhBBCCCGEELVMAhghhBBCCCGEEEKIWiYBjBBCCCGEEEIIIUQtkwBGCCGEEEIIIYQQopaZ6uqNPTw89KCgoLp6eyGEEEJcovDw8FRd1z3ruh03OvnbSQghhLj2XejvpjoLYIKCgti9e3ddvb0QQgghLpGmaafqug1C/nYSQgghrgcX+rtJhiAJIYQQQgghhBBC1DIJYIQQQgghhBBCCCFqmQQwQgghhBBCCCGEELWszuaAEUIIIa6GkpIS4uLiKCwsrOumXPPs7OwICAjAbDbXdVPEJZLr+8LkmhZCCHEtkQBGCCFEvRYXF4eTkxNBQUFomlbXzblm6bpOWloacXFxNGnSpK6bIy6RXN81k2taCCHEtUaGIAkhhKjXCgsLcXd3l87pRWiahru7u1RSXGfk+q6ZXNNCCCGuNRLACCGEqPekc3pp5Dhdn+S81UyOjRBCiGuJBDBCCCFELXN0dKzrJgghhBBCiDomAYwQQgghhBBCCCFELZMARgghhLhKdF3nxRdfpG3btrRr145FixYBcObMGfr27UtoaCht27Zl8+bNWCwWJk6cWLHsRx99VMetF6Jmo0ePpnPnzrRp04bZs2cDsHr1ajp16kSHDh0YNGgQALm5uTz44IO0a9eO9u3bs2TJkrpsthBCCHFVyV2QhBBCiKvk559/Zt++fezfv5/U1FS6du1K3759WbBgAUOHDmXq1KlYLBby8/PZt28f8fHxHDx4EIDMzMw6br0QNfv6669xc3OjoKCArl27MmrUKB599FE2bdpEkyZNSE9PB+Dtt9/G2dmZiIgIADIyMuqy2UIIIcRVJQGMEEKIG8Zbvx7icEL2Fd1ma7+GvDGyzSUtu2XLFu655x6MRiPe3t7069ePsLAwunbtykMPPURJSQmjR48mNDSUpk2bEh0dzdNPP82tt97KkCFDrmi7Rf1Tl9f3jBkzWLp0KQCxsbHMnj2bvn37Vtz+2c3NDYC1a9eycOHCivVcXV2vaHuFEEKIa5kMQRJCCCGuEl3Xq32+b9++bNq0CX9/fyZMmMC8efNwdXVl//799O/fn88//5xHHnnkKrdWiEuzYcMG1q5dy/bt29m/fz8dO3akQ4cO1d6BSNd1uTOREEKIG5ZUwAghhLhhXGqlSm3p27cvX375JQ888ADp6els2rSJ999/n1OnTuHv78+jjz5KXl4ee/bsYfjw4djY2HDHHXcQHBzMxIkT67Tt4tpXV9d3VlYWrq6uODg4cPToUXbs2EFRUREbN24kJiamYgiSm5sbQ4YM4bPPPuPjjz8G1BAkqYIRQghxo5AARgghhLhKbr/9drZv315RHfDee+/h4+PDt99+y/vvv4/ZbMbR0ZF58+YRHx/Pgw8+iNVqBeA///lPHbdeiOoNGzaMWbNm0b59e1q2bEmPHj3w9PRk9uzZjBkzBqvVipeXF2vWrOHVV1/lySefpG3bthiNRt544w3GjBlT17sghBBCXBVaTeXQta1Lly767t276+S9hRBC3DiOHDlCq1at6roZ143qjpemaeG6rnepoyaJMtX97STX98XJMRJCCHE1XejvJpkDRgghhBBCCCGEEKKWSQAjhBBCCCGEEEIIUcskgBFCCCGEEEIIIYSoZRLACCGEEEIIIYQQQtQyCWCEEEIIIYQQQgghapkEMEIIIYQQQgghhBC1rH4FMKnH4bOucOLPum6JEEIIIYQQQghxTbJY9b+0XkGxhX+vOkJSdmGV57MKSnhywR5iUvPOWyevqJRpKw8z/Y9jf+k9Vx9MZHFYLLquk5xTSHKOeu8jZ7JZEh7Hwfisi27DatV5Z8Vh/vvbUXT9r+37lWCqs3euDboVUiOhMLOuWyKEEEL8ZY6OjuTm5lb72smTJxkxYgQHDx68yq0S4u+70LUthBDi6jiWmMM9/9vBK8NbcWfngMtad9bGKGZvisbGaOAfQ1oQdjKDDoHOzN9xipUHzuBoY+LdO9tzMjUPi66z93Qmn/wZSWx6AZoGd3YOpJG7Q7Xbnr/jFBuOpdDIzYGHbgoiwNWBFQcSePqHveg6rDp4hp3R6bg72vDbs314eG4YCVkqjHnvzvbc3SWw2u3qus47K4/w9dYYAJp6Nqhx2dpWvwIYQ9nuWErqth1CCCGEEEIIIcQ1xmrVeWVpBOl5xfzr10P0a+GJk52JFQfOYGc2MKK9H+uPJnP4TDaT+wejaRqFJRbeW32Mpp4N+HJTFACrIs7QsZELD3+7m1vb+7IrJh1Ng2X742kb4Mxrv1R+UBTi48Sn93TkuUX7mLf9JK+OaH1euxaFnebVXw7i72LPpuMp/Lg7lo6NXdl6IpUujV3p1MiVLzdF0yHQhf2xmUz4ahcJWYV8eFcHFoXFMm3lEQa09MLTyZbFu2PZdiIVPxd7fF3s+S3iDNui0pjYK4jjyTlMXRrBu78dpUOgC19P7Hq1Dj1Q3wIYo436KgGMEEKIa8hLL71E48aNmTx5MgBvvvkmmqaxadMmMjIyKCkp4Z133mHUqFGXtd3CwkImTZrE7t27MZlMTJ8+nQEDBnDo0CEefPBBiouLsVqtLFmyBD8/P+6++27i4uKwWCy89tprjB07tjZ2V9xAruS1nZuby6hRo6pdb968eXzwwQdomkb79u357rvvSEpK4oknniA6OhqAmTNn0qtXr9rbWSGEuILyiko5lZZPiI8TBoNW7TIFxRa2R6diNBiwMRoIcLUn0K366pGL0XWd1QcT+e1gIuGnMnhqQDNmb4rm9i+2kl1QQnZhKTYmA31bePLOysNEpeTRwMbIxN5N+H7n6YrqEVuTgcn9g/liQxRv/XoYs1Fj5YEzAEwd3oppq47w2i8H6djIhft7NsbD0ZabmnmgaRp/HE5i0e5Ynh3cHCc7MwAxqXnM3HCCJXvi6dPcg68ndiUxq5BXlkZwKi2Ph3oH8dTA5jjbm3n4piZ4Otkyfs5OtkWl0TXIlTGd/OkQ6MLwTzYzZdFebuvgx0tLInBrYEN2QQmlVh1nezP/GtWG+7o3Ji2vmBl/Hsei64T4OP2lY/l31LMARp1ELMV12w4hhBDXpt/+CYkRV3abPu3glv9ecJFx48YxZcqUik7q4sWLWb16Nc899xwNGzYkNTWVHj16cNttt6Fp1f8RVp3PP/8cgIiICI4ePcqQIUOIjIxk1qxZPPvss4wfP57i4mIsFgurVq3Cz8+PlStXApCVdfHx0uI6UwfX95W8tu3s7Fi6dOl56x0+fJhp06axdetWPDw8SE9PB+CZZ56hX79+LF26FIvFIkObhBC1JjW3iANxmQxo6XVZ/0/XJL+4lHv/t4P9cVm4NbBh5vhOdG/qTmGJBTuzEVCBycPfhrEtKq1iPU2DUR38eG1Ea9wdbQE4kZzD6fR8BoZ4X/A9f9gVyytLI2hgY2Rc10D+MaQFHo42LN+fQJ/mHjTzcuLtFYf59M/jRKXk4eFowzsrj+DhZMuXG6Po0dSNpwY0x2TUaOblyKyNUZxOz+fVW1txMD6L1NxiHunThDWHkzhyJpsZ4zqeFxY91DuIFQcSGPThRoa19SEpu5A1h5MwGw3c170RLw4LwWw0EOjmwHcPdz9vH7wa2gHw8i2tuP/rnfzfsBA0TbXn7dFteO2XQ2w9kUaHQBcWPdYDs9FAUnYhzvZmGtiq6MPTyZa3R7f96yfvb6pnAUxZBYy1tG7bIYQQQpylY8eOJCcnk5CQQEpKCq6urvj6+vLcc8+xadMmDAYD8fHxJCUl4ePjc8nb3bJlC08//TQAISEhNG7cmMjISHr27Mm0adOIi4tjzJgxNG/enHbt2vHCCy/w0ksvMWLECPr06VNbuytuIFfy2tZ1nVdeeeW89datW8edd96Jh4cHAG5ubgCsW7eOefPmAWA0GnF2dq7dnRVCXJfyi0t5duE++rbw5N5ujVi6N56ewe74u9hf8jZe++Ugvx1MZGCIFx/e1QHXBjbnLXM6LZ+9sRmMbO9XUdFyNDEbD0dbPMrCEoDjSTn8a8VhIuKzeP7mFny/8xQz1h3nXVd7bvl4M/d2b8TLw1uxfH8C26LS+MfNLejVzJ2iUisbI1P4ZutJ4jMLWPBoDxKzChn75Q7S8oqZdV8nbEwGNh5LwWDQmNgriMbuDQA158tbvx6iT3MP5j7YDWNZ+yb2bsLE3k0A9Tt47rYY5myJwaDB4sd78uzCfTy1YC8AH48LpVewR8V+9GjqzsH4LMZ1a4SjrQld19E0jS8ndCavuJQA1/MrdTo2cmXRYz35aE0kS/fG08DGxKN9mvJwnyZ4Odld8vloF+DM3teHVHlubNdGdGzkyvc7TjGpf7OKIMvvMs7z1VC/ApiKOWCkAkYIIUQ1LlKpUpvuvPNOfvrpJxITExk3bhzff/89KSkphIeHYzabCQoKorCw8OIbOktNs/jfe++9dO/enZUrVzJ06FDmzJnDwIEDCQ8PZ9WqVbz88ssMGTKE119//UrsmrhW1NH1faWu7ZrWK/+jXgghIuKymL/jFFNubo6v88U71rqu88rPEaw5nMSaw0l8u+0kJ5Jz6d7EjYWP9UDTNBIyC8gtKqWFd9XhKPN3nGLZvnheH9GG3w8l0jXIlc3HU3h39VH+e0d7QM2nUmyxUlhiYfxXO4hNL2BRWCwj2vuxMTKZ3w8lYdDglna+fDquI19tiWHaqiPYGA38+/Z2jOvWCIDpayJ57ZeD5BSV8uWmaLILS/n9UCLtA5yZPKBZRWDSK9iD1r4NeXbhPiZ8tZO4jAJKrTpt/Bry5IK9WKw6DWyMlFh0fj+YyLyHu3EyNZ8XftqPk52ZD+/uULGtc2maxsj2fnyxIYpezdxp6unIj0/0ZNrKI+QXW+jZ1L3K8u/d2Z7sglIcyypLyn9PuzawqTagKtetiRs/PNbjoufur2jh7cRbo+quuuVS1K8ARuaAEUIIcY0aN24cjz76KKmpqWzcuJHFixfj5eWF2Wxm/fr1nDp16rK32bdvX77//nsGDhxIZGQkp0+fpmXLlkRHR9O0aVOeeeYZoqOjOXDgACEhIbi5uXHffffh6OjI3Llzr/xOihvSlbq2s7Kyql1v0KBB3H777Tz33HO4u7uTnp6Om5sbgwYNYubMmUyZMgWLxUJeXh4NGzaszV0VQtSx//x2hG1Raaw+lMiQ1t609mvIxF5BNYa0i3fH8su+BJ4Z1JyY1DzWHE7k1na+rIw4w5zNMSzbH8/B+GwMGsx9sBt9W3gCsOd0Bm8uP0SpVefuL7dj0DRm3NORGX+eYMmeOF4c2pLVhxL536Zo4jIK8HSyJS23mKcHNuOrLTFsi0rDzmzg+ZtbkJZbxLfbT9Ex0IWP10bSr4Un0+/uUDGE6O4ugXy8NpL1x1K4u0sAp9Pz+WHXaTo2cuG9O9qfF5iMCvUnOiWPBbtOE+Bqz0djQ2nk5sCUsiqfR/o0ITIph3GzdzB4+iZATYI7677OF60yGd3Rn1kboxgV6g+AndlY43CdAFcHcL30cyeUehbAlM8BIwGMEEKIa0ubNm3IycnB398fX19fxo8fz8iRI+nSpQuhoaGEhIRc9jYnT57ME088Qbt27TCZTMydOxdbW1sWLVrE/PnzMZvN+Pj48PrrrxMWFsaLL76IwWDAbDYzc+bMWthLcSO6Utd2Teu1adOGqVOn0q9fP4xGIx07dmTu3Ll88sknPPbYY3z11VcYjUZmzpxJz549a3NXhRC1JLeoFAez8bzJaE+m5rEwLJZTaXlM6NGYbVFpTOjRmJjUPDZEpvBjeBxeTnbc2t6XwhILmyJTWHHgDBn5xTzRL5i3VxyhZ1N3pgxqjqZBYYkVk1HjUEIW01YdwcPRlldvbcVP4XE8uWAPnRu7cjwpl7S8Inxd7Li/RxDTVh3htg5++Drb81DvIH7YdZqxs3dwIjmX0EAXBoZ4s/VEKi8MackdnQN4ckAzMvKLaWBroqGdGatVZ29sJu+sPIJBo8r8LQA+znYMDPFmw7Fknh7YHE8nW1Jzi6odwlPuuZtb8NzNLao8d3ZVSRs/Z5ZM6sXGYyl4O9sxuJUXDjYX7/q38HZiwwsDCHS7tobt1CdaTeXLta1Lly767t27r/yG33KFPv+Aga9e+W0LIYS47hw5coRWrVrVdTOuG9UdL03TwnVd71JHTRJlqvvbSa7vi5NjJMTfo+s6czbH0Ma/YZU5QADeXH6IYC9HJvRoDEBabhE7otMZ3s7nokMHi0ot2JqMnMkqYPgnmxnd0Z83RrapeH390WSemB9OqVXHqGmUWq3YmAzseHkQLg42WKw6t87YTG5RKWM6BfDNlhhyikpxdTCjA5n5JTjYGPl9St/zJoPdGZ3Gj+Fx/N/Qlng1tCMuI5+xX+7A3sZIO39nGtgamdgriGZeTqw7mkTHQNeKYTX3f72LTZEpTOwVxBsjW1/SEMkd0WmMm72DOzoF8OHdHc57/UxWAafS8ulxzjAfcX260N9N9asCBsBgljlghBBCCCGEEDektYeT8GpoS/sAlyuyvW+3nWTaqiP4Otux4cX+2JrU5KYnknOYu+0kAK4OZvq28GT8nJ0cTczhk3GhjAr1p7DEwpSF+ziWlIOLg5m+zT2JSsllY2QKOYWl9GnugVXXycgvYVFYLM8Oas5bvx7myJlsolJyaeHtxJwHuhCdkscj3+5mbNdAXBxUEGI0aLw2ojXj5+xkxp/HGd7Oh7FdG9Er2L3iNsZ3dAqo9rbN3Zu60/2ssCPA1YGt/xxY7f6fe3ehaaPbsismnTGd/C95fqoeTd358YmetPGrfpikr7P9Jc1pI65/9S+AMdrIECQhhBDXvYiICCZMmFDlOVtbW3bu3FlHLRLiypBrW4jac+RMNo/PD6eVrxMrnq56t7viUismg3beMJ8LCTuZzjsrj9DS24ljSTn8FB5H58aueDvZ8cveBAyaGu7y1IK9ONqaKCyx0NjdgWkrjzAwxIs3lx9m9aFEhrXxISmnkE/+PI6zvZlb2/nibG/m2+0nKSyxclfnAH4Mj2P8nJ0cSsimT3MPQgNdeHl4K5ztzfg627Pj5UE42lXtvvZu5sG7d7Qj0M2hSnVOTbcxvhIC3RyqDXUupmuQWy20Rlxv6mEAY5IARgghxHWvXbt27Nu3r66bIcQVJ9e2EEr5Hb7iMwt4f/VR0vNLGNHel7u7BF72tpbvTyA1p4hl++KxWHUOxmdzIjmXZl6O6LrOL/vieWeFmvPkvTvb0yGw+uqYn/fE4ediT4+m7iRnFzL5+z0EuNqz+ImeTPxmF28uP0SJRSfYswGFJVZ6N/Pgs3s6sWDXaY4n53BrO188HG0Z/cVW2r/1B7oOzw5qXjFfSUZeMQ62xooqmjs6B7C9bF6Xo4k5RMRncUtbH2be1/m8tjk7mKtt89iujS77eAlRV+phAGMjQ5CEEEJUIbexvTR1NS+c+Hvk+q6ZXNPiWrUxMoUXf9zPO6Pb8r/N0RyMz8bFwcwby9Lp39LzonerOduaw0k888Peisev3tqKaauOsHxfPMFejny67gQnknPpEOhCUlYhd8zcxpJJvWjn70xsRj5+LvaYjQZ2RKfx/OL92JgMvDGyNfN3nCa3sJT5D3fH2d7M1OGt+M9vR+nZ1J3Zm6MpLrXy/M0tcHYwM6l/cJU2fXlfZw7EZeHnYs+4rpWB0rm3J27h7VRx++dJ/YP54PdjvDWqDULUV/UzgLGW1nUrhBBCXCPs7OxIS0vD3d1dOqkXoOs6aWlp2Nld+h/9ou7J9V0zuabFtSq7sISXfjpAck4Rj30XDsCHd3Wgc2NXBk/fyGfrTvCvUW05kZzD8v1nmNw/GDuzsdptRSbl8PyifbT1b8iMcR1JySmiWxM31h9LZtYmFZK09m3Ix2NDua2DH9mFJQz9eBMv/xxBc29Hlu1LwNHWxMAQLw7EZRLoZo+92cjUpQdxb2DD5+M70tJHBSRdgtxYMqkXAB0bubAwLJZhbX2qbdeQNj4MaVP9azUZ3s6X4e18L2sdIa439S+AMZikAkYIIUSFgIAA4uLiSElJqeumXPPs7OwICAio62aIyyDX94XJNS2uluzCEuZsimZ8j8Z4N1Sh36PzdmNjMvD5vZ0qlrNadd5YdojknEK+e7gbX6yPwt/VvmJC17FdA1mw8zRnsgrZfDyFwhIrno42jO7oz+fro/jzSBIejrY80Ksx9jYm/rF4H3Y2Rmbd15kAVweaejoCcHeXQLaeSOOJfsG8OLQlxrJ5X1wcbHhzZBsmfb+Hw2eymdgriMISC78dTCSroITvH+lOc29HfotIZHRHf5ztqx/2M6iVN4NaeVf7mhCiZvUvgJFJeIUQQpzFbDbTpEmTum6GELVCrm8h/p7swhKm/xFJa7+GDGntjZ3ZyIs/HSCroITH+jQlIbMAJzsTN7f2xmQ01LidxWGxzFh3giV74vn2oW5YdZ01h5MAGNI6nu93nia7oASvhnZsikzhucEt6NPckz7NPats58WhLSm16Gw5kUqPpu4kZRfxzdaT7D2dyS/74ukV7EFUSi5PzN8DgJeTLT881oMA16qTwo4K9adnsHu1Q5mGtfVhcv9gfF3sK24f/eZtbUjMKiTIowEAD/QK+svHVAhRs3oYwJglgBFCCCGEEEJc1Pwdpypupfy62UCAqwNRKbk425u576vKO3MFuNoz/+HuFQHFnM3R/HEoCRuTgQ/u6sDvhxIJcLWnsMTC/V/tpGsTN2yMBnxd7Hh24T5MBo1mXo5sikzhleEhPNY3uLrm4OJgw7t3tq94vGxfPM8u3Ed0ah5PDWjGC0NbUlxqZc/pDHILS+kQ6IKnk22126ppHhlN0/i/YSFVnrMzGyv2TQhRe+pnAGOVAEYIIYQQQghRs1KLle93nKZnU3deHh7C/B2nWHskmY/HhjIwxIsNx1Jo6ePEqbR8nl+0j9eWHWTeQ93IzC/h/d+P4etsR0JmIVOXRrD7VAZTBrVgYIgXd8zaxrJ9CYwO9ePuLoE8u2gf74xuy5DW3mQVlODiYHPxxpW5pa0v7zofpYGtiacHNQPAxmSgR1P32josQohaVP8CGINZ5oARQgghhBBCVFFcauVfKw7hZGfmpWEh/Hk0mfjMAl4b0Zr2AS68d2fVWzOP7OAHqDv1vDC0JW8sP8Ty/QkkZhVSVGrlywld+HF3LHO2xABqaE9LHyfeGdWWN5YfYmLvJoQGurDrlUEVk2RfTvgCKmxZ+mRv7G0qb90shLh+1b8AxmgDFrkLkhBCCCGEEPWBxapXTCJ7MaUWK2uPJNOvhSf2NiqwWLYvng3HUojPKGDXyXQ0De7o5M/MDVH4OdsxuJXXRbd7X4/G/Lw3nhd/PEADWyPdmrjR0seJJwc0Y9HuWDwdbWnhXTYBbtdAbgv1q7hz0d+9Q1n5pL5CiOtfzTNJXa+MchckIYQQQggh6oMjZ7Lp/M4aFofFVjyXkFnAtqhU8ovVh665RaVsPp5CicXKvO2neGJ+OE//sBeLVUfXdd5bfYxVEWc4ciabl28JwdZk4IGvw9gXm8nzQ1pecHLdckaDxrcPdqVrE1cy8kt4oGcQAK4NbPjqga5MHxtaJWip6bbRQogbW/2sgLFm1XUrhBBCCCGEEDXQdR2ovjrEatXZEZNGp0aufLb+BJn5JUz9JYLG7g50b+rOswv3EnYyA6NBw8/FjtScYgpKLAxr48P26DR8ne1YeySJ91Yf5a4ugcRnFjDt9rbc260RmqZxJquQudtO0ivYnTs6+V9ym10cbPj2wW5ExGcRGlg5XKlbE7e/f0CEEDeE+hfAGOQuSEIIIYQQQlxNsen5ZOaX0C7A+ZKWf+vXwxyMz2L+I93ZF5tJfnEpA0O8Afhiwwk++COSLo1d2XM6g/HdG7E9Ko0XftrP4sd7EnYygzGd/AlwsScmLZ+Gdiac7MzM2hiFQYNFj/dgzuYYvtl6EpNRBTx9m3tWhD2T+weTVVDCc4NbXPbwIJPRQMdGrpe1jhBClKtXAUxsej5nYnNoZy7Evq4bI4QQQgghxA3iH4v3E5mcw+6pg6sM6Vm6N474jAKeHNCsIuyITc/nux2nsFh1Hpobxq6YdGxNBsJeHUzYyQw+XBNJhwBnwk9nYDYaeHZwc3o382Dy93t4+ecIQIUozbycqrQhwNUeq64T4tOQSf2D+Sk8jlkbo2ni0YBAN4eK5bwa2vHR2NCrcFSEEKKqehXAFJZYSMgppU1DmQNGCCGEEEKI2vS/TdEciM/i+ZtbsOtkOgC7TqbTK9gDgO93nmLq0oMApOYW8/qI1hgMGl9ticGgwe2dA/gpPA5/F3viMwtYeeAMn647QTNPR354rAc7o9MpLLHg5WTHza298Wlox4ZjKTTzcjwvfAE1UW65YE9H+rf0ZMOxFPo297gKR0MIIS6uXgUwJqOBEt2EZpUARgghhBBCiNqSVVDCx2sjySu2cCI5F00Ds9HAH4eSAHj3t6Psj8tiYIgXjd0d+GbrSVZFnKGljxM7Y9IZFerPf8a0o3sTNwa38mbEp1v414rD5BSWMuf+LjjYmBgQUnl3IrPRwPjujfhwTSTD2vhcUhsf7dOUDcdSGNzau1aOgRBCXK76FcAYNEowolnlNtRCCCGEEELUlh92nSav2EITjwYcOZNNn+Ye2JqMrDiQwI+7Y3F3tOWNka25t3sjbIwGOjVyZfXBROIzCxjY0ovnb26B2Wjgri6BAIzp5M+n607QPsCZQTXcFnp8j8YcPpPN2K6Bl9TG3s082PbPgfi5yH7v/5MAACAASURBVOQEQohrQ/0KYIwaJZgwWGUSXiGEEEIIIa4Eq7X8jkXw4k8HWH80meJSK72buTN1eGvGzNzKhB6NySooYe2RJNwb2LDo8R74OlcGHyM7+DGyg1+N73FX50C+33ma/xsaUuPEuG4NbJh5X+fLaruEL0KIa0n9CmAMBkowSQWMEEIIIYQQNZi7NYaNkSnMvr8L3+84xYG4LD68uwOapmGx6iwJj+Pm1t64NrAB4NVlB1l9MJHezTz4dX8CvYLdSckp4rnBLWjt15ADbwzFxmQgq6CEJXvimDK4RZXw5VI0cndgz2s318buCiHENaNeBTBmo0YpJgy6VMAIIYQQQogbW1GphYJiCy4ONhXPWaw6MzdGkZRdxAs/7mfFgTNYrDojO/gxIMSLOZuj+c9vRxkT7c/0saGcSstjUVgsTnYmft2fwKhQPz4eG1qlSsXGpO565GxvZuFjPa/6fgohxPXiogGMpmmBwDzAB7ACs3Vd/+ScZfoDy4CYsqd+1nX9X1e2qRdnNGgUY8QgFTBCCCGEEOIG9++VR1i+P4HVU/ri3dAOgC0nUknKLqKRmwPL9iXg3dAWk8HAjHXH8XSy5cM1kTjZmVi6L55H+jTl220nMRo0fp/Sl7iMfNr5u9Q4REgIIcSFGS5hmVLgH7qutwJ6AE9qmta6muU267oeWvbvqocvoGZHL9VNGLCC1VIXTRBCCCGEEKLO6brO74eSyMgv4eWfI9B1NY/LkvA4nO3NLHq8B/1bevLR2FAm9Q9m7+lMRny6hQY2RpZO7k1DOzNjZ29ncXgs93QNxLuhHZ0bu1VUuwghhLh8F62A0XX9DHCm7PscTdOOAP7A4Vpu22VTd0Eq2yVLCRiMddsgIYQQQggh6sDRxBwSswvp3NiVdUeTeXP5IXoGu/P7oUTu7hKIr7M9cx/sBkDnxq6cSM4lwNWeW9v74utszzuj27LiQAKtfBvy8E1N6nhvhBCifrisOWA0TQsCOgI7q3m5p6Zp+4EE4AVd1w/97dZdJmPZbagBsBSD2e5qN0EIIYQQQohaZ7Xq6Ki/f7/cGMW6o8n88GgPDAY1PGj9sWQAPr+3E7M2RjF320m+3X6KEB8nnugfXGVbtiYjb97WpspzF7trkRBCiMt3yQGMpmmOwBJgiq7r2ee8vAdorOt6rqZpw4FfgObVbOMx4DGARo0a/eVGX6CNWLSyXZJ5YIQQQgghRD2TVVDC5O/DCT+VgYejLfMf7s6MP4+TV2xhQ6QKXdYfTWHP6Qxa+zbEx9mON29rw8AQL04k53Jfj8YyjEgIIerIJQUwmqaZUeHL97qu/3zu62cHMrqur9I07QtN0zx0XU89Z7nZwGyALl266H+r5TWwGsqHIBXXxuaFEEIIIYSoM2/9eogd0emM6xrIorBYRn+xlbxiCy4OZj758wTRKbnkFKoPIiefVenSt4UnfVt41lWzhRBCcGl3QdKAr4Ajuq5Pr2EZHyBJ13Vd07RuqMl9065oSy+RVTOrbyxyK2ohhBBCCHH9yC8uZUl4HE52Zqy6zsoDZ5g8IJjOjd04mZrHj+Gx/LwnnmcGNuP5IS3xd7XnvdXHGN7Oh1Y+DflwTSR2ZgNLJvUiIi6T20L963qXhBBCnOVSKmB6AxOACE3T9pU99wrQCEDX9VnAncAkTdNKgQJgnF4+1fpVphtMoCMVMEIIIYQQ4pq1+2Q6rf0a4mBjqnj81IK9JGYXViyjaZCWV8y7d7Rn5KdbKLZYGRjixVMD1Uj/x/sG42RnZmhrbwwGjR/D45jUP5jOjV3p3Ni1TvZLCCFEzS7lLkhbAO0iy3wGfHalGvV3WDQbFcDIHDBCCCGEEOIatPLAGZ5csIc7OwfwwV0dKCq18I8f92M2aSx+vCcNbI0Ullg4mpjD1KUHeWhuGA62Rv58qh+Bbg4V2zEaNCb0aFzxeOOL/VHF60IIIa5F9W4GLl3mgBFCCCGEENeomNQ8XlpyABujgaV74zmZmsc3W09yKi2faaPb0a2JG238nOnc2I07Owfg09CO+MwC/m9oSJXwpToSvgghxLXtsm5DfT3QjSYoRQIYIYQQQghxTSixWInPKCAhq4BnftiHyagx/5GejJu9nYe/DSM2o4BBIV7nTZJrazLyzui2rD+WzLiugXXUeiGEEFdK/QtgKibhlSFIQgghhBDi6tJ1vaISJSWniNmbovh5TzxpeerDwUA3e75+oDvNvZ145KamzNkSzagOfrw4rGW12xvc2pvBrb2vWvuFEELUnnoXwFgN5QGMVMAIIYQQQojaVVhiISY1jyYeDXhlaQRrDifRK9gdqw5bT6RSVGplSGtvBrT0wmTUGNDSC9cGNgD8Y0gLpgxujslY72YFEEIIUY16F8DoRvUfGla5DbUQQgghhKg9hSUW7v96F7ti0mlgYySv2MKQ1t4cjM+mga2R4e18mdw/mKaejtWur2kaJqPM2yKEEDeKehfAUDEJrwQwQgghhBDiytJ1nXVHk9lwLIWjidmEnczg8X5Nicso4Ja2Poxo71fXTRRCCHGNqncBjF4xBEkCGCGEEELUDU3ThgGfAEZgjq7r/z3n9cbA14AnkA7cp+t63FVvqLgsFqvOxG92sfl4Ko62JhxsjLw9qg0TegbVddOEEEJcB+pdAINR5oARQgghRN3RNM0IfA7cDMQBYZqmLdd1/fBZi30AzNN1/VtN0wYC/wEmXP3WisuxLSqVzcdTeW5wCyYPCMYsc7cIIYS4DPXvf42KOWDkLkhCCCGEqBPdgBO6rkfrul4MLARGnbNMa+DPsu/XV/O6uAb9uDsOZ3szj/drKuGLEEKIy1b//ueQChghhBBC1C1/IPasx3Flz51tP3BH2fe3A06aprlfhbaJS3Q4IZuP1kTy/u9HKSyxkFVQwu+HEhkV6oed2VjXzRNCCHEdqndDkDSZA0YIIYQQdau629ro5zx+AfhM07SJwCYgHjivfFfTtMeAxwAaNWp0ZVspANhwLJn2AS64ld0aGtTto+//ehe6rmPVYc+pTJztzRSVWrmzc0AdtlYIIcT1rP5VwJgkgBFCCCFEnYoDAs96HAAknL2ArusJuq6P0XW9IzC17Lmsczek6/psXde76LrexdPTszbbfMPRdZ3payKZ+E0Y09ccq3g+KiWXSfPDCfZswK6pg5l+dwd2xqSxITKZx/s1pZ2/cx22WgghxPWs3lXAYCifA0YCGCGEEELUiTCguaZpTVCVLeOAe89eQNM0DyBd13Ur8DLqjkjiKvpuxylm/HkcG5OBsJgMADLzi3nk292YjAa+eqArHo62jOkUQPsAFzwdbXF2MNdxq4UQQlzP6l0FjGaSOWCEEEIIUXd0XS8FngJ+B44Ai3VdP6Rp2r80TbutbLH+wDFN0yIBb2BanTT2BpWUXch7q4/Rt4UnT/ZvxrGkHDLzi3lqwV7iMwqYPaEzgW4OFcs383KU8EUIIcTfVu8qYLTyuyBZ5C5IQgghhKgbuq6vAlad89zrZ33/E/DT1W7XjS4qJZele+LZFZNOscXK26PakJBZCMCXm6LZciKV10e0pkuQWx23VAghRH1U7wIYo7Fsl6QCRgghhBBClNkRncZj83aTU1SK2WDgpVtCaOzeAC8nO0wGjdmbonGyMzGuW+DFNyaEEEL8BfUwgDFQjAkbCWCEEEIIIW5oH6+NpFewB639GvLIt7vxbmjLymf6VBleZG9jpK2/M/tiM7mjUwAONvXuz2MhhBDXiHo3B4zZaKAUI1hlCJIQQgghxI0qLbeIj9ce561fD7FifwK5RaW8d2eHKuFLuW5N1JCj8d3lVt9CCCFqT72L+E0GjRLdJEOQhBBCCCFuYHtPZwJwKCGbD9dEEuzZgE6NXKpd9ol+wdzUzIPm3k5Xs4lCCCFuMPWuAsZUNgQJi9yGWgghhBDiRrU3NgOTQcPFwUxKThF3dQlE07Rql3VrYEPfFp5XuYVCCCFuNPUvgDFolGCUAEYIIYQQ4ga251QmrXwbcn/PIGxMBsZ09K/rJgkhhLjB1b8hSEY1BEm3FFP9ZxxCCCGEEKI+K7VY2R+XyV2dA3hmYDPGdQ3Eq6FdXTdLCCHEDa7eVcCUT8JrlTlghBBCCCFuSJFJueQXW+jYyBWT0YCfi31dN0kIIYSofwGMyaBRjAm9VIYgCSGEEELcaHRd5+c9cQB0rGHSXSGEEKIu1LshSEaDRilGdKmAEUIIIYS4Yei6TlRKLv/bFMOi3bGM6eRPo2puOS2EEELUlXoXwJiNBkowocskvEIIIYQQ9VZ8ZgGuDmZsjAb++XME648mk5ZXjEGDx/o25Z/DQmq865EQQghRF+pdAGMyapRgglKpgBFCCCGEqI+KSi0M/2Qzvs529Gjqzk/hcYwO9aNzkBtD23jj5SQT7gohxBWXeRoKMsC3Q1235LpV7wIYs8FAiW6UChghhBBCiHpq98kMsgpKyCoo4WhiDmO7BPLune3rulniSrFaIPJ3aDYITLZ13RpR12J3wdZPYNh/wSWwrltzY1v1IsTvgRciQSoM/5J6NwmvmgPGBBLACCGEEELUG4UllorvN0amYDZqzLqvM3d1DuCN21rXYcvEFbdtBiy8R3W6xbXh1DZIj6m97Z/cAlnx1b92Yi0cXQH/GwgJ+2qvDZfi5FYoyLy8dVIiIeKnv/e+R35VlSflrNba6+9ardU/r+sQuxPykiHlaO289w2g3gUwFUOQrBLACCGEEELUB19tiaHLO2tJzikEYFNkCl2D3BjW1of37+qAg029K+q+cSUfhfX/Bs0IO76Aolz1/O6vYeF42P0NZJ+5+HaSDkHC3tpt6/UmeqM6LpdL1+GHcfDb/136OgvHw45Zl7ZsaRHMvwOWTa7+9dxksHECkx0suBuyEy69HVdS5B8wdzhs/+zy1lvzOix5BPLTK5+LC68MVAqz1TEud2w1/DoF8tLU45RjsOg+VX1SbsO/YWavyvV0HfZ8V/U9LsRqUT9L5y6fegI+bAG7/nf+OunRlW2O2Xxp73O15afDkkchJ7GuW1KjehfAmI0GijGC3AVJCCGEEOK6l5pbxMdrIsktKmVxWCyJWYUcTcyhXwvPum6aqA1r3wSbBjD2O9XZ2/Ot6lxu/ggiV8OKKTA9BJY9WfM2LKUqMJh/BxTnX/j9cpJUWBD5e83LZCfAoglwekflc7pec6XAuS40N2V6zKUFSpcj6RAkH6n6nK7Dkodh3mi1z5cj4yQUZqkApzwQO1thtqpgKQ8DrBY49htsel+FKxeTGAGlhRC9ofrQLC9FDT0avxiK89T5OvfYb/sUPm5/8fNdE6vlwq/nplQGRPHhF9/evgXw1VAVBJxYC+gQvV69lhUHcwbBzJtg+TPwbmPY+516rSgXlj8N4d/AzJ6QdFgdF4CIH9VwLIDjayA1EpIOqsfJh2H5U7D14wu3q6RAnact09XP0s5ZqpJm6yeQFgUb/qOO92//V9bus8SFqa8mOzi56eLHANRxvdixvZi81Etf9sSfELFY7dfFRPwEh3756+36i+pdAGMylFXAyBAkIYQQQojr3sdrI8kvsdDS24kfdsXy/c5TAPS93gOY3OS/3zGpC2cOXDisqI7VAuFzoSjnwssVZqlOX+h4CLkVGt8EO2ZC2gnIOq3mAJm8AzrdD3vnQ9Q6FSacG2Ac/kVNFpqfpgKcC9k3Xw1vWXA3/PbPs0IEq6p4KC2GrTPgyHKYeyv8+bZ635m94R1P+KCF+rfmjeq3H/kH/NsP1r5V2T/JTVGf1OelqY74iinq+TP7IfV45bq6rjrFNTm7aqKc1QrfjYEvesLPj1cOl8lJVB3rvGT4+RHVlqRD8PtUKCmsfvubPlBBSuIB9dhSVBkilIvZpN5r7q2w9g3Vptxk0C2Qn1rZwdX1ymoLXa/6nnG71VezA2ypJkDITYYGnuDVCoZOg4Q9VYOak1tUlUnmKTj+hwruotZVvm4pgd9eglPbq9/PDf+Fj9pcuP+4c6babqNe6r11Xe17bnL1y59YC7E7VBBoLQGDqbJN0RsBXR3PPfPAaKMCFVDVNXnJMHKGCqW2TFfLOweCo486X8X5lcHL8T+qHsNDS6u/LkBVsLzbBGb3h/X/ATS1/OFl6vh9NQQOLoHuT4BnK/jlSRVmlosLU5VIrUeroVjnhmC6rvZx73wV6CydBO83g69urmxTaZGqjCrMrvlYny1hn9pG1PqLLwtwpmyI2t75lcHnummw+uXzl908XR3/q6zeBTBmo4ES3YQmAYwQQgghxHUtM7+YRWGxjO0ayJTBzYnPLODTdSe4tZ0vIT5Odd081bn8K9ULxXkwoyOEzVGPUyJr5w6eR1fB18NUh6v8k/OaxO+5tGqFP15VwymqC490HWLDqnbaQHUuf30Wtn9x4W0fW606q61Hq8ddHoSsWPjzLfW42SDVCR/+Abg0huXPwqed4Luy5dOi4ODPsOUj8GihOstbZ6gOenWVGwARS8C/C3R7THWyd81Wzx9YBAvugpXPqeqEkBHQehRs/gC+u111xns+CS2GgZOPWq84T62bmwK/TFbt+eNVNZHwlumqssFqhW9ugc+6wE8PqpCovFplySNq2+VVHFs+UvtXXVVIbjLM7nd+8JN8CHIToUlf1ZmeM1i1ozxE6ThBBQcLx6v32v6Z6sTrugqtyhXlqKFgG99TFSqaEeyc1TV1apu67i0lamiMyRba3a063TtnQU7ZECHNAGFlQ1l2zYbprVU1w4HFKrQqH2ITFwYN/aHHJBWe/fJk1flO8pLB0Ut9HzIS0ODEmsrXf5kErk1USHPoZ1VV8t3tKjwCVdWxc5aq6jg3nIjfo/Yx54wKF2tyZr+69trfpdoWsxG+HalCi/IhUUdXwrKn1PflwVnCXtW2kFvhxLrK4MbBA57eA0+Hq+vq9A4Vlm2doR53fkAd08PL1fLNBkHvZyBuFxz8CaylKtQ5XlalEl8WwGSeVudny8fnD8PZOVutV5ChKooGva6qaNa+qQIegwlsG0K/l2DAy+o6ivqzcv24MPDvBE37Q0G6utbOdmCROu7LnlSBTuRq9XMYH15ZxbPxXVj90sWD0XKHlwG6Ope5KbBgbNW5gjJPq+vz7PNkdlBh47GV6udt91dqOGNsWOVyBZmqaqhRj0trxxVU7wIYY3kFjFWGIAkhhBBCXM9WRpyhxKJzb7dGDG7tTYiPE2M6+vPxuFC0ur4DR0Em/PgA7Pry8tdNPQ7FuapjlZeq5nL4Y+qVb+O2T1UHK/UEbP+86mtZ8WrIxrHfVMfkfwNUx/hCSgpUR7EoW20XyjrusaqjEzYHvhoMmz+sul7Ej+rrvvkXHrZzeJnqiPt3Vo9DbgVbZzUBqWsQuDVVz5ts4ea3VFWM2V5NCJpyDBbfr0KNpIPQ+1no/0/ViZx7qwqizg2Nko+oTmT7sTDsXWh5q/qkPC4c9i9Qy+ydr85V3xfhzq9Vp3nkDJi8DW7+F9w2A4ZMg5L8ysqgmI2w73t1XlOPweiZ0ONJ1UHdOw/SjquQKmYjOHqrTmRhVlmlT6w6fokRKgAB1QkvVx5yzR2hOptnV3pAZaXA7bPg/mWVw0nKA5ih/1aVRMd/VxUWds6qumfnl/BJqBoeA6oTr1tUtcmJP8GzJTQfCvt/UAHSiudUqFeYBYPfhDGzVWc7emNlINHhXtVpj1qngp7SArXdyN+gKAuOrVLLxYVBQBfV8b/pefUen3VT5x1Ux7tBWQDTwF0tW175UZitjl/nB1RwcXSV2h+DWbVx0weq0sE1SB2DU1vVdZB0WFVi/PQg2LuqbZ2uoUIGIPEgeLcF31D1eO2b6mteigpi8lLVHC17v1Pfp8dA494qhGo/FoIHqWAq5aj6uW/SF+wagnuwCgHyktXwoZI8de0CdLxPVckU50CTfpXB5Lpp6mvoeDUpbkGmumYDuqpqmgV3q2qk/QvVdbb2LXVc9s6HNrfDM/vgyTBVSaYZ1DXXYzI8sQUeWw8ObupcO7irdUCFi4kH1XsED1Dvs/7flYFWQYYKG/27wLMH4J+n4aUYdQ028FQBSFx4ZYXT4WVVj29pMWz7rDIoLS+mKA/Roter3x+Rq6uGQr9Mhlk3qSCwMEv9TLQfC86NIPxbdbzzy4K+Na9Vtjd+N6BDYLeaz3ktqXcBjMmoUYxUwAghhBBCXO+W7U2gmZcjbfwaYjYa+O3ZPkwfG4rZeA38CVseQPyVO8OUDzOJ213WISxRk8xeaLjJpdr6CcweoIZ7xO6EzhMh9F7VkTm7qmD9NDVk489/qc4RVO5TTU7vUB1CUJ3mY6vhkw7wcVtVjfH71LLJcz+vHGJQlKs62i6NVEc5ZmPl9qI3qEqM1ONq+RNrodVtYCg7v2Z7aDtGfd90QNW2tLkdJm2DR8sCiD//pYKX/q/AxFWqc9q0nwpMbn4bkiJUdcXZIn5SHdA2o9V73j4LGniouT5iNqswIKCb6oz6lXW83YNVZ7+80w7QuJcKUg79rB5nlF0Tdi7QqCe0GqmqZdBg5T9Ux/apXTDqcxj0BqCrKiHdCk6+qspm1k2qI+zXqTKoAFj6uAq5chPVEK3UyKrBUtQ6NXykoR8E9Vad+OiNKvhwbaI6/T0mwfgl8OBq1bZjq1XnX7dUTtIbc9YcHwl7wKed2m/3YLVPJ9aqSgzNqI6zpqkAJiOmMoDp/09VWfHjg5XVNae2qraAGvqVm6Kuw4CuKlgb/AY8tgGcvOHHiSooLMkDx7OGHDYfoipX8lJV5QqAk5+6JqwlYO8GE35WVULr3lZByCN/qud/naKqb2b2VJUYRlu4a646Nqe3qyqwc38O81LV8fZuC95tVLiTsFe1+d5FamjP7P6QXVaZEbNRBUwhI2DyTujzvKpgATUUKidBBTDlGvVSX7d9Bm7B6pwD+HYA73bq+yZ9wdlfXY+5iSqMDL1XnbOIHyHlCAQPhGaDVVhislcVHmf2q+qrhfeoIKfHE+paN9moa71JP1UxEnqvOsbuwer9TDYqyDj2m6pU2vU/9V7BA1XF1+A31XUZPlctv3m6CjpGTAfXxirYAzDbQddHVWA2Z5CqZOr1jPr9cXKrmlspLUr97PwxVQUzaVHwb381TCrliDo3qZGqggdU2AoqzE3Yq673I7+qIYRF2apKp8M4dR4OLlHL9p6izm95SBq7S/3sl4e9V9E18L/XlWU2GijBhCZ3QRJCCCGEuG7FZeSz62Q6o0P9Kqpd6rzq5WzlnYCMk5e/bnnQkZuoqk5M9uoT5TWv1zx/A0DYV/BBS/i4XWVJ/7kOL1Md5jWvqQ5Ts8GqM2IpqpyPIzFCTRLq3VZ10iqCg4vsS/QG1fm0dVbhzup/gsEIA15VndQGHnDvYvVJdPlQnmOrVHXIyBkqkCifbHTLRzBvlOqE//GqqtCxFEH7u6u+Z/mn9CG3nt8e7zYq2PHrpLZjslcdzKDeKhAAcGsCPZ8CzxD1if3yZ1RH11KqKgSa9q8c3mLXUAUiKUcBXb33Q7/DuAUXPi4Go+r8R/6hgqSMkyqQeXYfTFiq2uLsr4Iea6kKRZx81FevVmobR1eor2Nmqw7qgFfhgV/VuUs5qjqlmbGqiqbTA/DcIegwVlWxZKp5kVSF0nZVoVAuZIQKJaLWgW/7yuebDwbv1tBqlOqYlwcZ5ddm9EYI6qPmHQHwaQ9BN6khM0P/rbYZPhcCu1d2tl2DVCCZFauu54b+MPA1KMxUAUmjXir0yk1SFS1R61RFA6gwo5xvexgwVR2r8rCmvAIG1DWNripzysOehr4qGArqoyqTmvRV1RzP7IMHV6prs8ckSI9SbR49U4VzT+2CJn3Uuqd3qIDr825lw7YiVJVM+Xwr3m1USOTdRj1uc7s6Jv1fVvvs3VY9X16x5B4Mni3UOs4Bap/KA8izAxiPFirQs5ao67/82tU0VenV/2XVflDXUPnxCuiq3vP3qSq88+8CIz9RoWTQTarKp3xOlI73QZeHzg8cRn6sqlTsXThPxwnqHCyeoCqJmg9VP1sA3Sep81le7XZyszr2vh3O3063R9VQvX7/p4KwzhPV89+NVtVKG99Vv49AHfNTW9Xvgo3/Vc8NeUd9LR/aVvG7N0ZVp/WcDM1urqxa8+2gjqNuVVWADQNg4Kvqd8Wm99Tv2NM71LGzvfpDWevdPfvKJ+E1yBAkIYQQQojrUqnFylu/HkbTYFSo/9V9c6tVdWptHKo+v3+hmhD2noWqs5d6VifAaoHv71SfTvd7qbKCoyapkWq+BWupmjeiaT/VYVr3Dqx8Xs1xYjBWXSctSg2P8W6jKkYO/KjCg7MV56lPvEENHbBxUp00gwk8WqpPsT1aqDka7F3h/uUwZyBknFIBRUY11TxHV6l5J0z26lPsgK5g66jmWinJh9GzIPQeNT+FpVh1aFoMU3fAcfKBDe+qjk+TfmqIyMGfVfixd76qTAi6SXXAotap1/07VX1//07wj2OVIUl1Qm5VoVOb0ZVhwNkMBlWN8eNEdRw1g+qcZcfBLf+tumyHe1SwYNNAhTdqAzW/99lt2DlLdewyTqkwwmxfdZmbnlfDjLo+WvmcezP19fga1a7A7lU752Z7NYTo6IrKSpebnlPH2bMsvEk+qioiTqxV127wwMr1A7up+UbyU1UVy7ma9lPzfrg1VcOPUo6pMC0pQnVaXRurc3V2eOPXUVUlZMRUVnaAOl6WIojfq869wQDt7lLDnYIHqZDidFmgMnCqmhdo+VOqSqZ8aM+5x6V8WNDZ5983tCwE3FEZ3Dj5qp+ZiSvOWu6cMKDPC2qun+rChkY9VAf+0FL1eM3rak6YrNPQ9k71XHnA4tdRBRutR5Vt9x/qZ6zVbfDNsMoJdd2Cq75H3xfVOT6zv3I4HajjFNhDDc1qd1fVdZoNqnqMW49W1V5N+qn9vW2GmucHVLjSwF0dK+/WKjCND1c/67d9VhnsIQfATgAAIABJREFUnM01SP2rjndrFQj+MhnQ1QTIZ7e55TB1nHKT1XC+ro9Uvx0HN1UpVGXb7VT4G9RHVamUX9uJEer3iI2TardNA/Wz5eijruEm/SoDmPJhdT7t1DV0Yo0K/jxbqQoe31B1noJ6g9Gsfv5WTFG/x+LD1c96HaiHAYyhLIApUenWtfRJiRBCCCGEuKhXfznImsNJvDmyNYFuDhdf4Ura9aWalPO5g+qPf1B3bFnzhqpYWXA3PPibmjgXVLVH+TwXUevUcIQ7/ld1m4kR6g4s3R9Tj1OPq072yS2qs9H4JtU5LM4rm0C2parkKKfrsOoF9Un6uAXq+7NvA7t3virB7zFJhToBXVWbmvZTHQ9Qk2oufQLmDleVKPctUZ21UV+oMCn5iAqZdF1VWbS8RU20uvAeVfViLQV0NcRH01QnxsYJWt+mtm+yVf/4f/buPE6us77z/fepvXqXultSa7VkydiSAS/CK14Ak8GOgxNCCCRh8RCcECBAcklI7iUQXnndm2Eyk5kEbnKZIUNICIwhLMZxQlicgFm8AMa2JNvabEnW1i313rXXc/946nRVd1dvUp06rTqf9+ulV1WdOt39SGqp63zr9/s9cp/zb3+uGvS8+cvugm3bLW745v6vuyDiqrdKL3u7a7/KjFTacepYKHyRpBe/3oUm19wz/zk7f95VBrT1ul17vvEhV6Fxye0zz4tEZl7EL9W6SkBx6ikXwNQb7rnucuk3Zm3fm+py1TITp1zo4P0Zeno2u7/Ph/7cBSWbrq0GQ/0vcreD+9zxBz7gAraLbqr5/UTdhfJP/r66xlqxZOV7od/9fQ3tr7Yfbb1FuuhmV0mx/srqxxgjXf6LrlVq+23V416o4A1rldyf5+v/xt0/VKn+aF8jXfFrbuBs54B7Pp6a9fve4tqbvACmvaYFKRJx1SVnD7u/Q8m1XC0mEqkfvkiuAkZy67n0Tjc82ETclstPfdH9HXltUC9/n/te7t5Y+bxR12YkuZDm0INu7au2zPwaxkg3/x/1v/4N73YBSu/F9Z/3dG+Q3veU+z6W3Mfc/Hvuz7y9t3reml2uombf/dKGK8/9mvglb3B/r1Nnpb4dM5/zAq49X3bBnxdQLcXr/j/3OVdtcbOHZF1weOxHLthdf4X0i59yvwdjpBve41ooYyk3AyY/6QKySExas9MFL2tf7ILzWKK69hOPu5BXcm1W3/kz93+4FMgAXqkVA5ioUd5WflvlYvWHDgAAAFa8PcdH9flHj+o3bt6mt924dfEPaLR9X3M7fBx80F2A/eAT7h3WiZNuOOb3/9KV3Q89IyU6XAm8N1By+23Sk/dKd/xn13Zx5GF3ofqle9y7vRt3u4uWswel7a907SovPFZtmbntI+5d6yc+PzOA2fMlF+7c/p9d9c3WW1xFxPBz7t3rp//JtXJMnZFk3Lvdn3q1a5Hw7PoF16Lw2KfcbAev9eWiG92vH3zCzU849KBrw3jlh6oXq+962L2ufuJetzORNydk189XQ6pa7b2ufebf/h93vtey4QUD//4xd7vlRvfxb/wHN8x0sYvP+ay6yAVmCzGm2n5x7W+4QOPqu6Voncuh2SHIUqR73LvwJx53lTWzL74X0rvDBTD9l9Z//nWfdLvtjDzvKo08qS4XQJx+2r2zP3XGtYDNDjOufIubeVHb5lPLG0Tat8N9Lx/4lqskWn+V+/O5p84WwDe+11VI1FaZrKr8ey3l6gciG1/mwrwt17vP+65H6v/5S+4iumez+3cjzQ3hVm9zoUPvxS5QnF1ttFx9O1yly0ve4P6dPH2/a0GbOC396H9Vv4elRapGdrl/Qz2bl3cdetHLq0HBYmrn4UguXJ2zjp3uNjc6t7pouTburn/cC/Qe/2zla+6qf149tedec4/7/t96k/t/7vhP3MykzrXVc26o7C7ltXcNPeuC7f5Lq/9e3/wlV9nmueJXXRh6WU1I/JavSvu+6sKfS16z9PU2UMsFMPGoUUGVks1SngAGAADgAvLJ7xxSeyKq33rF9uZ/8dxEdbvmZ//ZtQqcfFJ6/iF3UXbbH7uL3cf/wV0wXPqz7kJt71ddi87Vd7s2kKFnXTXJY59ylRmn97p3xH/wcbf1azHrKhUkd27tXIZdr3PVGYPPugubVVukf/tTdxH1sre7c7ZWgozD33UXgt7cjmOPuneh11wqfeDg3NfBPZtcyFOPd/H8+Ofc7cFvu2qAzgF3sWuM9KoPueeSnS7E8XZrqae9V/rZP5t5rGONu2A68bgUb69evDd7J5KXv99V+lwzT8vEuVp7uQvubHn+C/R6ei9232PzBTCrt0m/+kXX4jS7RaX/UjdYNDfqQrOBOlUum6+V3v3o3OOz9b3IVRns+5oLCOcLRyQX/lz+izOPeVsZl4v1A5hEmxt4633vL/T5JVcR5LXFtfXNfG71NhdMjhxZWvXLYoyRXv+p6uP3PeXWd/LJSgCzxOoO77zaFqMg9F3i/s+xpfpzWRqhbbXbbejET93fu1eRtVxeG+CxylbasjMrrmpNV30941qQatvtZod06R7pjo/NPNa33bWMBajlhvB6LUiSXAADAACAC8LhoUnd/8QJ/cq1m9WdXsKbaNa6waPny1rXSuTtSNS1UXryH93F1ys/5C7Y7/yv7iLtije5ahhZtxuL5HY/WXd59V3nwWfcx5qom3mx+XrXHrTnK9X5EL073HDN3/zuzIoLr2rl737B7Uxz//tddcjP/bfqXJj+S11LxnPfddu1ekNfpWorRSyxvLYDLzDwtv49+rAbGLr5+rmfJ56ubDs8qyVhKbwqmM3XLn4B7pdUtxtuWruTUSOs3eWqiCTXQrNU3p+jV5VUz/or3C5Ns2fcrLnMhS+d6ys7LZ0HLxjJjbqhpssVjbnKD8mtp57L7nSDaZfCq4hK9VTbSjyrt7mg6+gjLiRsNO97c92LpTd8Zul/tl5lx7lWczVKLFn9vvIrgJGqgV/fJedWOVZrzWWSKv/XzFe1s3qbC3sOfNOF4PXa6la4lquAiVaG8Eqq7h8OAACAFStbKOmzDx/Rf/vGs0rGIvqPL5/VemTt3NbyySHX2vPCj6T3Peneka99rpitzmhYSDEvffFu13ax4Wo3Y+DW35fue4/bnvWad8y86L3kdjcANDfq5lx4A04HXuouuqNJN0/l1B7XftN3iQtqonE3BPeBygyIvktc+83sFp6eTW6ex9GH3e49l73WtTnNnsFx0U2uAmb4Ofdnc/MHXBvS7AqJpfJaZoqZ6pbRk4Nui+VG2nqzm6/R6M+7EsxuU1kqrzVnvnf9F7KmEvq94g/Pvw2nNhipne2yHKu3uTlIjahK8Qbx1psB5M3ByY64tjw/ecN2l6L/UjeLZNutfq1m6dZe7na38rMaZ+ClrgpwOe1H80m0u7/z8ZPzrzkad+c8+QUXxGy75fy/bpO1XAATj0aUpwIGAADggvD40RG94zOPaXA8p5t29Omjd12uge5ZF5JP3Cv98wek9/60WrXwt6+VTldmkZx8sro96v94lZurEk1I735s4Vkc1kr/+HZ3AdGx1rWBbLvVDeG8/3fccNfZFQfxlHT561x7UO92dyHoBTCRqHvXef/XpcKkO3bVW6of++vfdPNjCpmZAzNne+WH3PbNt/3x/FUiW29yLRjeNr4DV7iw6Fwl2qvDYK97l9tppTDZ+EGVF7/Szdp48RsWP/dC4+0yFIkvL4DYfJ30wefrz9NZzK5fcJUHtfN+zlXXRhc69u2YOX9jObxWtq4G7F7mVZG01wtgai7Q56u2CUIsIb3zoaBX4dz2YTfvyM9NabzqmuUM4F3IS97ggt+FdpJ7yS+7dstXfqhadXgBabkAJhY1KlgCGAAAgJXOWqs/uX+vjKTP33Odrt26WqbexcLer7gWoYMPuvAjM+LCl2vfKT38V9WtRjPDLny59E43F+P7f+nmlpw97Fo4PE/c66oNxl6Q9t3nttu94lelL9wtXf02N9vg178x/7uwr/5j6co3u4qDVRe5iwGvZL7vEheMSHMvSgZeIr3hbxf/g9l6U3XOy3wuqmxV/Fhlh5m+BszMWXWRC2C23SIdusltqbymwRc4yY6ZszZayeptroKqa/3cbcQXcy7hi+Tmqrz49ef2sbNFIm6Xnr5znOUhVWZ0GFfJdb68bZxnD52VXAueNwTb7wqYC1XP5mpLmF82X+eq8V50++LnLsUtv7f4Od6OUxeolgtg4jNmwNCCBAAAsFJ978AZPfb8sD561y5dt22eipBSwbXaSK7v//LXuRYHye0asvcrbgikJJ2pHL/iV1ylzI8/U9ka+qD0mw+5CoWx49KX3uEuTDrWufkR17/HVba8/evVr7tQO0iqW9pYGZy77iWufckboOrdmsjCMz3OV+/Fbu1nD7mL0UbMM+nbIZ056C7Ab/+Y2wFmuUFCmEWiro2t0bNlmul8B5Re+WuuHaVz3fmvpXujG9Zcr5rGGFd9dvLJlVUBEzap7nPbtj3EWm4IbzRqGMILAABwAfiLb+/Xuq6U3rB7gXfLjz0m5cel9GoXwJTL1QBm9TZXAu8FMGcPVo5f7AbnlguuKibZ5XYSkqR9lYuF0RekY49IN7xn7ra9y3Hdb0m//ePqkFBvjkbvjvOfybEQY9w8Fe9rNcKrPiK97Z9cJcSqLdKmebYtxvx++e+luz4R9CqCE083br5PJOou7l/+/vrPexVqVMDgAtJyAUwsYpgBAwAAsMIdHJzQI4fP6u4bL1IqPqvK4vt/KT3/fXf/0IOumuSW33PtMaeedFUaknsHfOClbhvm/GTluHGtNL0XS2+5T/qNf3chydP3S8cfdy1HfS+S7vxzt8PP1W87v99INDaz4sFr31jXoJkIC/F2FGpE+5HkWj3WzLMVMpambbXb/haNseEqqb2v/nNeAEMFDC4gLRfAxKO0IAEAAKxo1uqBR59RxEg/f+Ws9oJySfrmR6RH/6d7fPDb0vqrpF2vc4/3f6Oyy8pG9277wEvddrSn9rgKmO5N1YqWrTe5VqPr3ulCknvf4raa3vla6eq3Sv/xX8599sZ8ei92Q0O96hQ/bb1ZkpHWNGAHEuBCc8WvSa/68PwBDbACtVwAE40YFamAAQAAWLHsk1/Q2x+5Q6/eltTarlntP+Mn3bbKZw64N9OOVwbsdq6tzFv5pgtavG1ovV04TvzUVcD01hmcm+6RfvWLrh3JlqXLfs6/31wsKf3OPumqt/r3NTyrtridla5uwtcCVpq+7W4gq5+7/AAN1nIBjCSVIgQwAAAAK9WpH31NbcrqjdvqVCuPHnW3Zw5KQ/vdHBdvN6Edr5aOPiKdfrq6RW3XBteCsP8blWDm4vpfdONu6a1fk/7D/+2CHD9FY827KNy4299ZMwCAhmnJAKZsKkPQaEECAABYUT778PMqPufmu1zfOzX3hJFKAJOfkA79m7vv7Sy0/dWSLbmhvF7QYoz00l+W9v+r26q6d54ARnJbUV//Lt4xBwAEoiUDGBuNuztUwAAAAKwM+Sl951++oI9/+d+10QxJklKTx+aeN3qken/f19wA3r7KzkIbXyYlu9391TWtRle+WZKtHF8ggAEAIECtGcBEvACGChgAAICV4Pi//Bfd/MNf18dWf7V6cOTI3BNHjkqmsivSkR+4oMUbqhuNSRff6u7XVrr0XixtvmHucQAAVpCWDGDKESpgAAAAVpLyvq9Jkm6a+qYUb3c794wckY49Jv3966Vizp04elRau0uKpSXZavuR56q3SBt2z6yAkaSbflfa8nK3BTUAACtQSwYwlgAGAABgxbAjR7Qx84yeSV3hDmx6mdvFaOSI9MS90oFvSCefcs+NHHVbR3uVLGt2zvxk22+T3vEtt9tQrR23SXf/k+S1ogMAsMK0ZgATZQgvAADASnHykS9Lkg5e+yfSDb8tXfubUs+WSgXMI+6kU09K1roKmJ4tUu92d3zNpfN8VgAALiyxoBfgC4bwAgAArBilvffp2fIG7d79MqnzJndw+HmpMCUdf9w9PvmUNHXWHevZVN1auf+yYBYNAECDtWQAQwsSAADAyrFqdJ8eT79Cd3amqgd7NlXuWCkSk049Vd0BqXuTdNFN0sSp6g5IAABc4FqyBUkRWpAAAABWgscPn1S7ndSa9ZtnPtFT8/jSO6VTe6q7IvVsktZdLt31cbfzEQAALaAlA5hoLKayIlTAAAAABOxvv/GYJOkll+6Y+UR3pQJm9TZp2y1Sbkz60aelSJydjAAALaklA5hYNKKiYgQwAAAAAfrR88Paf/iwJCnVvW7mk+keqb1f2nyDtPbF7tjBb0vXvENKdTd5pQAA+K8lazpjEaOiiSlBCxIAAEBg/vu39mtrekoqy4Uts731a1L7GimekmSkVJd08weavUwAAJqiNQOYqFGBChgAAIDA/Oj5YX3n2UH9rysS0tOSOuoEMGtqdjh62a9Lm66R2lY3bY0AADRTawYwkYiKJk4AAwAAEJC/+NZ+9bYndONA2QUw9Spgav3snzVlXQAABKUlZ8DEpytgaEECAABotmyhpO8dGNIv7d6kRPasFG+XEu1BLwsAgEC1ZAATi0RoQQIAAAjIUy+Mqli2unrLKmnitNTeF/SSAAAIXEsGMOlEVHkbJYABAAAIwONHRyRJV2zqkSYHpY41Aa8IAIDgtWQAk4pHlbe0IAEAAAThJ0dGtKEnrf7OpAtgFpv/AgBACLRkANOWiCpHBQwAAEBzvPAj6dN3SrkJSa4C5orNPe45AhgAACS1eABjCWAAAAD898QXpOe+Kx39oU6PZ/XCSEZXbuqRymVpcogABgAAtWgAk05EVbBR2SIBDAAAgG/KZXf7/EPu9ugjeuTwWUmV+S+ZYcmWCGAAAFCrBjDxqAqKqUwAAwAA4I+DD0of2yqdeEI6+ZQkqfDcD/Un9+/TRb1tevHGbmnytDu3gwAGAICWDGDaEgQwAAAAvho+LGVHpC+9Q5KV1uxU6cgjGp3M6OO/cpWSsaib/yJRAQMAgFo0gEknYsorJhHAAAAANMbh70iDz1YfF3PudvBpKZrQmcvfrpTN6P/cbXX5hm733ESlAqadbagBAGjJAKat0oLEEF4AAIAG+fJvSt/9L9XHhUz1/oar9ZXR7ZKkO1cfrR4felYyEal7Q5MWCQDAytWaAUwiqoKNSaVC0EsBAAC48GRGpIc/KVnrHpfL0vhJKT9RPcergLnlg7I3vEef3lvSaKRHPcN7qucc/q40cIWU7Gze2gEAWKFaMoBJVWbAGCpgAAAAlu/pf5L++QPSySfdY283o8JU9ZxiVoompVf8gR5LXa+jw1mVujZJ48fd8/kp6dij0kUvb/76AQBYgVoygGlLRJVXTKZMBQwAAMCyZUfd7dlD7tbbzai27aiYk2IpSdKjz7mtpzv7N7pKGUk69ohULkgX3dSMFQMAsOK1ZgATj7kKGAIYAACA5cuNudvhw+524pS7nV0BE0tKkg4NTmpNZ1Lx7vXS+An3/OHvSiYqbb6uSYsGAGBla8kAJp2IqqCoIgQwAAAAy5cbd7deBcxEZTvpeSpgDg1OaFt/u9Q54NqVijnpuYek9VdIqa4mLhwAgJWrhQOYmCK26IbGAQAAYOm8CpizlQqYui1IrgLGWquDg5Pa1t8hda5zz42fkE4+IW28pnlrBgBghWvNACYeVd7G3AOqYAAAAJYnOyuAqduClJPiKZ2dzGs0U9C2vkoFjCQd/4k7t29789YMAMAK15IBTDRiZCNx94CdkAAAAJbHa0Eae8EFLXVbkLJSLKVDQ5OSpIvX1FTAPPeQu+0lgAEAwBMLegF+MbGEZCWVqIABAABYFq8FSVYafr6mBWlKslYyZnoGzKHBCUnSxX0dUirqziOAAQBgjpasgJEkRRPulgoYAACA5cmNS53r3f2zh6otSJKrfPFuY0kdGpxUIhbRhlVpqW21FIlLg09LsXT1cwAAgNYNYEyMAAYAAOCc5MalgZe4+8OHqy1IUrUNqdKCdHBwUlt72xWNGFcZ47Uhrd4mRVr2pSYAAMvWsj8VI9MBDC1IAAAAy5Idk1ZtlRKd0tB+aXJQ6ljrnsu7mS9eBcyB0+NuC2qPF8D0XtzcNQMAsMK1cACTdHeogAEAAFi6clnKj0upLmnTNdKeL0m2JPVscc9PV8DkNFmK6bkzU7p6y6rqx08HMMx/AQCgVssGMNE4LUgAAADLlq/sgJTslHbfLWWG3eNVXgBT2Yq6mNXR8bIk6bbL1lY/3tuKmgAGAIAZFg1gjDGbjDEPGmP2GWP2GGPeW+ccY4z5C2PMAWPME8aYq/xZ7tLRggQAAHAOcjUBzCW3Vwfp1qmAOTxS1MX97bqojxYkAAAWs5QKmKKk37XWXibpOknvMsbsnHXO7ZJ2VH7dI+mvGrrKcxCN04IEAACwbNMBTJcUjUlXv8099ipaKhUwtpjV86OlmdUvkrTxZVLPZmnNZc1ZLwAAF4jYYidYa09IOlG5P26M2Sdpg6S9NafdJekz1lor6YfGmB5jzEDlYwMRTxDAAAAALFt2zN0mu9ztje+VNlxVHcJbyEjlskwpr0w5rlfNDmC23iy978nmrRcAgAvEsmbAGGMuknSlpIdnPbVB0tGax8cqxwITS6TcHVqQAAAAls6rgElVAph4Strxaine5h4XMlIpJ0kqRZO6anNPAIsEAODCs+QAxhjTIekfJb3PWjs2++k6H2LrfI57jDGPGWMeGxwcXN5KlylWaUEqF3O+fh0AAICWkvMqYDpnHo+n3W1hym1BLWnNqm7Foi27pwMAAA21pJ+Yxpi4XPjyWWvtl+qcckzSpprHGyUdn32StfaT1trd1trd/f3957LeJfNakPI5AhgAAIAlWzSAyWhickKStL6P6hcAAJZqKbsgGUmfkrTPWvtf5zntPklvqeyGdJ2k0SDnv0hSIukCmEI+G+QyAAAALiy1Q3hrTbcgTWnvEVfJvHnN6iYuDACAC9uiQ3gl3SjpzZKeNMY8Xjn2h5I2S5K19q8lPSDpDkkHJE1JurvxS12eRGUGTCFPBQwAAMCSZcckGSnRMfN4LOmOF6a098gpXSNp81oCGAAAlmopuyA9pPozXmrPsZLe1ahFNUIi6QKYPAEMAADA0uXGXftRZFahtDGuCqaQ0TPHhiRJqVRbAAsEAODC1LJT01IpWpAAAACWzQtg6omnZfNTOjo47B7Hks1bFwAAF7iWDWCSCTcorlSgAgYAAGDJcqMLBDBtyk5NqFSovMEVSzVvXQAAXOBaNoBJpdwLgiItSAAAAEuXG587gNcTT2tickxJ5d1jAhgAAJasZQOYtBfAFPIBrwQAAOACkh2bvwIm0abs5ISSKrjHtCABALBkLRvAtKeTKtoILUgAAADLseAMmDbls5NanSy7x1TAAACwZK0bwCRiKiimUpEABgAAYMnyk1Kyo/5z8bTKuSlt7oy6x1TAAACwZC0bwLQloyoopjItSAAAAEuXn5Ti7XWfsvG0VJzS+k7jDsTTTVwYAAAXtpYNYOLRiAtgqIABAABYGmulwqSUqB/A5JRUopzTQHslgKECBgCAJWvZAEaSiiYmWywEvQwAAIALQykvlYtSoq3u0yOFmNImrzVe4QszYAAAWLKWDmBKJi5LBQwAAMDS5CfdbaL+DJizhZjSyqkvZSUTkSKxJi4OAIALW4sHMDGpRAUMAADAkngBTLx+BcxQNqK0cmqPFFz1izFNXBwAABe2ln7bomTirpQWAAAAiytMudt5ZsCcykYUM2UpP878FwAAlqmlK2DKkbhUpgIGAABgSaZbkOoHMCcmKxUvmWHmvwAAsEwtHcDYSFwRAhgAAIClWSCAyRZKOpmpvHTMDFMBAwDAMrV2ABNNyNCCBAAAsDReC1J8bgBzeGhSQ7bLPRg6QAUMAADL1NIBjKJxRS0VMAAAAEuSn3C3dbahPjQ4qcfL292D8eNUwAAAsEwtHsAkFLHFoFcBAABwYcjPP4T30OCETmuVyl0b3AEqYAAAWJaWDmBMNKGoLSpfLAe9FAAAgJVvehvqOgHM0KQGulOKbHyZO0AFDAAAy9LaAUwsoYSKyuRLQS8FAABg5SvMP4T30NCktvW3Sxt3uwOxdBMXBgDAha/lA5i4iprI04YEAACaxxjzGmPMM8aYA8aYD9Z5frMx5kFjzE+MMU8YY+4IYp1z5CclE5lT3WKt1aHBCW3r65CogAEA4Jy0dAATiSUUNyVN5QhgAABAcxhjopI+Iel2STslvckYs3PWaf+XpHuttVdKeqOk/7e5q5xHfkpKdEjGzDg8NJHXeLboKmAGXipFYsyAAQBgmWJBL8BP0XjSVcAQwAAAgOa5RtIBa+0hSTLGfF7SXZL21pxjJVX2dFa3pONNXeF8CpNSvN4OSG53pG39HVI8Ld3w29LAS5q9OgAALmgtHsC4GTBTzIABAADNs0HS0ZrHxyRdO+ucj0j6V2PMeyS1S7qtOUtbRH5y3vkvkrStr/LcbR9u5qoAAGgJLd2CFKtUwExSAQMAAJrH1DlmZz1+k6RPW2s3SrpD0t8ZY+a8LjPG3GOMecwY89jg4KAPS50lPyUl6lfAJGIRbehh8C4AAOcqHAEMQ3gBAEDzHJO0qebxRs1tMXq7pHslyVr7A0kpSX2zP5G19pPW2t3W2t39/f0+LbdGfqL+FtSDk9ra265IpF62BAAAlqKlA5h4IqWYKWsqmw96KQAAIDwelbTDGLPVGJOQG7J736xzjkh6lSQZYy6TC2CaUOKyiMLUwltQAwCAc9baAUzSbY+YyWQDXgkAAAgLa21R0rslfV3SPrndjvYYYz5qjHlt5bTflfQOY8xPJX1O0tustbPblJovPzmnBSlfLOvI2SkCGAAAzlNLD+GNx10Ak80RwAAAgOax1j4g6YFZx/6o5v5eSTc2e12L8rahrnF0eEoGK9PwAAAgAElEQVSlstW2vo55PggAACxFS1fARGIugMllCWAAAAAWlZ+Ysw31ocHKDkhUwAAAcF5aOoBRNC5JyuUJYAAAABZVZwbMqTH3Omo9OyABAHBeWjyASUiS8rQgAQAALKxckorZOQHMWLYgSepOx4NYFQAALSMkAUwu4IUAAACscHnXajQ7gBnNFJSIRZSKRwNYFAAAraPFAxj3Tk2eFiQAAICFFabc7awZMGOZAtUvAAA0QIsHMK4CpligAgYAAGBB0xUwM3c7Gs0U1JVq6Y0zAQBoilAEMAVakAAAABY2HcDMrIAZpQIGAICGaPEAxr1YKNCCBAAAsDAvgIkTwAAA4IcWD2AqFTB5KmAAAAAWVJi/BYkABgCA8xeKAMaW8iqUygEvBgAAYAWbrwVpigAGAIBGaPEAxr1YSKioiWwx4MUAAACsYIWMu61pQSqXrcZzRQIYAAAaoMUDGFcBE1dJEzkCGAAAgHnV2YZ6PFuUtVIXAQwAAOetxQMY92IhrqLGqYABAACYX6GyaUE8NX1oLFuQJCpgAABogBYPYCoVMKao8coLCAAAANRRrLQgxdLTh0YzBDAAADRKKAKYhIq0IAEAACykkJFkpFhy+pAXwNCCBADA+WvxAIYWJAAAgCUpZKR4WjJm+hAVMAAANE6LBzDeEN6ixqmAAQAAmF8xK8VSMw4RwAAA0DjhCWCYAQMAADC/QmbGDkgSAQwAAI3U4gGMe7GQipQ0QQsSAADA/AqZGTsgSS6AiUWM2hLRgBYFAEDraO0AxhgpEld7tMwQXgAAgIUUszN2QJJcANOdjsvUzIUBAADnJhb0AnwXTagtUmYILwAAwEIKU24Ibw0vgAEAAOcvBAFMXG2WAAYAAGBBheycFqSxTIEtqAEAaJDWbkGSXAVMtMQQXgAAgIUUM3NakMaogAEAoGFCEcCkIyVmwAAAACykkJnTgjRCBQwAAA0TihaklEq0IAEAACykkJ0TwAyO59TfkQxoQQAAtJYQBDAJJS0VMAAAAAsqZqRYdQbMRK6oqXxJa7oIYAAAaIRQtCAlTUkTVMAAAADMr5CR4m3TD0+PZSVJazoJYAAAaIQQBDBxJU1J+VJZ2UIp6NUAAACsTIXMjF2QTo/nJElrOlPzfQQAAFiGEAQwCSXkql9oQwIAAKijVJBsacYuSNMBDC1IAAA0RAgCmLjixgUvDOIFAACoozDlbmuG8NKCBABAY4UggEkoZisVMAQwAAAAcxVc2FLbgjQ4nlMiFlE321ADANAQoQhg4vIqYAoBLwYAAGAFKmbcbe0Q3soW1MaYgBYFAEBraf0AJpZQtOx6mMeogAEAAJirUAlgYrVDeLPMfwEAoIFCEMCkFCvnJVEBAwAAUJcXwMyYAZPTWnZAAgCgYUIQwCQVKVEBAwAAMK+iNwNm5i5IVMAAANA4IQhg0jJeAJOhAgYAAGCO6RYkF8BkCyWNZgrsgAQAQAOFIIBJyhSz6kzG2IYaAACgnukWJNdyNDju3rxaQwsSAAANE4IAJiUVs+pKxTTGDBgAAIC5pluQ3C5IpysBTD8tSAAANEzrBzCVd3JWp2hBAgAAqGvWLkiD4y6QoQUJAIDGaf0ApvJCojdVpgIGAACgnlm7IJ2ddK+ZetsJYAAAaJQQBDDuhcPqRFljGWbAAAAAzFGcGcCMZPKSpJ62eFArAgCg5YQggHEVMKsSVMAAAADUVajMgKnsgjQ6VVAyFlEqHg1wUQAAtJbwBDDJMjNgAAAA6ilMSdGEFHEvDUemClS/AADQYKEJYLpjJY3niiqXbcALAgAAWGGK2en2I8m1IPWkEwEuCACA1hOeACZRkrXSRJ45MAAAADMUMtPtR5I0mimomwoYAAAaKgQBjBvC2xktS5LGswQwAAAAMxQyUjw1/XBkqqDuNAEMAACN1PoBTKWctjPm5r8wBwYAAGCWYkaKt00/HM0U1EMAAwBAQ7V+AFOpgOmIliQRwAAAAMxRyE63bUsM4QUAwA8hCGDci4m2iGs9GqMFCQAAYKZCZrpqOFcsKVMo0YIEAECDhSeAidKCBAAAUFexGsCMVl4rdbexCxIAAI0UngBGlQAmSwADAAAwQ00L0uiUe63EDBgAABorBAGMmwGTNF4FDC1IAAAA00pFaewFqW21JGmkUgHDDBgAABorBAGMezcnWs6pPRGlAgYAAKDW89+TsiPSjp+R5AbwSmIGDAAADdb6AUw0LpmIVMiqKx3XOAEMAABA1b6vSbG0dPGrJEkjU3lJUk+aGTAAADRS6wcwxrgqmGJWXak4LUgAAACecll6+n5p+6ukRJuk2iG8VMAAANBIrR/ASG4OTDGnrnSMFiQAAADP8R9L4yeky147fWg0U1DESJ3JWIALAwCg9YQkgKmpgCGAAQAAcDLDUv9l0iX/YfrQyFRBXem4IhET4MIAAGg94XhroxLAdKZi2n+aFiQAAABJ0o5Xu181RjIFtqAGAMAH4aqASVMBAwAAsJCRqby62xjACwBAo4UkgKnMgEnFNZYpyFob9IoAAABWpDEqYAAA8EU4Aph4Wipk1JWOqWylyXwp6BUBAACsSCOZgroJYAAAaLhFAxhjzN8YY04bY56a5/lbjTGjxpjHK7/+qPHLPE81FTCSe2cHAAAAc01ki+pMhWNMIAAAzbSUn66flvRxSZ9Z4JzvWmvvbMiK/BBLSVNn1VV5N2c8yyBeAACAesazRXUQwAAA0HCLVsBYa78j6WwT1uKf2RUwDOIFAACYI1csKV8qT79mAgAAjdOoGTDXG2N+aoz5Z2PMrgZ9zsaJpaWimwEj0YIEAABQj1cl3JGkAgYAgEZrxE/XH0vaYq2dMMbcIekrknbUO9EYc4+keyRp8+bNDfjSS1SpgOmkAgYAAGBeEwQwAAD45rwrYKy1Y9baicr9ByTFjTF985z7SWvtbmvt7v7+/vP90ksXS0nFrLpSXgUMM2AAAABmm8i510gM4QUAoPHOO4AxxqwzxpjK/Wsqn/PM+X7ehoqnpEK2WgFDCxIAAMAcXpUwQ3gBAGi8RX+6GmM+J+lWSX3GmGOSPiwpLknW2r+W9HpJ7zTGFCVlJL3RWmt9W/G5iKWkUk6JqFE6HqUFCQAAoA6vBYkhvAAANN6iAYy19k2LPP9xuW2qV65Y0t0Wc+pKx2hBAgAAqMNrQWIGDAAAjdeoXZBWtljK3Raz6krFqYABAACow9sFiRkwAAA0XvgCmHR8+sUFAAAAqqYrYAhgAABouPAFMKkYFTAAAAB1jGULSkQjSsaiQS8FAICWE5IApnYGTJxdkAAAAOqYyBZpPwIAwCchCWAqFTCFjDpTMY3RggQAADDHRK5I+xEAAD4JRwAT91qQcm4Ib6aglbZTNgAAQNDGqYABAMA34QhgZg3hLZatMoVSsGsCAABYYSayRbagBgDAJyELYFwFjCSNZWhDAgAAqDWeK6ojGQ96GQAAtKRwBDDJLnebOauutHtXh52QAAAAZhrPFtRFCxIAAL4IRwCzeqsUiUun99ZUwBDAAAAA1GIILwAA/glHABONS/0vkk7tVVe6EsBQAQMAADDNWss21AAA+CgcAYwkrdkpnd6r7koAMzJFAAMAAODJFsoqli0zYAAA8El4Api1O6WxF7Q6MimJAAYAAKDWeM69NqICBgAAf4QngFmzS5LUObpfxkgjU/mAFwQAALByjGfdDpEEMAAA+CM8AczanZKkyKBrQxqmAgYAAGDaRCWA6UgSwAAA4IfwBDBdG6Rkt3R6r1a1JTTCLkgAAADTJnJeBQwzYAAA8EN4AhhjXBXM6X3qaYvTggQAAFBjvLJDJBUwAAD4IzwBjCR1b5LGT6onHdcwAQwAAMC0yVxJEgEMAAB+CVcAk14lZYa1qi2h4UlakAAAADyZggtg0olowCsBAKA1hS+AyY5qVTqqUWbAAAAATMsSwAAA4KvwBTCyWpfIaiJXVL5YDnpFAAAAK0Im7wKYVCxcLw8BAGiWcP2ETa+SJPXHpiRJIxnmwAAAAEiuBSkRjSgWDdfLQwAAmiVcP2ErAUxv1AUwo1O0IQEAAEgugEnFw/XSEACAZgrXT9lKALPaTEqShglgAAAAJLkZMMx/AQDAP6EMYLrMhCSxFTUAAEBFJl9SOk4AAwCAX8IVwLStliR1lMcl0YIEAADgcS1IBDAAAPglXAFMqluS1F5yAQwVMAAAAE6mUKYFCQAAH4UrgIlEpVS34vkRJaIRZsAAAABUZGlBAgDAV+EKYCQpvUomM6zutrhGqIABAACQJE0VigQwAAD4KJQBjDLDWtUW1wgVMAAAAJLcEN4ULUgAAPgmtAFMT1uCGTAAAAAV2UKZChgAAHwU2gBmNQEMAADAtEyBGTAAAPgptAFMb0dCQxMEMAAAAJJrQWIXJAAA/BPOACY7or72uIan8iqWykGvCAAAtBhjzGuMMc8YYw4YYz5Y5/k/N8Y8Xvn1rDFmJIh1eqy1yhRKSlEBAwCAb2JBL6Dp0qslW9ZAuiBrpbOTea3pSgW9KgAA0CKMMVFJn5D0aknHJD1qjLnPWrvXO8da+/6a898j6cqmL7RGrujekKIFCQAA/4SzAkbSQDwjSRqcyAW5GgAA0HqukXTAWnvIWpuX9HlJdy1w/pskfa4pK5tHJl+SJKXj4XtpCABAs4Tvp2wlgOmPTUkSc2AAAECjbZB0tObxscqxOYwxWyRtlfTtJqxrXplCJYBhBgwAAL4JbQDTG5mUJA2NUwEDAAAaytQ5Zuc5942SvmitLdX9RMbcY4x5zBjz2ODgYMMWOJsXwDADBgAA/4QvgEl2SpK6oy54oQUJAAA02DFJm2oeb5R0fJ5z36gF2o+stZ+01u621u7u7+9v4BJnqrYgEcAAAOCX8AUwcTdwN2VzSsejVMAAAIBGe1TSDmPMVmNMQi5kuW/2ScaYF0laJekHTV7fHFlakAAA8F0IA5g2d1vMqK8zoSEqYAAAQANZa4uS3i3p65L2SbrXWrvHGPNRY8xra059k6TPW2vna09qmukZMFTAAADgm/BtQx1Pu9tCRn0dSYbwAgCAhrPWPiDpgVnH/mjW4480c00L8VqQmAEDAIB/wlcBE/MCmKlKAEMFDAAACDd2QQIAwH/hC2CicclEpUKWAAYAAEA1M2CogAEAwDfhC2CMcXNgChn1dyR0djKvUjnw1msAAIDAsAsSAAD+C18AI7mdkApT6utMqmyls5PMgQEAAOGVKZQl0YIEAICfQhrApKWia0GSRBsSAAAINW8GTDIWzpeGAAA0Qzh/ysbbpofwStLgOAEMAAAIr2yhpHQ8KmNM0EsBAKBlhTOAiaWkQlZrOglgAAAAMvkS7UcAAPgsnAFMpQJmbVdKknRqPBvwggAAAIKTqVTAAAAA/4Q0gElJhYzSiai6UjGdGiWAAQAA4ZUplJSKh/NlIQAAzRLOn7TxNqmYlSaHdH/kdxQdfDroFQEAAAQmSwsSAAC+C2kAk5YKU9LQs9pcPqbtww8GvSIAAIDA0IIEAID/whnAxFwLkrJjkqRtmT0BLwgAACA4rgWJAAYAAD+FM4CJt7kAJucCmF2lp1UuFgNeFAAAQDAyeSpgAADwW0gDmPSMAKbTZDR29ImAFwUAABCMXLGsJAEMAAC+Cm8AU8pJmZHpQ5kD3wtwQQAAAMEpla2iJuhVAADQ2sIbwEjSxGlZE9Mp2yNz7OFg1wQAABAQK6uIIYEBAMBP4QxgYl4Ac1LlZJf2lC9Scnh/sGsCAAAISLksGQIYAAB8Fc4ApqYCJpLq0pRSsoVMsGsCAAAIiLVWEfIXAAB8Fe4AZvykTKpLNpaSKWaDXRMAAEBAyla0IAEA4LNwBzATp6VUtyLxtKKlXLBrAgAACEjZWkXC+aoQAICmCeePWi+AKUxKyU7Fkm2KlQlgAABAOJUtM2AAAPBbSAOYtur9ZJfiqTYlRAADAADCiRkwAAD4L5wBTCxVvZ/qUjLdrpjKKuYJYQAAQPiUrZURCQwAAH4KZwAzqwIm1dYuSTozMhbQggAAAIJjJSpgAADwWUgDmJoKmGSn2ts6JElDIyMBLQgAACA45bJlBgwAAD6LBb2AQNRWwKS61NFRkiSdGRkNaEEAAADBsWxDDQCA70IawKSr95Nd6uwoSJKGx8YDWhAAAEBwygzhBQDAd+EMYGI1AUyqWx0xN3x3bIwZMAAAIHzKVoqQwAAA4KtwBjCRiBRNSqWclOxU1LhROGPjVMAAAIDwKVsrOpAAAPBXOAMYybUhlXJSskuyZUnSxOREwIsCAABoPmbAAADgv3DugiRV58CkuqSY2xVpigAGAACEUNlaEb8AAOAvAphk5/T9zBQBDAAACB83hJcIBgAAP4U3gImlJRkp0TldAVPKZZQrloJdFwAAQJNZiV2QAADwWXgDmHjaVb9EItMVMCmT1+mxXMALAwAAaB5rrayVDBUwAAD4KuQBTJe7X6mASSmn0+PZABcFAADQXNa6W1qQAADwV3gDmESHlOp2970KGBV0bDgT4KIAAACaq1xJYGhBAgDAX+HdhvoVfyDlKkN3owlZGaVMngAGAACEStmrgCGBAQDAV+ENYAZeWr1vjEy8TasjJT1xZiq4NQEAADSZVwFDBxIAAP4KbwAzWzyl3lhZR84SwAAAgPDwZsAYkcAAAOAnAhhPLK1VpkQAAwAAQoUZMAAANEd4h/DOFk+pO17SidGM8sVy0KsBAABoikoBDLsgAQDgMwIYTyytrmhRZSu9MMIgXgAAEA7MgAEAoDkIYDzxlNqjBUmiDQkAAISGrRT+UgEDAIC/CGA8sZTShgAGAACECzNgAABoDgIYTzyteDmnZCyiowQwAAAgJKYDGBIYAAB8RQDjiaVkilltWt2mI2cIYAAAQDiUvW2oaUECAMBXBDCeeFoqZLRpVZoWJAAAEBqWFiQAAJpi0QDGGPM3xpjTxpin5nneGGP+whhzwBjzhDHmqsYvswliKamY1UBPWqfGskGvBgAAoCmmK2BEAgMAgJ+WUgHzaUmvWeD52yXtqPy6R9Jfnf+yAhBPS4WsBrpSOjOZV7ZQCnpFAAAAvrOiAgYAgGZYNICx1n5H0tkFTrlL0mes80NJPcaYgUYtsGliKamY0brulCTp9Fgu4AUBAAD4z6uAYRtqAAD81YgZMBskHa15fKxy7MIST0ulvAY6E5KkE6OZgBcEAADgv3IlgSF/AQDAX40IYOr9uLZ1TzTmHmPMY8aYxwYHBxvwpRso5ipfBjrcw5PMgQEAACFgqYABAKApGhHAHJO0qebxRknH651orf2ktXa3tXZ3f39/A750A8XTkqS1be7hiVECGAAA0PrK3i5I7I0JAICvGvGj9j5Jb6nshnSdpFFr7YkGfN7mqlTAdEQK6kzGdJIABgAAhMB0AEMFDAAAvootdoIx5nOSbpXUZ4w5JunDkuKSZK39a0kPSLpD0gFJU5Lu9muxvqpUwKiQ1bruFAEMAAAIheltqAlgAADw1aIBjLX2TYs8byW9q2ErCkqlAsbbCekEM2AAAEAI2EoFDPELAAD+otvXU1MBM9Cd0kl2QQIAACHg7ZxACxIAAP4igPHMqIBJ6/R4ToVSOdg1AQAA+Kw6AybghQAA0OIIYDzTFTAZretKyVppcDwX7JoAAAB8Vq6838QMGAAA/EUA42lb7W6nzmig21XDsBU1AABodVTAAADQHAQwns4Bdzt2XOt7XDXMseEpaeqs9NlfksaOB7g4AAAAf1TyF2bAAADgMwIYTzwtpXqk8ZPa0tsmY6TDQ5PSkR9K+/9Vev77Qa8QAACg4aYrYHhVCACArxbdhjpUOgek8RNKxaNa3512AUzbIffcxOlg1wYAAOADL4BhBgwAAP4igKnV5QIYSdrW3+4CmA4vgDkV4MIAAAD8Ua60IBG/AADgL4pNa3UOSOMnJUnb+tp1eHBS9iwVMAAAoHXZ6SG8RDAAAPiJAKaWF8CUS9ra167xXFHlMwfdc1TAAACAFlQpgCGAAQDAZwQwtTrXSbYkTQ5pa3+H4ioqMnbMPUcFDAAAaEHlMttQAwDQDAQwtbrWu9vx49rW166NZlDGlqV4OxUwAACgJU3PgKECBgAAXxHA1Opc527HT2p9T1rbo5Wql427pakhqVwKbm0AAAA+qM6ACXghAAC0OAKYWp2VCpix44pGjK5oP+seb75esmVpcii4tQEAAPjAq4CJkMAAAOArApha7f2SiUzvhHRZakiTSktrd7rnaUMCAAAtpkwFDAAATUEAUysak9rXSOMnJGt1eXGvDpbXaSrZ755nEC8AAGgxXgAjkcAAAOAnApjZugak0WPSni9pzeQz+rvSq3Uw0+aeowIGAAC0GC9/oQIGAAB/EcDMtv5K6dCD0lffrULfTv1j6WY9OZxwzxHAAACAFmPltSCRwAAA4CcCmNle85+kW/9QiiUVu+NP1dWW1BOni1KikxYkAADQcspld0sAAwCAv2JBL2DFiSWkW39fuvX3ZSTtWv9D7Tk+JnWupQIGAAC0HG8GDPkLAAD+ogJmETsHuvTMqXGV29dM744EAADQKqa3oSaBAQDAVwQwi9i5vkv5YlljbVukwX3VSXUAAAAtwHrbUPOqEAAAX/GjdhGXDXRJkp5LvUjKDEvDzwW7IAAAgAaiAgYAgOYggFnE1r52RSNGT5S2uQPHfxLsggAAABrImwHDNtQAAPiLAGYRyVhUW3rb9MPJNVI0IR3/cdBLAgAAaJjydHs1CQwAAH4igFmCS9Z06unTOWndi6Xjjwe9HAAAgIajAgYAAH8RwCzBjrUdeu7MpErrrnABTLkc9JIAAAAaotqCRAIDAICfCGCWYPuaDpWtdLrzMik/Lp05EPSSAAAAGsJ7X4kABgAAfxHALMElazslSfu12R04sz/A1QAAADSOVwFD/gIAgL8IYJZga1+7IkbaPxZ3BzIjwS4IAACgQbwZvBGGwAAA4KtY0Au4EKTiUV3U2649w5UXJpnhYBcEAADQIGxDDQBAc1ABs0Q71nbo8dMlyUQIYAAAQMsoexUw9CABAOArApgl2jnQrcNnM7KpHilLCxIAAGgN0zNgAl4HAACtjgBmiXat75K1Ui7eRQUMAABoGZUCGBkqYAAA8BUBzBLt2tAlSRo3nQQwAACgZVhmwAAA0BQEMEu0riulVW1xnSm1E8AAAICWUS57AQwJDAAAfiKAWSJjjHat79bJfIoABgAAtAyG8AIA0BwEMMuwa32XjmaSsgQwAACgRUwP4eVVIQAAvuJH7TLsXN+ls7Zdyo5J5VLQywEAADhvlgoYAACaggBmGS7f0K0R2y4jK2VHg14OAADAeSszhBcAgKYggFmGbX3tyie63QPakAAAQAvwZsAYkcAAAOAnAphlMMaor2+de5AZCXYxAAAADWBVmQFD/gIAgK8IYJZp4/r1kqTJ0cGAVwIAAHD+mAEDAEBzEMAs08WbN0mSnn/heMArAQAAOH/lMjNgAABoBgKYZXrRVhfAnDhJAAMAAC58ZSpgAABoCgKYZWrv7pMkDQ+dCnglAAAA58/bBYn8BQAAfxHALFc0rmykTZnRoemSXQAAgFrGmNcYY54xxhwwxnxwnnPeYIzZa4zZY4z5h2av0WOtlTFuswEAAOAfAphzUEr2qK08rv2nJ4JeCgAAWGGMMVFJn5B0u6Sdkt5kjNk565wdkv5A0o3W2l2S3tf0hVaULe1HAAA0AwHMOYh1rFa3JvTjI8NBLwUAAKw810g6YK09ZK3NS/q8pLtmnfMOSZ+w1g5LkrX2dJPXOK1srYhfAADwHwHMOUh09Ko/OqkfP08AAwAA5tgg6WjN42OVY7UukXSJMeZ7xpgfGmNe07TVzUIFDAAAzRELegEXItO5Tutjz1ABAwAA6qmXZsweHBeTtEPSrZI2SvquMeZya+3IjE9kzD2S7pGkzZs3N36lkqwsA3gBAGgCKmDORdd6rS4N6dDguEam8kGvBgAArCzHJG2qebxR0vE653zVWluw1h6W9IxcIDODtfaT1trd1trd/f39vizWUgEDAEBTEMCci64NitqiejWuRw6fDXo1AABgZXlU0g5jzFZjTELSGyXdN+ucr0h6hSQZY/rkWpIONXWVFeWyVYT8BQAA3xHAnIvOAUnStuSovrUvsJl5AABgBbLWFiW9W9LXJe2TdK+1do8x5qPGmNdWTvu6pDPGmL2SHpT0AWvtmSDWywwYAACagxkw56JrvSTpVRtL+h9Pn1KpbBXlrSMAAFBhrX1A0gOzjv1RzX0r6XcqvwJVtsyAAQCgGaiAOReVAOb6vpyGJvJ6/CjDeAEAwIXJWqsIbyQBAOA7Aphz0d4vRWK6pG1csYjRN/bShgQAAC5MtCABANAcBDDnIhKVOgeUmjqpa7et1refPhX0igAAAM5J2dq6+2YDAIDGIoA5V50D0thxvXx7v549NaHT49mgVwQAALBsVpKhAgYAAN8RwJyrrvXS2HHduL1XkvSDg4FsXAAAAHBerGUbagAAmoEA5lx1bZDGjmvXQJe603F978CQtPc+ac+Xg14ZAADAkpXLzIABAKAZ2Ib6XHUNSIVJRQvjun5br763f0j2yAdlEh3Srl8IenUAAABLUqYCBgCApqAC5lxVtqL22pC6xp6RGXtBOntIKhWDXRsAAMASlS0zYAAAaAYCmHO1+mJ3+80/1s2bYnpF5CfucbkgjTwf3LoAAACWwVqrCK8IAQDwHT9uz9X6K6TX/Kl04Jva/OW79LrkI8qZlHvuzIFg1wYAALBErgWJChgAAPxGAHM+rnun9Nb7ZMaOa3v5OX2hdIs7PrTf3WaGpYf+XMpPBbdGAACABZStRPwCAID/CGDO15YbpDd9TuM9l+lThZ9RIblKOrNfyk1In/0l6ZsfkZ57KOhVAgAA1GXFLkgAADQDAUwjbLtVsd/6nl6IbtDJ2EZp6IB03/+CK9gAACAASURBVLulY4+658dPBLo8AACA+ZStFfkLAAD+I4BpkHQiqhsu7tUT2X7Z4z+W9nxZuvF97snxk8EuDgAAYB6WGTAAADQFAUwD/eyLB/Rkdo1MYUqKt0s3vldq66UCBgAArFjlMi1IAAA0AwFMA/3MrnU6Yja4B7vvltpWS50D0sQpafKM9IW73S0AAMAKQQsSAADNQQDTQN3puCLbb9X/jtyh8o3vdwc717kKmMP/Ju35knTowUDXCAAAUKtsqYABAOD/b+++46Oq0j+Of86kEtIIgQQSeg+9iA3BLlbUta91rbu6ruu6u7r6W3f9ras/V1ld66prL9jXBgqKSlM6SO8EQkkIKaS3ub8/ngmhhBCESUL4vl8vXjNz5+bOmTM34Z5nnvOchqAAzCF2+uDu/LH4SmZmBi5kopOhIBO2r7XH21Y0XuNERERE9uB5Hj5dEYqIiASd/rs9xE7rk0R0RCgfzMuwDTHJNgUpOxB42ba88RonIiIisge/5+FQBoyIiEiwKQBziLUID+Hs/u0Yv2gLRWWVFoDxqmDDTNtBGTAiIiLShHiAT/EXERGRoFMAJgguGpZKcXkVXyzeakV4AfI32G3OGqgsb7zGiYiIiOzC74FTDRgREZGgUwAmCIZ1akXn1lG8NWsDXnRSzRPtBoK/EnLWNl7jRERERHbheZ4yYERERBqAAjBB4JzjuuO7MDc9l9nbI2qe6H2O3WZrGpKIiIg0DX7P0ypIIiIiDUABmCC59KgOtIuL5NEZeTUbe50JONWBERERkSbD79cy1CIiIg1BAZggiQwL4VcndWfWhgIqIluDLxTa9Ib4DloJSURERJoMv+eh+IuIiEjwKQATRBcPTSUmMpQsrxW06gwhYdC6u2rAiIiISJPhecqAERERaQgKwARRZFgI5w5sz5slR1Pa/wrbGJcK+Zsat2EiIiIiAcqAERERaRgKwATZJcM68Ez52XwUdbFtiE2FoiyoLNv3Dy14Cxa+0zANFBERkSOaivCKiIg0DAVggmxgahw9k6J5e9YGPM+DuBR7YkcdWTDTn4Av7obK8oZppIiIiByxPFAGjIiISANQACbInHNceUwnfszIZ96GPIgNBGD2NQ3J8yA/A0pyYPVXDddQEREROSL5VQNGRESkQSgA0wB+NsSK8b48fZ3VgAHLgCkvgh2bLehSrTQPygvt/o/jGr6xIiIickTxPA+f4i8iIiJBpwBMA2gZEcplR3VgwuKtbKW1bczPgHE/h7F97N+2lTXbAeI6wIovoCSvcRotIiIiRwTVgBEREWkYCsA0kJ8f3Ykqv8dny/KgRSvIXQ8bvocuo6B0B0x91Hasnpp01PVQVQab5jZam0VERKT58/ttyrSIiIgEV70CMM650c65Fc651c65u2t5/lrn3Dbn3ILAvxsOfVMPb50TW9K3fSyfL9piKyGtmgSVpTDsOhh6LSx6H3LTIX+j/UCXkXabt6HR2iwiIiLNn19TkERERBrEfgMwzrkQ4GngTCANuNw5l1bLru94njco8O/FQ9zOZuGs/u2YvyGP0qhkKNxqG1OHw7G3gvPB90/bFKSQcEgeCL5QBWBEREQkqDwV4RUREWkQ9cmAGQ6s9jxvred55cA4YExwm9U8ndW/HQBry1vZhtgUW5Y6LgV6nQnLP7cMmNgUCAm1gr156Y3YYhEREWnu/J6nZahFREQaQH0CMCnAxl0eZwS27elnzrkfnXPvO+c6HJLWNTNdEluS1i6W6dsibEPqsJonu54IOzIgfUbNSknxnZQBIyIiIkHloQwYERGRhlCfAExt/yN7ezz+FOjsed4A4Cvg1VoP5NxNzrk5zrk527ZtO7CWNhN3ntaTxYUx9iB1eM0T1TVfCrbYCkgA8R2tLsyuPA+Kc4LfUBERETkiKANGRESkYdQnAJMB7JrRkgps3nUHz/O2e55XFnj4AjC0tgN5nve853nDPM8b1qZNm5/S3sPeqWlJxHYdRpEXwfiSNEorquyJ1t0hxqYo7cyAadUJirKgosQef/80PNYbHumq1ZFERETkkFANGBERkYZRnwDMbKCHc66Lcy4cuAz4ZNcdnHPtdnl4HrDs0DWx+fnVxWdxaev3+dWkEq76z0yq/B44V5MFs+sUJLBpSJlLYeJ90LobhEXBnJcbp/EiIiLSrGgVJBERkYax3wCM53mVwG3Al1hg5V3P85Y45x5wzp0X2O1259wS59xC4Hbg2mA1uDloF9eCT399Ag9e0I/Z63N5ZcZ6e2JnACZQYmfXAMzE+yAiBi59A/peAEs+grLC+r/oovdhzeRD9h5ERESkebAAjCIwIiIiwRZan508zxsPjN9j2593uX8PcM+hbVrz5pzjiuEd+WppJo9+uYIz+yXTvu8FULAVOp9gO8V3tNtZz8Oar+H0ByEqAQZfCQvegDcvgs0LoKrMtp335L5f8Ms/2TSnbicH/82JiIjIYcPvt+sSERERCa76TEGSIHHO8cCYfpRWVvHO7I0Q3hJG3gWhgVWSopMgJAJWTYS2aTD8Jtve8RhI7GV1YPr9DNLGwLzXYNlntb9QQSYUZkL2qoZ5YyIiYkXUl/y3sVshsl+epiCJiIg0CAVgGlmHhChGdE/k/bkZVgtmVz4fxHcA54MxT0FouG13Dq79DO5YDOc/DRe+AEn94PM74bt/QM7a3Y+zdZHdFmVBaf6BN/KjW2DaPw/850Sk/vI3wZuXaJWz5mTaP+G9a6Bo+773SZ8BX/3FqqCKNJLqUnQiIiISXArANAGXHtWBTXklTF+dvfeTR98CZz4CKXssLBXdFmKS7H5IGIx52orzfvM3eO18qCyr2XfrjzX3s1cfWOP8VVZvZuWXB/ZzInJgVk+CVV/CxlmN3ZKmqawQ1k0J7mt4Hrx3HayYcGiOt3m+3W6Yse995r9pgZq13xya1xT5CTxUA0ZERKQhKADTBJyWlkR8VBhPTV5NUVnl7k8Ov9H+7U/7QfCbBXDlh5CXDrNfrHlu6yIICWTPbD/AaUh5G6CydO+sGhE5cDlrbVpKbbKW223uuoZrz+Hk+6fg1XPtb1Kw5KXDkg9h+hMHf6zKMshcYvfXT9/3ftsDQfHv/nHwrynyE1kGjAIwIiIiwaYATBMQERrCvWf1YU56Dpc+/z35xRU//WDdT4Fup8B3j0BJnm3b+iN0PQlcSM3Ffn1lr7TbwkwoK/jp7RI5HBVth7evsOLYh8I7V8Gnt9f+3LZldpvTxAIwWxZC3sbGbgWs/tpu0+vIJjlYGXPsdsP3NiXsYGQuAX+FBb/Tp+17v+2rITLOsmTW17FfbTbNg6/+CgvHQXnxwbVXjmiqASMiItIwFIBpIi4e1oEXrxnG8i0F3Pfx4oM72Cl/htI8WPCmpe1vX2NTmFp1OvBCvNtW1NxvagNDkWBb+w2s+PzQTMEryobMxbBtZe3PN1QGTHkxVNUzyOv3w+sXwMT7gtumPe1ZD6Ukz4qOA6TXkU1ysDbNBV9gccClB1k8t3r60YBLYOtiKMmteW7dFOvTklwozoZjb4MWrWDuK3Ufsyi7pm+qKuGjm2HaWLud+/LBtVeOaH4PTUESERFpAArANCEn907ijlN78OnCzXwwN+OnH6j9IOhwNMz+D2yaA3iQ3N+Woa4tA6Y4B5Z+Yhf2xTmWCl89QNstAKNpSHKEqa6ftGXBwR+rOnBQsBkqSnZ/riQXCgNZNocq0PnhzfDN33ffVlkG/x4J//1V/Y6RtQSKt9cU8g62qgr4/C4YmwaF22q2r58KXpWtDBfsDJiUYZA8ABZ/cHDH2jwfWiTAwCsAb/d2f/cIzHgS1k21x0n9oO+FtpJd/iZ49TyYcDfs2FLzM+nfw2O9YPxd9njBm5ahePGrEJtq2TAiP5FfGTAiIiINQgGYJuaWUd0Y3jmB37+/kOenrMH7qStjDLsectbYlIeWbaHTsdC6h2XD+P01+62fDs8eB+9eZRfwSz60Qr5rJtvz2Sug/RC7n7Pm4N6cSENbM/nglgHeGshG27Lw4Nuy6/SS3PW7P1ed/dI2zeqQ+KsO7rXKCmHx+5ZRsevfkJn/tjpQS/9bv9WWqove5qyF8qKDaxPY3549a+D4/TVtfPdqmP2CBanmvVqzz5pvIKwlDL/JgsiFWbUfv6qi9ho7KyfCuJ/XTMusTWW5fc6pw6D/xZYNU/251EdlObx7DXzyazvnNs6E9oPteC0SYOpjlrVSuK0mGDfvNbtt3R0GXg6VJfDK2dbvs56HF06y4xZth/d/YSvizX7Rsmcm/y+kDoe0MdBuwO7F1kUOkN/vqQaMiIhIA1AApokJDfHxyi+OYnS/ZP4+fjmPf3WAU4aqpY2BqNZ2/6oPLb09sbtd4G/8wbaXF8E7V9pFPUDmIsgK1KFYNckGRdtW2PSllm2VAVMbz4OPflkTsGpuSnItaNeQNs2DL++F7585+GN9eR98dsdPD2hkBgIwWxfXPm2nrKCmNlJ5cd11ONZPg6hEu79nlkt1/ZdeZ0JVORRs4aBkzAZ/pdVuqs5e2boIpjxqQZ6q8rozPErzbeBfnaGBV3swoqpy3/VhKstgwh9h+ec126aNhSeH1BTSraqAZ462YELeRlgxHkb+HrqeCHNesuNnLrFjdDnBtoNNCavt85j+BDw5dPcAV95G+OAGWP6Z3f74Lnz9wN6fQeZiqCqzv3eDrrDaLXP+YysivXPV3vsXbbeslGqznrfA1qIPbOnp7JUWfAmNgLMfs4DO1Mdg2Sfg+W2q0+qv7O9vq862b0I3m4J27K1w6et2HqyeBF/db1OVrptg7ZvxpAV1zhlrawe3G2jTS8sKa/8sRPbD0xQkERGRBqEATBMUFR7KU5cP4ZJhqTzx9SpemPITAh9hkXD1x3DjZJt+BNDjDIhNsfT2ua/Yt68lOXDRSxAebQOdzKW27+qvrPBo2Q5o0wsSutoAZOE79m0yWNDhx/fqV6DT82Dy32DZp/vex++3b3nnvX7g77exbJ4PC9+ywWJz9MEN8J/TDiyAUbgNxvaFtd8d+OtlLoUXT7UVbybeB/kHMRWvKNum0JTk1tQPAVuKfc/Msm0r4OPboCCzZlthlgUw2g+xgfm2WgIQb11qmRUA7/wcXj9/72ODDdazlsLAy+zxnsHMrOX2O9h5ROD5g5yGlD6jJrC6aqItrfzcCJvGc/ErkNTfprCABY1WfVWTGVdWAM8cBy+eYsfpMirQxiW7v7es5XZuPDEAtuyRfVFVCR9cDzOfg6ljbVtJHkz/lwWGqmvqLPvUAhXzXq9Z+nnApZbpsmMTvHIW/HuU/cyIOy3QEB4Dn9wG/+y3+/nhefDjO1b4dtYLUFFqxWnfvtze9wm/s2DGhzdaIOTJofb3y/Pgs99ae8ECIS0TbUrQgrfg/estaPLcCTXndEWpfdYvn2kZN0XZNq2o+2nwx3Vw07dw+Tir7QLQ70Lofwl8+3cLNrXuDt1OBjyI7wSh4RZIOe42y2o56V77ex2VCDOesvcx5Bpr28/fhxu+hltn1vxtTx5gx6pedUnkAPk9D8VfREREgk8BmCbK53P8/YL+nN2/HQ+OX8bfxy878OlIyf0hsUfN47gUuGUadB0Fn/4GJj8InY6HjsdA2z42+M1aagPB3HWw6D37uTa9oHU3G8R+dBO8dbEtB/v6BfDhDTaQqQ7c7Kk0376V/eEZmPIP+OgW2LG59n0XvWvfyv/wrD1eOfHAiwbvqrIMNsz86T9fH8s/s9v103af2tVQygqDtyxvxhwLxBVvh8211EApL4aPb937M1oxHnZkwJKP6j6+58G0xy2AMedlG7Qv/9yyA66bYLcHE9haP7Xm/qpA0HDdFHhqqL2vaisnWtBn/uvww9M126szR4ZcZbd79kH2aptKsn6a1epYN8WmnawJrNZTXmxBzvJiWPaxbet7ga14s2eh3cwlNYFOqH8h3qoK+OE5eOUceLSX/U6u/NLa1W6g/Q2YOtamFh7/G7hjkb3OoCsCwcNxFvR882cw7xU75pRH7fPLXAJl+TD4Kpv+s3URvDTapgllzLXgS146hEbC909b5tInt9s5OfNZC64k9YfN82y60/dP2/GiWsPKL+y1Zj4HvjAoyoKpj0KrLhac6Dna2l6wFYZdB7fOgo5HQ0gY/GICnD3WgsOf3lETFMpcbMGcyDgL6LzxMytOW7wdLngOTv4fGPOMBTDuXGZ/86Y+Cht+sPMsKtGCNHEd7HhH3QDlhRAeBddPgui28Pmd1ucT/lAz5WfBm/DNg7bvGQ9axkv7wZbNFBlb81mNeQqO/qUFBPtfbH97wd5vtWG/gBsm2WuGhEL/i2x1JM9vwRmAqAQLxOw6Wm430G41DUl+Ig9UA0ZERKQBhDZ2A2TfQkN8/OvywcRHhfH8lLWM7NGGET0SD+6gUQlw2dvw3rW2usuIO2172zQbjFWVwTG32kB00v9AXEcbTGycCZWlNkBKGWr1JY6/w6Y6vXK2ZSycv8eUkZJc+ya9KMsGSV1G2XG+uBsueW33fcuLbDlVX6h9054xF8ZdYbVrrqkja6YuM560b5tPuR8iYmxa1ZkP2yC3rNAGq4OvhKHX7P9YlWU2aI9LrRnsgBXNDAm395q52GoxHAr5myAmGXwh+95n83yrOVGYad+IJ/erfb/qAeqBfr353f/ZYLY031YDSh26+/ML3oT5b9jA9bS/1myvHlxX1w+prQ3lRTD+D7DgDWjZxgJZJTn2sylDodNxNoCd+wqM/INldFUryrbgUPdTbEC+L+umWLZEm1722Z98nwV6wIIm3U+1KStf/cUG+y3ibeB+4p/s9aoDMH3GwMQ/B+rAXGVTcwB+HBd4b1XWV/5KOxe++4ctBT/9cdu+bYVld7QbZO8toevuGTB5GyxgcsLvrJiqL9QyYErz4Yt7bDpNm14QGW9Bgw7D7bzbtgw2zrY6Tcn9LbC64QebVuh5cPTN1p6tY+29nvrXmv4fdp0Fyj662R7Hpth7rKq0QMmgn9vS9T88bf2clAbz34SKQB2YpZ9AbHv4xZf2uz/7RcuIK8qy6Y4L37bf95Pvs0DNko8ssNrnPAtwzH7Bfp82zrTfz6mP2Xl89C+tjS7EgsW1Se5v/6rK7W/J4g8sULH4A/u585+DcZdb4GLMM1ZbxRf4rmHwz2uOM/xGC0SPvwsiYuHq/0J4y5rnU4fB6X+zQEnKELs/7nL7e7dxpv3t3LLQsm1K8yxrp02vfZ+PoRH292f4jRDfsSagt2sAZk8DLrEgVd8LbJrSvsS2t8DWoSgWLUckK8KrCIyIiEiwKQDTxIX4HP9zThofzMtg0tKtBx+AAUt3v+RVq/dSHTBI6mvBF4DeZ1n2QGUZXPWRBS/aD7HB3PnPWsbM6Ichuo3tP/gq+wa528n2rfmoP0BYC5j4PzaoGn6jfQN+9qNWJ2Hy36x2Qqdja9o063krvHnek1bE8sMbbSrBuik2QI3vuPt72DjLvtm/8Pndv2XeVXUA4OtAcMAXCs+fCOc8bu8vY5YFOIZeY1Nsdg12+KssUyemnQ0up/zDajAAdD7B+iV3vQ1+j7sdZvzLjtlugGXCFGyxQdGBXNB6nu2fvwn+NdgGXRf+u/Z9t6+Bl860gFpknNWcuH6SPa5uf2WpDSi/fdgGp7fOqhmI7s/2NZY1cvJ9sPRjWPstjLyr5vmqSht4Q02mydZFNrhe8w1ExFnR5vwMG9x/eBNs+N6yMMoLYWbg8x75BzjpT5atMONJm6Zy0r12vKNvtiDB1MdscDv5Adg0PzAVxg+j/w+OuWXvtk951DIBNi+wQE6H4RaIy1xSk7G0aa5Nr/nqfkg7387rjNnw2nkWOIlqbbVCYlOhZWsbgC/9GAZdDv+91YIjXpWdCxlzLNMlNNKyLCbea+fL908HskMC/XTRS/b5tupSs0Qx2M86B0OvtayH+I429WXR+9ZH7QZZAKOswII81WLaWZDusreg99m2rTjHpg7lrLX33rqHBQnOe3L3czGshU2R+fAmqw019Dorxj3h95DYC079i2V8DLjY9m+bZv3Tpjcc92sLClz4AsR3gGN+ab+/5UXQ8TjrP7DfzfZD7Pz88l47H0/6k03t+uFpm74Vm2qZJlnLLAOu5+n1Oz/BzokFb9pKT73OtOlE3U6y+yP/YKvBVfdLbfpeaAGuzMXWhl2DLxCYEvTrmse9zoQOx1gNraHX2me97BPLeIqMh1F/rF+7W3ez2/aDLDDWa/S+920/BM79F/Q4re5jOmfTkPacCiZST34PFeEVERFpAArAHAYiw0IY0T2Rr5Zl8ZfzDtFKBSFhu2drJPWtud82Da4bD6EtbEAI9i34H9MtNR5qgi9gA7DZL9TUUCgvsukC81+3AfdpD+yy7602ZWLqo9Dpg5r9Zzxpg5EhV9u0lJw1tjRr5mJY8DacuMfgZupYWDnBshfOGbv3+6sstyDNUTdAWBQk9rT6Gu9dC+9fZ/tEtbZBZdF2W22kbVpNQOebv1sbQyNt4NhllNVz2DTHshpWTawZ7Bx9s023WDfV2v/25RaUiGkHF/zbMhP2p3QHPD3cgjllOywY9uM4C4aljdl9X8+D8b+3gNL1k2yw/dp58GgPqzMx5mnLDFj2KfzqByskWrTNBv17ZrHsS3Xh1AGXWtt+eNY+p+pB6rKPLQCVPMACHau/hjcutGBLZQmcdA9M+rMFwYq32+A6vmPNErqpR8HFL1swDyy48/KZdr/nGXbbZZRlYkx5xFbEKc23gEfvsy0wM/tFO2c+vd2yE1KGWP2Z7x6pCSYOv9Hqcnzzd3jhFMua6HS8TZdZ+jGERFjmVngUdBlp58m3DwXOj0Q7t8ECji+PhhdOtt+LuFRbTej0By0wueZr6DTKggLpM2xKivPBteMtkysixjJpwDJgln1i/Vqw1QIwPU63YAZYZtni9+0cvvDfFkgBC+xtnGnnbI/ToW3vvT+3qAS44j34/knLYAmPsiLctYmIhsvfqnl83QQL8KQM3TtwWF1rZOTvLdtk8JU1z7XqbAGs+I4WEHpquO3fZZQdp8soe7/9LrIMnoRuliUTEWPZbZGxcPzt1l+dRtTe1tr4Qqw9715tda12ZMCYQKDp5Hv3//ORsYE6L29YAGp/nLOpTGu/gSHXWjCz11l2Lg/7RU3ws75CwuDK/Sx17Vz9MvTAMvOWfWrnSX0DrSIBnpahFhERaRAKwBwmTu2TxFfLsliyeQfhoT56JsUc2hdom2a30cn7HkhUB1/2lNAFznrUsi6yV8Ksf9s34h2OhhPv2fsYx91mgZNNc22wN/cVG6SP/IPt0+tMyxo48W47zpz/2GC39znQ93yrt7Fqok1dmfOfwJQFH0QnWZp/bIoNkCtLbPCXdl7N69842V5v01ybDvH2pfDFH62WRV46/Od0+NmLVrOm8wk1gZu+F9jrdDsZ5r5qAYmti6xQZlyqrdCy4C2rh1OUDSN+a9/If/vQvgMwVZU1Aa6F4yxrZvL/2sC0y0jLePj0NzaYra4NAjXfuo9+2Or6xKXAzYFla+e9Zt+wz3nJBtPvXWvBF7CAlS/Epop0O9kGbPsK5q0Yb/U74jvayjMz/mXTc46+2YqjTrgb2vSxqUevX2C1P8Ja2ucYHg3Db7bpPZMftCyOPufCxa/BlvmWJRPddvfX63ScBUbyN9YM9p2Dc/5p2Tg5ay0omDK05pz76GarRVSwGT7/nU3Dmv2iBV8ufcOmHQ241F7r2s+tXk1se8vYSp9u06e6jqoJKjln0/OyV1jfxKbU9E9SGlzxrtX+OPWv0PFYSJ8GXU+2z23N1xbwqM4u++IeK+Ta6Vi4IbDSTfVnndDFPptHe1hwD2zZ+GpDr6l90O3z2fF2zRyrTWJ3OPeJuvepTftB+35uwCWWNdP3gtqfry4uDBbw2bXv0sbY7+uJd9vj0HD7PWzRyv6Bfeb7yvaqS+9z7Xd00xwYeEWgsO0BOOXPll2yr+l7e0roYv+qhYbb59sUnPLn3acCihwAv1ZBEhERaRDugAu7HiLDhg3z5syZ0yivfTjK2lHK8L9/TUxEKAVllTx1xWDOGdD+0L7IY32shsHV//3px6gogdfG2ED3/Gdt0LansgJ4vL9lGIz6A3x+F7QfWFPrJW+DDY5H/sEGbh/dYt8WF2fb9JSKYpj2T7h5qmU/ZC2zTIay/JrXSBlmg7Lfr7GBcK1tLYX/6xSobdPZBvvjfm5FNj0/3Da7ZrrArr5+wKbFANww2bJKtq20oFFJDgy+2qZSTHvcprjcNtcGxdU8z5ZGXjHBsg4SusLTR9tr5m+09lzyumUlvXiKLQF+3QSbClNZBk8dZUGam76rGdSDffP9wklWByIk3LINtiy0TJ/W3W16T2WpBZrAMnQ6HW9ZCyW5Fkga9Ue7/2gPyy446U/2mq+cbZkXbftaHZCi7XDj1xagebijZZYce5sVdy3Jg87Hw4c3WyHno64P1OGJrvvcKc6xLJvqTJBqVRUWsNj1XKoohbF9rL/7XmhFZo+51bKGUofDFeP2Pr7nWZAwL92WQgb7zIf9ou527U9uumVhXPIatOq0//23r4E3L7K+7zLS+r/zCQdeo+dw4Xk27SziEAeNq62aZL9rl75+4FkoUm/Oubme5w1r7HYc6YJ17dTlns/59UndufP0OuoYiYiISL3Udd2kAMxh5PLnf2DNtkKiwkOoqPL4+nejiAyro0jrgVr7LbRIOHSFZOuSPgPevsymlbTpDT9/b+86L7uqLLNAzJLAdIpOx1tGRHXdFLDgT1U5vHmJ1WlI7AW3zaq7Ha+db1MKTrkfTrjTlpl96xIr3Hnu47X/TM46+FegfkNdUwgKtsLYNJuGder9ts3vh2/+ZgEcX5h9mz7wMgvqjHnGinkuHGcZAiFhNq3p9fMtoDL4SptONf1xuPJDmxa2V79+b1Nlht9kGUOvnQdH32LBlYn3bWRRKwAAIABJREFU2T6XvG6BjpUTbFpSYZbV6SjMtKVuY9rBdw9bgKc6K8JfZVk1yz61AE115gDAy2fZ53n7/N2zA0p3WLAtLqXuz+CnWvyBLYF+3O3wn1MtqykyDq76r01H2hfPg0e6WvDmzmUWLBSROikA0zQE49rJ8zy63DOe35zSg9+e1vOQHltERORIpABMM1FaUYXPOeak53DFCzO549Qe3HHqYXyxlLXMMiSO/40NnPfH86zg6YYZtsrMvqYNZK+G50bY8sFn/aPuY8583qZD3T4fYpJsW9F2WxGnrhWIlo+3QFVcat3Hf+syq9sx+iGbHjXzeWv/4Ktseszr51t2R+vutupLbRlDmUtgxlPWV/4Km2ZxVR1LPG9dbJlMvlAr3tr1RAuaPDnEpmlc8GztP7drZk9yf8swqk9WxtpvbaWfo2/e/77BUpJrAa/WPXbPCtqX966zgNN144PfNpFmQAGYpiEY105+v0fXP40//K8pREREmggFYJqh34ybzycLN/PEZYM5b6C+wd9LzjqberS/aQ9+v01dqq5FcahlLoEPbrSVe8AyjE57wIrL+nxWz6ai2OqihIbXfawdW6w4a9r5e0/TqY/0Gbaizr5q+XieLQ0cFmUZJLUFg5qLyjKb8tWc36PIIaQATNMQjGunyio/3e+dwJ2n9eT2U3oc0mOLiIgcieq6blIR3sPU//1sAFvyS/nNuPk8NH4Z14/owg0ndN3/Dx4pdp0KUxefL3jBF7A6LrdMs4Kt4S2tsO2ugZbYdvU/Vmy73ZfFPVDVq+nsi3P7X+62uQiNaOwWiIg0CdVfw2kVJBERkeDTWpWHqciwEP5zzTB+d1pPUuJb8PCE5azdVrjz+coqfyO2Tnbj81mx1ZSh+89yERERaUD+QCa0a66FwEVERJoQBWAOYzGRYdx2cg+evXIoEaE+/u+L5QBMWLSFwQ9MYtqq7EZuoYiIiDRl1TPRtQy1iIhI8CkA0wy0iYnglyd248slmTz9zWr+5+PFFJRVcsc7C9hWUNbYzRMREZEmqjoDRlOQREREgk8BmGbixpFdOS0tiX98uYLc4goev3QQBaUV3PjaHPKLKxq7eSIiItIE+ZUBIyIi0mAUgGkmIkJDePbnQ7hlVDf+cl5fzh+cwhOXDWbJ5nwue+EH8ksUhBEREZHd1dSAaeSGiIiIHAEUgGlGQkN83H1mb646phMAo/sl8+I1R7Eqs4C7P/iRxlpyXERERJomL1CzXxkwIiIiwacATDM3qmcb/jC6FxMWb+WBz5byw9rtjJ20kk8Wbm7spomIiEgjUwaMiIhIwwlt7AZI8N0woivLtxTw8vT1vDx9/c7t01dl8+AF/QgNURxORETkSFSdG6sMGBERkeBTAOYI4PM5xl46iLvO6MWPGXkM7ZTAKzPW8fQ3a+jWtiU3jezW2E0UERGRRqBVkERERBqOAjBHkPbxLWgf3wKAu07vxYqthYydtJLRfdvRsXVUI7dOREREGlrNFCRFYERERIJNc0+OUM45/vf8voT6fIx+Ygo3vDqHGWuyVahXRETkCOJpGWoREZEGowDMEaxdXAvevvEYLh6aysKMPK54YSbXvzqHvOLyxm6aiIiINABNQRIREWk4moJ0hOufGkf/1DjuOasPr3+fziNfLuf0f05hQGo8Zw9I5oLBqY3dRBEREQkSvzJgREREGowCMAJAZFgIN47sylFdEnhq8mpWZRXw23cymboqm9PTkujTLpaOCVGaIy4iItKM+P1ahlpERKShKAAjuxnUIZ4XrxlGld9j7KQVPPvtGj6ctwmAxOhwBndsRbc20bSPj+S0tCTaxbVo5BaLiIjIT1VdA0ZfsIiIiASfAjBSqxCf4/dn9OaWUd1Yl13Eok35zEvPY96GXL5dkUVFlcf9nyzhoiGp/O/5/YgMC2nsJouIiMgB8lANGBERkYaiAIzUKSYyjAGp8QxIjefnR3cCwPM80rcX88YP6bw4bR2rsgp58IJ+pG8v5uMFm4gIDeGSYR0Y0SOxkVsvIiIidVENGBERkYajAIwcMOccnRNbct85aQzp1Ip7PlzE2f+aBkC7uEjKK/1MXLqVD355HH3bxzVya0VERGRfqldBUvxFREQk+BSAkYNyVv92HN89kbdmbqB1y3AuHJJCbnEF5z45jZtem8ujFw8kPNTH0s35nD2gPdERoWwvKlPtGBERkSbA27kMtSIwIiIiwaYAjBy0uBZh/PLEbjsft4mJ4MVrhnHTa3O4/IUfdm5/eMJynHMUllUyonsio/sl0zMphuFdEmo9bmFZJVFhIfg0MV1ERCQoNAVJRESk4SgAI0HRLyWOyXedyHtzNhIRFkLv5Bje+CGdEJ+PpNgI3vhhA9NWZwNw71l9uHFkV8C+iXPOkVVQyllPTKNH22he+cVRRISqyK+IiMih5vdUhFdERKShKAAjQRMZFsJVx3be+fiRi+J33v/1yT3ILizjgc+W8uD4ZSzMyKNleCgTFm+hd7tYwkIcO0oq+H7tdm55fS7Hd0/kvIHtaRsb2QjvREREpHny++1Wy1CLiIgEnwIw0ihCfI6k2Ej+eckgYiNDmbw8i/ySCk7u3Zapq7IpKK3kL+emUVHl8fAXy/lmxTY+/XEL//3VcTjnyCsuJ317MX3bxxIa4mvstyMiInJYUhFeERGRhqMAjDSq8FAfD104AKiZfrQpr4RZ67YzZmAKPp/jmuM6897cjdz70WLGzd7Idyu2MWlZJlV+j4SW4dx/bhpjBqU08jsRERE5fKkGjIiISPApACNNRnX6c0p8Cy4YnLpze3ioj0uHdeClaeu458NFhIf6uOGELvROjuHl6ev504eLGNKxFXPSc+jXPo7UVlHc+NocosJDuPb4zhzXLZHNeSV8tSyTfilxDEqNV2FfERERVANGRESkISkAI4eF0BAfD4zpx9hJK7n/3DQGpFo9mWGdEjjj8SmMfnwKReVVJLQM55iuCUxbnU2rqDAmLs3kxhO6MH7RVjbllQBwcu+2PHrxQN74IZ1jurbe5ypMIiIizZ1WQRIREWk4CsDIYeP47okc3z1xt20dEqK47+w0HvlyOXef0oPnp6xl/KKt3DSyK787vSf3frSYF6auIzYylLduPJpFGfk8NGE5x/z9a8qr/ISH+njy8sGM7NGGFuFaaUlERI4sqgEjIiLScBSAkcPeFUd35PLhHXDOcUKPRMYv2sJvTulJeKiPf1w0gOO6taZv+zh6JcdwXLdEWkdH8O6cjdw8sitjJ63k5tfnAnDDiC788czevPZ9OsM7J5DaqgWXPf8DbWIiuOa4zhzXrTUtI/b+lSmv9BMW4vZaQSIjt5g2MRFaQltERJosb+cUJEVgREREgk0BGGkWqoMffdvH0bd93G7bLxySutu+Fw1N5aKhtm14lwQmLc1k6qpsXpy2jknLMknfXkxMZCi9k2NYl11EXkk5N742hxCf45ZRXblpZDfGTlxBdmE5eSXlzFqXw40ndOUPo3vvfI35G3K55N/fMzA1nld+MZzoWgI3AEs376Brm5ZEhilIIyIiDU9TkERERBqOAjByRIuJDOPCIamcPygFn3N8unAz957Vh5enr2P2+lweGNOXy47qyMx12/lgbgZPf7OG175Pp6S8ik6towgPDWFAajzPfbeGXskxfLUsi8TocCYtzSSuRRjzN+Zx9X9m8txVQ8GDzfmlDOpg9Wumr87m5y/OZMyg9jxx2eD9ttXzPKauymZwx3hiIsOC3TUiInIE8PtVhFdERKShKAAjAvh8jkcvHsD956URGxnG6H7JzFqXw4VDUgJTm9owonsiHROi+GzRFh752QCGdbbivQWlFZzxzyn8ZtwCoiNCKa/04/c83rn5WLYVlHHHO/M58/GpFJZVUlbp55GLBnBy77b87t2FhPocHy/YzLXHdSY+KhyfA4dj/sZcPpy3ifySCl67fjgl5VXc9d5Cpq7K5vS0JJ6/ethu7V+fXUT7+BaEh/oao/tEROQwVZ0BgwIwIiIiQacAjEiAc47YQGZJh4QoOiRE7fX8naf34s7Te+22PSYyjCevGMLHCzZx20ndCQvxkVNcTrc20QB81Pp47v7gR3olx7Alv5Q/fvAjIYFU79evP5rbx83n0ud/oLzSv9txk2Mj2VZYxu1vz2dVZiE5ReWc2ieJiUszeX9uBn3axdC9bTRTVmZz0+tzGNWzDS9cPYywkAMLwhSVVdZa20ZERJo/1YARERFpOBp1iRwCQzu1YminVjsft2oZvvN+n3axfHzbCABKyqsYO2kFYSE+zurfjn4pcTx4fj/enLmBU/q0pUVYCBVVHv1SYunbPo4Xpq7l4QnLiY8K452bjyGtXSxjnp7OXe8tBKBdXCT5JRW0j2vBtyu2cdd7C/nnJYPw1SOXfGNOMY9NXMEnCzdzy6huO2vYTFyylY25JVw/osuh7CIREWmCqhNgFIAREREJPgVgRBpQi/AQ7j07bbdtp/dN5vS+ybXuf9MJXYkKD+G4bq3p3jYGgOeuHMq3K7JoGRHKmzM3EBrieP+W43h/bgb/+HIFcS3C6NMulvkbcnE4TunTllP7JOH3PEJDfHiexzcrsrhj3AIq/R6DO7bimW/X4HOO3OJy3py5AYDWLcMZM6g9U1Zl8/bMDVxzXGeO7db6oPsgq6CUvOIKeibFHPSxRETk4Pg91YARERFpKArAiDRhPp/j6mM777atQ0IUVwW2XTgkFc/zcM7xqxO7kVtUzovT1gGQGB1Opd/jnTkbiQzzUVrhp11cJKEhjo05JfROjuGFq4eRHBfJL16ZzVPfrAbg6mM7sWzLDu7772L+9fUq1mYX4XPw7cosrju+C7PW5dC9TTQDO8RT6fdzUq+2e03X2tN/529i/fYifjYklcue/4Et+SX89tSe/PLEboT4HBm5JbSJifjJq0Fl7SilpKKKTq1b/qSfFxE5UlXXgHHKgBEREQk6BWBEDnPVF83OOf50Vh96JsfQrU00QzrGU+X3+HzRFhZszCM2Mox12UWUVFTxy1HdOX9we6LC7U/Aq9cNZ1NeCfFRYcREhpGRW8zV/5lFSqsW3DSyK6N6teGal2bx7LdrSGsXy+eLtvDOnI0AhPiWct7A9txwQhdKK6qICA2hX4otBV5WWcVfP13KW4GsmicnryYi1McpfZJ4bNJKxs3eSGJ0OAsz8gkP8dE/NY7hXRK49rjOJMVG7nyPnucxa10OXdtE0yYmYrf3vymvhAufmU5BaSUvX3sUO0ormb46m22FZfxyVDf6pcRRWlG13+BOld8jRF8Bi8gRRhkwIiIiDcdVF19raMOGDfPmzJnTKK8tIgeuoLSCnKJyOrVuSVllFdsLy6mo8vP69+m8OXMDJRVVO/c9q38yJ/Vqy9uzNjBvQx43j+rKMV1b8+TXq7j9lB6M6tmGycuz+PeUtewoqeDCISlsLypnzvpcFm7MIzoylF8c34WEluGM7pfM2zM38NiklQAkxUbgeTAgNZ5OraP4elkm24vKSYyOYF12EQBR4SGE+hwVVR7DOrdi6qps+rSL5bKjOnDF0R0JC/FR5feYumobcS3CmLB4K6/OWM/TVwzh1LSkvd57RZWf7MIyIkJDSNilvk9ddpRW8M6sjfxsaGq9f0akqXLOzfU8b9j+95RgCsa10+TlmfzilTl8fOvxDOwQf0iPLSIiciSq67pJARgROWg5ReV8/uNmkmIjWbalgOe+W0NJRRVR4SH846KBnD2gXb2PtWZbIXe9t5D5G/IAaBEWQklFFecMaEevpBg25hZT5YdZ67ezraCMlPgWPHhBf7omtuTRiSsY0aMNZ/VLJqe4nBtfm8um3GLOGdCe+RvzWLgxj25tWvLQhQP474JNOzNzABJahhPic7x87VF8vmgLM1Znk9AynIuGduAfXy5n/fZiAK47vjP3nNmH8FAfhWWVZBeUUen36JLYko05xXy9PIvT05K477+L+W7lNjoktOC+s9PomBBF7+QYpfnLYUkBmKYhGNdOXy3N5IbX5vDpbSPonxp3SI8tIiJyJFIARkQaVFllFZvzSomNDKV1dMT+f2APnudRVF7FptwSnpy8Cg/45yWDCA89sCW2/X4PDwjxOTzPY/LyLP7y6RI25pQAcP2ILhzVuRVJsZGEh/oY89R0Kv0eoT7H4I7xrMwsDKwyFcktJ3ZjxdYC3py5gYhQH9ERoWwvKt/5WnEtwigsq6TK7+EceB7ceEIXPl6wmayCMgC6tmnJhYNTGNY5gVVZhaS1i2Fop4Sdx8gpKueOdxbg93v86qRuJEZHkBgdQXREKBOXbqV1y4gDLoRcXumvs98qq/ws2JjHgNT4A+rfpZt30Do6fLepYtJ8KQDTNATj2mnikq3c9PpcPvv1iJ3TR0VEROSnq+u6STVgROSQiwgNoUviTy+I65wjOiKUXskxPHXFkJ98nF2X43bOcUqfJI7t1prHv1qFzzn+OLrXbhkpD13Yn7XZRVxzbGeS4yJ3ZvacO7A98VE2jej0vslMW7WNwrJKUltF0S4uEr8Hs9ZtJz4qnHMHtOfNmem0jY3kztN6cvspPViZWcjqrALem5PBoxNX7ny9sBDHQxcOYM76HNZlF7Exp5jsonJiIkK54oWZO/eLiQiloKwSgAuHpPDAmH5ER9T8+Z6+Opvnp6ylb/tYBndsRYgPJi3NZNrqbDJyS+jXPo4TeiTSqXUUny7cwoDUOH5/Ri8A7vlwEe/NzaB1YLrX8C4WEBqYGk/nxJaszy5iU14JMZGh9E+JwznHF4u3cttb80iMjuC9W47dqwjztyuyeOabNYy9dCBJsZEs2pRPqM/Rp10sYSH1C/JUVvnxObfzM6zyeyzbsoPc4nJ6JsXUO/CTVVBKQlQ4ofV8XZEjTU0R3sZth4iIyJFAGTAiIg0oa0cpP2bk0yEhijvfXcCSzTsID/UxICUO5+Ces/rQKymGb1ZkAZC+vZi124o4e0AyCzbm8/Q3q+nWpiWDOsTvrG0zZeU24qPCyC2uoCowmmoZHsKIHol0SYxmXnou8zbkUun3aBXY7/oRXajye7wyYz2XHdWBHaUVfLdiG0XlVssnxOcY2rEVs9bn7Gx7z6RokuNaMGO1ve6GnGJahodw6VEdOX9wezq1bsmW/BLOfGIqecUVpLWLJToidOcxBnaI543rhxMTGbbzmOWVfp76ZjVvzUwnv6SCAanxDOoQzzuzN9IiPISTe7WlS5uWfDgvg5WZhYANFE/q1Za/nd+P9vEt9tnX6duLGP34VE7okci/rxqKc46M3GI255XuDDQdCp7nUV7lJyL0p63idThQBsyBc86NBp4AQoAXPc97eI/nrwX+AWwKbHrK87wX6zpmMK6dvli8hVvemMeE35xAn3axh/TYIiIiRyJNQRIRaYK2F5bx9qwNjBmUst+lvKtNX53NrW/No7SiihHd27B0cz5p7eMYe+lAPA/WZxdRVFbJkE6tdlv5qbCskvTtRfROjuX37y/kw3k25rtgcAqPXTwQn89RXulnXXYRfs/jzZnpfLtiGxcMTmFE90TSc4p5a+YGyiv9DO4Yz91n9mbttiLu/2QJCzPy8DnHCT0SWZ1VSG5ROb8/oxd//WwpEaE+7j2rDwB//XQpXRJbktKqBT7n8HseSzfvIKugjNPSkuiUEMWXS7eyMaeEM/omEeJzTFuVzY7SSjq3juK2k3vQPj6S79ds56Vp6wgP9XHlMZ3olxLH6WlJbCsoY/qabDomRNG9bQy/fGMu36/djufBr0/uzvwNeUxbnQ3A/eemcfnwjpRV+olrEcbHCzbx/twMHrqwP5VVHisyCzijbzI/rN3OJws3c9fpvfYqpuz3ezw3ZQ3jZm0kI7eYXsmxDOvUir7tY2nVMpzjurXeLdhUl0UZ+XRt05KWgcym7YVlPD9lLR/N38SI7once3afnzSd71BRAObAOOdCgJXAaUAGMBu43PO8pbvscy0wzPO82+p73GBcO41ftIVfvTmPL+8YSa/kmEN6bBERkSORAjAiIs1IblE5Pp8jrkX9Bvd7qp7OkxwXSeIhGNRn7ijl+Slr+XpZJqmtorhxZFdG9WzD1FXbSIqNpGeSDeq+WLyFx79aRViIDw8Pz4OubaK5cHAKJ/Vuu7Nt+SUVO4Mdnuexvaic+BZhu00jqi7WvHBjHn4PhndOYGVWAXnFFbu17X/H9OXjBZuZk55LYnQE1xzbiUWb8pm4NJPwQDuuObYzr36/nooqj4SW4RSWVVJe6eeOU3vw2vfp5BSVkxLfgq5tWlJUVsklwzpwZv92/Pu7NTzz7RqO796aAanxLMrIZ/6G3J1ZRB0SWnD36D7kl1SwcGMe2wrL6NE2mtPSkuiQEMWnCzfTISGKHzPyePqbNfRKimHspQNZn13M/Z8sJqeonOO6JfLD2u1U+j3io8II9floGRFCn+RYTuiZSNuYSCYvz6KssorYyDD+cl7fg/48a6MAzIFxzh0L/MXzvDMCj+8B8DzvoV32uZYmEID57MfN3PbWfCb9diQ9khSAEREROVgKwIiISLNUXunn3Tkb+fv4ZXRvG82fz0kjv6SClZmF+D2PX47qRnZhGd+u2MY5A9sRFR5KeaWfJ75eSWWVx+qsQr5enkWXxJY8ctEA7vlwEX3bx5JfUsG3K7bRIiyEh3/Wn2e/XYNzDr/fsmOqCy1fcXRHHjy/385aQpVVfrbkl7J6WyH3fbSYTXlW8Dk+KoykmEjWZRdRXuXf632M7pvM9DXZFJRarZ/eyTE8ftkgeifHsjKzgC8WbyWroJQqP+woqWBhRh4ZuXbsmIhQYluEER0Rype/HRmUflYA5sA45y4CRnued0Pg8VXA0bsGWwIBmIeAbVi2zG89z9tY13GDce30ycLN3P72fL66cxTd20Yf0mOLiIgciRSAERGRZq2wrJKosJDdCi/Xh+d5TFi8lYEd4knZpZ5MQWkFd723kPMGpuy2jLrnecxNz2Xa6mzKK/3ceVrPfRb4LSitYOnmHbSPb0FqqxY45ygur+TThZvZlFvCmMEpZOSWUF7p59Q+bdmQU8y01dmkxLfg2G6t66wp43key7cWsL2wnOFdEg54hbADpQDMgXHOXQycsUcAZrjneb/eZZ/WQKHneWXOuVuASzzPO7mWY90E3ATQsWPHoenp6Ye0rZOXZ/Lg58t45brh9Z4KKSIiIvumAIyIiIj8ZArAHJj6TEHaY/8QIMfzvDrXgda1k4iISNNX13WT1uUUERERObRmAz2cc12cc+HAZcAnu+7gnGu3y8PzgGUN2D4RERFpBKGN3QARERGR5sTzvErn3G3Al9gy1C95nrfEOfcAMMfzvE+A251z5wGVQA5wbaM1WERERBqEAjAiIiIih5jneeOB8Xts+/Mu9+8B7mnodomIiEjj0RQkEREREREREZEgUwBGRERERERERCTIFIAREREREREREQkyBWBERERERERERIJMARgRERERERERkSBTAEZEREREREREJMgUgBERERERERERCTIFYEREREREREREgkwBGBERERERERGRIFMARkREREREREQkyBSAEREREREREREJMgVgRERERERERESCTAEYEREREREREZEgUwBGRERERERERCTIFIAREREREREREQkyBWBERERERERERIJMARgRERERERERkSBTAEZEREREREREJMgUgBERERERERERCTIFYEREREREREREgsx5ntc4L+zcNiA9SIdPBLKDdOzmRP1Uf+qr+lNf1Z/6qv7UV/UXjL7q5Hlem0N8TDlAQbx20u9X/amv6k99VX/qq/pTX9Wf+qp+GvS6qdECMMHknJvjed6wxm5HU6d+qj/1Vf2pr+pPfVV/6qv6U1/JgdI5U3/qq/pTX9Wf+qr+1Ff1p76qn4buJ01BEhEREREREREJMgVgRERERERERESCrLkGYJ5v7AYcJtRP9ae+qj/1Vf2pr+pPfVV/6is5UDpn6k99VX/qq/pTX9Wf+qr+1Ff106D91CxrwIiIiIiIiIiINCXNNQNGRERERERERKTJaFYBGOfcaOfcCufcaufc3Y3dnqbGObfeObfIObfAOTcnsC3BOTfJObcqcNuqsdvZGJxzLznnspxzi3fZVmvfOPOvwHn2o3NuSOO1vOHto6/+4pzbFDi3FjjnztrluXsCfbXCOXdG47S64TnnOjjnvnHOLXPOLXHO/SawXefVHuroK51Xe3DORTrnZjnnFgb66q+B7V2cczMD59U7zrnwwPaIwOPVgec7N2b7penRtVPddO20b7p2qj9dO9WPrp3qT9dO9dfUrp2aTQDGORcCPA2cCaQBlzvn0hq3VU3SSZ7nDdplqa27ga89z+sBfB14fCR6BRi9x7Z99c2ZQI/Av5uAZxuojU3FK+zdVwD/DJxbgzzPGw8Q+B28DOgb+JlnAr+rR4JK4Hee5/UBjgFuDfSHzqu97auvQOfVnsqAkz3PGwgMAkY7544B/g/rqx5ALnB9YP/rgVzP87oD/wzsJwLo2ukA6Nqpdq+ga6f6egVdO9WHrp3qT9dO9dekrp2aTQAGGA6s9jxvred55cA4YEwjt+lwMAZ4NXD/VeD8RmxLo/E8bwqQs8fmffXNGOA1z/wAxDvn2jVMSxvfPvpqX8YA4zzPK/M8bx2wGvtdbfY8z9vied68wP0CYBmQgs6rvdTRV/tyJJ9Xnud5hYGHYYF/HnAy8H5g+57nVfX59j5winPONVBzpenTtdNPo2sndO10IHTtVD+6dqo/XTvVX1O7dmpOAZgUYOMujzOo+yQ8EnnAROfcXOfcTYFtSZ7nbQH7RQbaNlrrmp599Y3OtdrdFkj/fGmXdGz1FRBIXRwMzETnVZ326CvQebUX51yIc24BkAVMAtYAeZ7nVQZ22bU/dvZV4Pl8oHXDtliasCP6d6medO10YPR/3IHR/3FDGsXnAAAC+klEQVT7oGun+tO10/41pWun5hSAqS0qpSWedne853lDsHS9W51zIxu7QYcpnWt7exbohqX1bQEeC2w/4vvKORcNfADc4Xnejrp2rWXbkd5XOq9q4Xleled5g4BU7NurPrXtFrg9ovtK9kvnx/7p2unQ0Lm2N/0ftw+6dqo/XTvVT1O6dmpOAZgMoMMuj1OBzY3UlibJ87zNgdss4CPs5MusTtUL3GY1XgubnH31jc61PXielxn4w+YHXqAmpfGI7ivnXBj2n+Kbnud9GNis86oWtfWVzqu6eZ6XB3yLzf2Od86FBp7atT929lXg+TjqnwYvzZ9+l/ZD104HTP/H1ZP+j6udrp3qT9dOB64pXDs1pwDMbKBHoJpxOFZk6JNGblOT4Zxr6ZyLqb4PnA4sxvromsBu1wAfN04Lm6R99c0nwNWByuvHAPnVaZFHqj3m216AnVtgfXVZoJp4F6xI2qyGbl9jCMwV/Q+wzPO8sbs8pfNqD/vqK51Xe3POtXHOxQfutwBOxeZ9fwNcFNhtz/Oq+ny7CJjsed4R842X7Jeuneqga6efRP/H1ZP+j9ubrp3qT9dO9dfUrp1C97/L4cHzvErn3G3Al0AI8JLneUsauVlNSRLwUaB+UCjwlud5XzjnZgPvOueuBzYAFzdiGxuNc+5t4EQg0TmXAdwPPEztfTMeOAsrXlUMXNfgDW5E++irE51zg7D0vPXAzQCe5y1xzr0LLMWqtd/qeV5VY7S7ERwPXAUsCsw5BfgTOq9qs6++ulzn1V7aAa86W7nAB7zred5nzrmlwDjn3N+A+dhFGYHb151zq7Fvby5rjEZL06Rrp/3StVMddO1Uf7p2qjddO9Wfrp3qr0ldOzl9ESYiIiIiIiIiElzNaQqSiIiIiIiIiEiTpACMiIiIiIiIiEiQKQAjIiIiIiIiIhJkCsCIiIiIiIiIiASZAjAiIiIiIiIiIkGmAIyIiIiIiIiISJApACMiIiIiIiIiEmQKwIiIiIiIiIiIBNn/AygXM+9CHQOnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 2160x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(hist.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(hist.history[\"acc\"], label=\"acc\")\n",
    "plt.plot(hist.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(out_dir,'losscurve.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(out_dir,'final_epoch.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
