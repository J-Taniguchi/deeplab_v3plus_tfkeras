{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3plus_dir=\"./src\"\n",
    "sys.path.append(deeplabv3plus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(visible_device_list=\"1\", allow_growth=True)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = gpu_options)\n",
    "tf.compat.v1.enable_eager_execution(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import deeplab_v3plus_transfer_os16\n",
    "#from image_utils import make_x_from_image_paths,make_y_from_image_paths,convert_y_to_image_array\n",
    "from data_gen import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from './src/model.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sys.modules['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"xception_transfer_os16\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 21\n",
    "image_size = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_img_dir = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/SegmentationClass\"\n",
    "img_dir = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/JPEGImages\"\n",
    "train_set_path = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt\"\n",
    "valid_set_path = \"../pascal_voc_2012_datasets/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt\"\n",
    "\n",
    "with open(train_set_path) as f:\n",
    "    train_img_names = f.read().split(\"\\n\")[:-1]\n",
    "with open(valid_set_path) as f:\n",
    "    valid_img_names = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "img_paths = [os.path.join(img_dir,train_img_names[i]) + \".jpg\" for i in range(len(train_img_names))]\n",
    "seg_img_paths = [os.path.join(seg_img_dir,train_img_names[i]) + \".png\" for i in range(len(train_img_names))]\n",
    "\n",
    "valid_x_paths = [os.path.join(img_dir,valid_img_names[i]) + \".jpg\" for i in range(len(valid_img_names))]\n",
    "valid_y_paths = [os.path.join(seg_img_dir,valid_img_names[i]) + \".png\" for i in range(len(valid_img_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "n_epochs=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(n_categories, image_size, batch_size, img_paths    , seg_img_paths, augmentation=True )\n",
    "valid_data_gen = DataGenerator(n_categories, image_size, batch_size, valid_x_paths, valid_y_paths, augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.applications.Xception(input_shape=(512,512,3), weights=\"imagenet\", include_top=False)\n",
    "layer_name_to_decoder = \"block4_sepconv1_act\"\n",
    "encoder_end_layer_name = \"block13_sepconv2_bn\"\n",
    "model = deeplab_v3plus_transfer_os16(n_categories, encoder, layer_name_to_decoder, encoder_end_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.categorical_crossentropy\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=loss_function, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(out_dir,'{epoch:06d}.h5')\n",
    "cp_cb = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                        monitor='val_loss', \n",
    "                                        verbose=0, \n",
    "                                        save_best_only=True, \n",
    "                                        save_weights_only=False, \n",
    "                                        mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1128 14:03:25.424393 140735685348560 deprecation.py:323] From /home/taniguchi-j/anaconda3/envs/ppc/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/92 [============================>.] - ETA: 1s - loss: 2.3962 - acc: 0.6048Epoch 1/300\n",
      "92/92 [==============================] - 199s 2s/step - loss: 2.3942 - acc: 0.6051 - val_loss: 2.6726 - val_acc: 0.7528\n",
      "Epoch 2/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 2.0288 - acc: 0.7350Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 2.3322 - acc: 0.7504Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 2.0278 - acc: 0.7350 - val_loss: 2.3336 - val_acc: 0.7495\n",
      "Epoch 3/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.8398 - acc: 0.7611Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 1.8389 - acc: 0.7617 - val_loss: 2.0137 - val_acc: 0.7770\n",
      "Epoch 4/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.6778 - acc: 0.7838Epoch 1/300\n",
      "92/92 [==============================] - 188s 2s/step - loss: 1.6784 - acc: 0.7840 - val_loss: 1.6405 - val_acc: 0.8576\n",
      "Epoch 5/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.5407 - acc: 0.8014Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 1.3754 - acc: 0.8798Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 1.5392 - acc: 0.8017 - val_loss: 1.3767 - val_acc: 0.8794\n",
      "Epoch 6/300\n",
      "91/92 [============================>.] - ETA: 2s - loss: 1.4109 - acc: 0.8183Epoch 1/300\n",
      "92/92 [==============================] - 297s 3s/step - loss: 1.4111 - acc: 0.8177 - val_loss: 1.1939 - val_acc: 0.8799\n",
      "Epoch 7/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.2954 - acc: 0.8333Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 1.0662 - acc: 0.8806Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 1.2945 - acc: 0.8333 - val_loss: 1.0686 - val_acc: 0.8802\n",
      "Epoch 8/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.2034 - acc: 0.8357Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.9653 - acc: 0.8808Epoch 1/300\n",
      "92/92 [==============================] - 191s 2s/step - loss: 1.2016 - acc: 0.8361 - val_loss: 0.9685 - val_acc: 0.8803\n",
      "Epoch 9/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.1130 - acc: 0.8436Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.9294 - acc: 0.8790Epoch 1/300\n",
      "92/92 [==============================] - 180s 2s/step - loss: 1.1137 - acc: 0.8436 - val_loss: 0.9303 - val_acc: 0.8787\n",
      "Epoch 10/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 1.0279 - acc: 0.8545Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.9139 - acc: 0.8744Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 1.0289 - acc: 0.8543 - val_loss: 0.9155 - val_acc: 0.8743\n",
      "Epoch 11/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.9613 - acc: 0.8577Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.7876 - acc: 0.8855Epoch 1/300\n",
      "92/92 [==============================] - 181s 2s/step - loss: 0.9622 - acc: 0.8574 - val_loss: 0.7897 - val_acc: 0.8852\n",
      "Epoch 12/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.8888 - acc: 0.8656Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.7434 - acc: 0.8938Epoch 1/300\n",
      "92/92 [==============================] - 176s 2s/step - loss: 0.8880 - acc: 0.8658 - val_loss: 0.7453 - val_acc: 0.8936\n",
      "Epoch 13/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.8413 - acc: 0.8658Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.7302 - acc: 0.8852Epoch 1/300\n",
      "92/92 [==============================] - 193s 2s/step - loss: 0.8412 - acc: 0.8660 - val_loss: 0.7328 - val_acc: 0.8848\n",
      "Epoch 14/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.7877 - acc: 0.8738Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6755 - acc: 0.8927Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 0.7871 - acc: 0.8740 - val_loss: 0.6785 - val_acc: 0.8923\n",
      "Epoch 15/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.7417 - acc: 0.8751Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6346 - acc: 0.8950Epoch 1/300\n",
      "92/92 [==============================] - 175s 2s/step - loss: 0.7427 - acc: 0.8751 - val_loss: 0.6377 - val_acc: 0.8945\n",
      "Epoch 16/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6945 - acc: 0.8824Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.6414 - acc: 0.8864Epoch 1/300\n",
      "92/92 [==============================] - 188s 2s/step - loss: 0.6940 - acc: 0.8824 - val_loss: 0.6434 - val_acc: 0.8862\n",
      "Epoch 17/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6703 - acc: 0.8791Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5754 - acc: 0.8954Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.6699 - acc: 0.8793 - val_loss: 0.5768 - val_acc: 0.8953\n",
      "Epoch 18/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.6294 - acc: 0.8859Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5711 - acc: 0.8943Epoch 1/300\n",
      "92/92 [==============================] - 208s 2s/step - loss: 0.6293 - acc: 0.8860 - val_loss: 0.5739 - val_acc: 0.8939\n",
      "Epoch 19/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5913 - acc: 0.8907Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5317 - acc: 0.8988Epoch 1/300\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.5928 - acc: 0.8903 - val_loss: 0.5356 - val_acc: 0.8983\n",
      "Epoch 20/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5715 - acc: 0.8912Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.5279 - acc: 0.8959Epoch 1/300\n",
      "92/92 [==============================] - 210s 2s/step - loss: 0.5715 - acc: 0.8911 - val_loss: 0.5327 - val_acc: 0.8951\n",
      "Epoch 21/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5411 - acc: 0.8938Epoch 1/300\n",
      "92/92 [==============================] - 187s 2s/step - loss: 0.5406 - acc: 0.8941 - val_loss: 0.5095 - val_acc: 0.8916\n",
      "Epoch 22/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5141 - acc: 0.8992Epoch 1/300\n",
      "92/92 [==============================] - 201s 2s/step - loss: 0.5139 - acc: 0.8991 - val_loss: 0.4865 - val_acc: 0.8930\n",
      "Epoch 23/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.5089 - acc: 0.8956Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4914 - acc: 0.8931\n",
      "92/92 [==============================] - 173s 2s/step - loss: 0.5088 - acc: 0.8957 - val_loss: 0.4971 - val_acc: 0.8924\n",
      "Epoch 24/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4856 - acc: 0.8971Epoch 1/300\n",
      "92/92 [==============================] - 184s 2s/step - loss: 0.4849 - acc: 0.8974 - val_loss: 0.4586 - val_acc: 0.9004\n",
      "Epoch 25/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4625 - acc: 0.9015Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4306 - acc: 0.9024Epoch 1/300\n",
      "92/92 [==============================] - 186s 2s/step - loss: 0.4632 - acc: 0.9012 - val_loss: 0.4348 - val_acc: 0.9017\n",
      "Epoch 26/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4477 - acc: 0.9013Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4429 - acc: 0.8943Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.4468 - acc: 0.9017 - val_loss: 0.4477 - val_acc: 0.8936\n",
      "Epoch 27/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4330 - acc: 0.9038Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4292 - acc: 0.8980Epoch 1/300\n",
      "92/92 [==============================] - 199s 2s/step - loss: 0.4335 - acc: 0.9034 - val_loss: 0.4335 - val_acc: 0.8973\n",
      "Epoch 28/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4148 - acc: 0.9059Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4290 - acc: 0.9003Epoch 1/300\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.4146 - acc: 0.9060 - val_loss: 0.4335 - val_acc: 0.8997\n",
      "Epoch 29/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4108 - acc: 0.9048Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4652 - acc: 0.8936Epoch 1/300\n",
      "92/92 [==============================] - 173s 2s/step - loss: 0.4099 - acc: 0.9053 - val_loss: 0.4691 - val_acc: 0.8931\n",
      "Epoch 30/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.4092 - acc: 0.9029Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4257 - acc: 0.8959Epoch 1/300\n",
      "92/92 [==============================] - 209s 2s/step - loss: 0.4096 - acc: 0.9027 - val_loss: 0.4336 - val_acc: 0.8949\n",
      "Epoch 31/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3916 - acc: 0.9060Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.3913 - acc: 0.9062 - val_loss: 0.4099 - val_acc: 0.8975\n",
      "Epoch 32/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3770 - acc: 0.9088Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3971 - acc: 0.8971Epoch 1/300\n",
      "92/92 [==============================] - 179s 2s/step - loss: 0.3773 - acc: 0.9087 - val_loss: 0.4032 - val_acc: 0.8960\n",
      "Epoch 33/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3702 - acc: 0.9092Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4253 - acc: 0.8978Epoch 1/300\n",
      "92/92 [==============================] - 183s 2s/step - loss: 0.3707 - acc: 0.9089 - val_loss: 0.4296 - val_acc: 0.8972\n",
      "Epoch 34/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3597 - acc: 0.9109Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4364 - acc: 0.8898Epoch 1/300\n",
      "92/92 [==============================] - 171s 2s/step - loss: 0.3600 - acc: 0.9107 - val_loss: 0.4421 - val_acc: 0.8890\n",
      "Epoch 35/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3501 - acc: 0.9117Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.4309 - acc: 0.8976Epoch 1/300\n",
      "92/92 [==============================] - 170s 2s/step - loss: 0.3518 - acc: 0.9110 - val_loss: 0.4369 - val_acc: 0.8966\n",
      "Epoch 36/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3356 - acc: 0.9138Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3793 - acc: 0.9021\n",
      "92/92 [==============================] - 178s 2s/step - loss: 0.3358 - acc: 0.9139 - val_loss: 0.3836 - val_acc: 0.9015\n",
      "Epoch 37/300\n",
      "91/92 [============================>.] - ETA: 1s - loss: 0.3274 - acc: 0.9157Epoch 1/300\n",
      "90/92 [============================>.] - ETA: 1s - loss: 0.3800 - acc: 0.9011Epoch 1/300\n",
      "92/92 [==============================] - 174s 2s/step - loss: 0.3279 - acc: 0.9155 - val_loss: 0.3846 - val_acc: 0.9004\n",
      "Epoch 38/300\n",
      "71/92 [======================>.......] - ETA: 28s - loss: 0.3280 - acc: 0.9165"
     ]
    }
   ],
   "source": [
    "#hist = model.fit_generator(data_gen, validation_data=(valid_x, valid_y), epochs=par.n_epochs, steps_per_epoch=par.n_batch, callbacks=[cp_cb])\n",
    "hist = model.fit_generator(train_data_gen,\n",
    "                           epochs=n_epochs,\n",
    "                           steps_per_epoch=len(train_data_gen),\n",
    "                           validation_data=valid_data_gen,\n",
    "                           validation_steps=len(valid_data_gen),\n",
    "                           shuffle = True,\n",
    "                           workers=8,\n",
    "                           use_multiprocessing=True,\n",
    "                           callbacks=[cp_cb])\n",
    "#hist = model.fit_generator(data_gen, epochs=par.n_epochs, steps_per_epoch=par.n_batch, workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(hist.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(hist.history[\"acc\"], label=\"acc\")\n",
    "plt.plot(hist.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(out_dir,'losscurve.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(out_dir,'final_epoch.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
